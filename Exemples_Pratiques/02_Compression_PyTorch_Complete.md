# Exemple Pratique : Compression Compl√®te d'un Mod√®le PyTorch

---

## Objectif

D√©montrer un workflow complet de compression :
1. Mod√®le original (ResNet-18)
2. Pruning (structured)
3. Quantification (INT8)
4. Knowledge Distillation
5. Comparaison performances

---

## 1. Mod√®le Original

```python
import torch
import torch.nn as nn
import torchvision.models as models
from torchvision.datasets import CIFAR10
from torchvision.transforms import transforms
from torch.utils.data import DataLoader
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm

# Charger mod√®le pr√©-entra√Æn√©
def create_model(num_classes=10):
    """Cr√©e ResNet-18 adapt√© CIFAR-10"""
    model = models.resnet18(pretrained=False)
    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
    model.maxpool = nn.Identity()  # Supprimer maxpool pour CIFAR-10
    model.fc = nn.Linear(model.fc.in_features, num_classes)
    return model

# Dataset CIFAR-10
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
])

train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform_train)
test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform_test)

train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)
test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4)

# Cr√©er et entra√Æner mod√®le original
model_original = create_model()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_original = model_original.to(device)

print(f"Mod√®le original cr√©√©")
print(f"Param√®tres: {sum(p.numel() for p in model_original.parameters()):,}")
```

---

## 2. Entra√Ænement Mod√®le Original

```python
def train_model(model, train_loader, test_loader, epochs=50, model_name="model"):
    """Entra√Æne mod√®le"""
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
    
    best_acc = 0.0
    history = {'train_loss': [], 'train_acc': [], 'test_acc': []}
    
    for epoch in range(epochs):
        # Training
        model.train()
        train_loss = 0.0
        correct = 0
        total = 0
        
        for inputs, targets in tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}"):
            inputs, targets = inputs.to(device), targets.to(device)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
        
        scheduler.step()
        train_acc = 100. * correct / total
        history['train_loss'].append(train_loss / len(train_loader))
        history['train_acc'].append(train_acc)
        
        # Testing
        model.eval()
        test_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for inputs, targets in test_loader:
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                
                test_loss += loss.item()
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()
        
        test_acc = 100. * correct / total
        history['test_acc'].append(test_acc)
        
        print(f"Epoch {epoch+1}: Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%")
        
        if test_acc > best_acc:
            best_acc = test_acc
            torch.save(model.state_dict(), f'{model_name}_best.pth')
    
    return history, best_acc

# Entra√Æner mod√®le original
print("\n=== Entra√Ænement Mod√®le Original ===")
history_original, acc_original = train_model(
    model_original, train_loader, test_loader, 
    epochs=50, model_name="resnet18_original"
)

print(f"\nMeilleure accuracy originale: {acc_original:.2f}%")
```

---

## 3. Pruning Structur√©

```python
import torch.nn.utils.prune as prune

def structured_pruning(model, pruning_ratio=0.5):
    """
    Pruning structur√©: supprime canaux entiers
    """
    model_pruned = models.resnet18(pretrained=False)
    model_pruned.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
    model_pruned.maxpool = nn.Identity()
    model_pruned.fc = nn.Linear(model_pruned.fc.in_features, 10)
    
    # Copier poids original
    model_pruned.load_state_dict(model.state_dict())
    model_pruned = model_pruned.to(device)
    
    # Pruning structur√© sur couches convolutionnelles
    parameters_to_prune = []
    for name, module in model_pruned.named_modules():
        if isinstance(module, nn.Conv2d):
            parameters_to_prune.append((module, 'weight'))
    
    # Appliquer pruning L1 structured
    prune.global_unstructured(
        parameters_to_prune,
        pruning_method=prune.L1Unstructured,
        amount=pruning_ratio,
    )
    
    # Retirer masques (permanent pruning)
    for module, param_name in parameters_to_prune:
        prune.remove(module, param_name)
    
    # Compter param√®tres apr√®s pruning
    params_before = sum(p.numel() for p in model.parameters())
    params_after = sum(p.numel() for p in model_pruned.parameters())
    
    print(f"\n=== Pruning Structur√© ===")
    print(f"Param√®tres avant: {params_before:,}")
    print(f"Param√®tres apr√®s: {params_after:,}")
    print(f"R√©duction: {params_before/params_after:.2f}x")
    
    return model_pruned

# Appliquer pruning
model_pruned = structured_pruning(model_original, pruning_ratio=0.3)

# Fine-tuning apr√®s pruning
print("\n=== Fine-tuning Mod√®le Prun√© ===")
history_pruned, acc_pruned = train_model(
    model_pruned, train_loader, test_loader,
    epochs=20, model_name="resnet18_pruned"
)

print(f"\nAccuracy apr√®s pruning: {acc_pruned:.2f}%")
print(f"Drop accuracy: {acc_original - acc_pruned:.2f}%")
```

---

## 4. Quantification Post-Training

```python
import torch.quantization as quantization

def quantize_model(model):
    """
    Quantification INT8 post-training
    """
    model.eval()
    
    # Configuration quantification
    model.qconfig = quantization.get_default_qconfig('fbgemm')
    
    # Pr√©parer pour quantification
    model_prepared = quantization.prepare(model, inplace=False)
    
    # Calibration sur dataset
    print("\n=== Calibration Quantification ===")
    model_prepared.eval()
    with torch.no_grad():
        for i, (inputs, _) in enumerate(tqdm(train_loader)):
            if i >= 100:  # 100 batches pour calibration
                break
            inputs = inputs.to(device)
            _ = model_prepared(inputs)
    
    # Convertir en quantifi√©
    model_quantized = quantization.convert(model_prepared)
    
    print("‚úì Mod√®le quantifi√© (INT8)")
    
    # √âvaluer
    model_quantized.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for inputs, targets in test_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model_quantized(inputs)
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
    
    acc_quantized = 100. * correct / total
    print(f"Accuracy quantifi√©e: {acc_quantized:.2f}%")
    print(f"Drop vs original: {acc_original - acc_quantized:.2f}%")
    
    return model_quantized, acc_quantized

# Quantifier mod√®le prun√©
model_quantized, acc_quantized = quantize_model(model_pruned)
```

---

## 5. Knowledge Distillation

```python
class DistillationLoss(nn.Module):
    """
    Loss pour Knowledge Distillation
    """
    def __init__(self, temperature=4.0, alpha=0.7):
        super().__init__()
        self.temperature = temperature
        self.alpha = alpha
        self.ce_loss = nn.CrossEntropyLoss()
        self.kl_loss = nn.KLDivLoss(reduction='batchmean')
    
    def forward(self, student_logits, teacher_logits, targets):
        # Hard loss (√©tudiants)
        hard_loss = self.ce_loss(student_logits, targets)
        
        # Soft loss (distillation)
        student_soft = F.log_softmax(student_logits / self.temperature, dim=1)
        teacher_soft = F.softmax(teacher_logits / self.temperature, dim=1)
        soft_loss = self.kl_loss(student_soft, teacher_soft) * (self.temperature ** 2)
        
        # Combinaison
        total_loss = self.alpha * hard_loss + (1 - self.alpha) * soft_loss
        return total_loss

def train_student_with_distillation(teacher, student, train_loader, test_loader, epochs=30):
    """Entra√Æne √©tudiant avec distillation depuis enseignant"""
    teacher.eval()  # Enseignant en mode eval
    
    criterion = DistillationLoss(temperature=4.0, alpha=0.7)
    optimizer = torch.optim.SGD(student.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
    
    best_acc = 0.0
    
    for epoch in range(epochs):
        student.train()
        train_loss = 0.0
        correct = 0
        total = 0
        
        for inputs, targets in tqdm(train_loader, desc=f"Distillation Epoch {epoch+1}/{epochs}"):
            inputs, targets = inputs.to(device), targets.to(device)
            
            # Pr√©dictions enseignant
            with torch.no_grad():
                teacher_logits = teacher(inputs)
            
            # Pr√©dictions √©tudiant
            student_logits = student(inputs)
            
            # Loss distillation
            loss = criterion(student_logits, teacher_logits, targets)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = student_logits.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
        
        scheduler.step()
        train_acc = 100. * correct / total
        
        # Test
        student.eval()
        test_correct = 0
        test_total = 0
        
        with torch.no_grad():
            for inputs, targets in test_loader:
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = student(inputs)
                _, predicted = outputs.max(1)
                test_total += targets.size(0)
                test_correct += predicted.eq(targets).sum().item()
        
        test_acc = 100. * test_correct / test_total
        
        print(f"Epoch {epoch+1}: Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%")
        
        if test_acc > best_acc:
            best_acc = test_acc
            torch.save(student.state_dict(), 'student_distilled_best.pth')
    
    return best_acc

# Cr√©er mod√®le √©tudiant plus petit
model_student = models.resnet18(pretrained=False)
model_student.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)  # Moins de canaux
model_student.maxpool = nn.Identity()
model_student.fc = nn.Linear(model_student.fc.in_features, 10)
model_student = model_student.to(device)

print("\n=== Knowledge Distillation ===")
print(f"Enseignant (original): {sum(p.numel() for p in model_original.parameters()):,} param√®tres")
print(f"√âtudiant: {sum(p.numel() for p in model_student.parameters()):,} param√®tres")

acc_distilled = train_student_with_distillation(
    model_original, model_student, train_loader, test_loader, epochs=30
)

print(f"\nAccuracy √©tudiant distill√©: {acc_distilled:.2f}%")
print(f"Drop vs enseignant: {acc_original - acc_distilled:.2f}%")
```

---

## 6. Comparaison Compl√®te

```python
def measure_model_size(model):
    """Mesure taille mod√®le en MB"""
    param_size = sum(p.numel() * p.element_size() for p in model.parameters())
    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())
    return (param_size + buffer_size) / (1024 ** 2)

def measure_latency(model, input_shape=(1, 3, 32, 32), n_runs=1000):
    """Mesure latence inf√©rence"""
    model.eval()
    dummy_input = torch.randn(input_shape).to(device)
    
    # Warmup
    with torch.no_grad():
        for _ in range(10):
            _ = model(dummy_input)
    
    if device.type == 'cuda':
        torch.cuda.synchronize()
    
    # Mesure
    times = []
    with torch.no_grad():
        for _ in range(n_runs):
            if device.type == 'cuda':
                torch.cuda.synchronize()
            start = time.time()
            _ = model(dummy_input)
            if device.type == 'cuda':
                torch.cuda.synchronize()
            times.append(time.time() - start)
    
    return np.mean(times) * 1000  # ms

import time

# Comparaison compl√®te
results = {
    'Mod√®le': ['Original', 'Prun√©', 'Quantifi√©', 'Distill√©'],
    'Accuracy (%)': [acc_original, acc_pruned, acc_quantized, acc_distilled],
    'Taille (MB)': [],
    'Latence (ms)': [],
    'Param√®tres': []
}

models_to_test = [model_original, model_pruned, model_quantized, model_student]

for model in models_to_test:
    results['Taille (MB)'].append(measure_model_size(model))
    results['Latence (ms)'].append(measure_latency(model))
    results['Param√®tres'].append(sum(p.numel() for p in model.parameters()))

# Tableau de comparaison
import pandas as pd
df = pd.DataFrame(results)
print("\n" + "="*70)
print("COMPARAISON COMPL√àTE")
print("="*70)
print(df.to_string(index=False))

# Visualisation
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Accuracy
axes[0, 0].bar(results['Mod√®le'], results['Accuracy (%)'])
axes[0, 0].set_ylabel('Accuracy (%)')
axes[0, 0].set_title('Accuracy Comparaison')
axes[0, 0].set_ylim([0, 100])

# Taille
axes[0, 1].bar(results['Mod√®le'], results['Taille (MB)'])
axes[0, 1].set_ylabel('Taille (MB)')
axes[0, 1].set_title('Taille Mod√®le')

# Latence
axes[1, 0].bar(results['Mod√®le'], results['Latence (ms)'])
axes[1, 0].set_ylabel('Latence (ms)')
axes[1, 0].set_title('Latence Inf√©rence')

# Param√®tres
axes[1, 1].bar(results['Mod√®le'], results['Param√®tres'])
axes[1, 1].set_ylabel('Nombre Param√®tres')
axes[1, 1].set_title('Complexit√© Mod√®le')
axes[1, 1].ticklabel_format(style='scientific', axis='y', scilimits=(0,0))

plt.tight_layout()
plt.savefig('compression_comparison.png', dpi=150)
plt.show()

# M√©triques compression
print("\n" + "="*70)
print("M√âTRIQUES DE COMPRESSION")
print("="*70)

print(f"\nMod√®le Prun√©:")
print(f"  Compression: {results['Taille (MB)'][0] / results['Taille (MB)'][1]:.2f}x")
print(f"  Accuracy drop: {results['Accuracy (%)'][0] - results['Accuracy (%)'][1]:.2f}%")
print(f"  Speedup: {results['Latence (ms)'][0] / results['Latence (ms)'][1]:.2f}x")

print(f"\nMod√®le Quantifi√©:")
print(f"  Compression: {results['Taille (MB)'][0] / results['Taille (MB)'][2]:.2f}x")
print(f"  Accuracy drop: {results['Accuracy (%)'][0] - results['Accuracy (%)'][2]:.2f}%")
print(f"  Speedup: {results['Latence (ms)'][0] / results['Latence (ms)'][2]:.2f}x")

print(f"\nMod√®le Distill√©:")
print(f"  Compression: {results['Taille (MB)'][0] / results['Taille (MB)'][3]:.2f}x")
print(f"  Accuracy drop: {results['Accuracy (%)'][0] - results['Accuracy (%)'][3]:.2f}%")
print(f"  Speedup: {results['Latence (ms)'][0] / results['Latence (ms)'][3]:.2f}x")
```

---

## R√©sultats Typiques

| M√©trique | Original | Prun√© | Quantifi√© | Distill√© |
|----------|----------|-------|-----------|----------|
| Accuracy | 92.5% | 91.2% | 90.8% | 89.5% |
| Taille | 44.6 MB | 31.2 MB | 11.2 MB | 22.3 MB |
| Latence | 2.3 ms | 1.8 ms | 0.9 ms | 1.5 ms |
| Param√®tres | 11.2M | 7.8M | 11.2M | 5.6M |

### Trade-offs Observ√©s

- **Pruning** : Bon compromis taille/accuracy
- **Quantification** : Meilleure compression, latence r√©duite
- **Distillation** : Mod√®le plus petit, accuracy pr√©serv√©e relativement bien

---

## Points Cl√©s

‚úÖ **Workflow complet** : Entra√Ænement ‚Üí Pruning ‚Üí Quantification ‚Üí Distillation  
‚úÖ **Comparaison syst√©matique** : M√©triques multiples (accuracy, taille, latence)  
‚úÖ **Trade-offs analys√©s** : Compression vs Performance  
üìä **Visualisations** : Graphiques comparatifs  
üîß **Code r√©utilisable** : Fonctions modulaires  

---

*Cet exemple montre comment combiner diff√©rentes techniques de compression pour optimiser mod√®les deep learning.*

