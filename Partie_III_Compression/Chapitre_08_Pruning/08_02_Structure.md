# 8.2 Pruning Structur√©

---

## Introduction

Le **pruning structur√©** supprime des structures enti√®res (filtres, canaux, couches) plut√¥t que des poids individuels. Cette approche permet une acc√©l√©ration r√©elle sur hardware standard sans n√©cessiter d'architectures sp√©cialis√©es pour la sparsit√©.

---

## 8.2.1 Filter Pruning

### Principe

Supprime des filtres entiers dans les couches convolutionnelles.

```python
import torch
import torch.nn as nn
import numpy as np

class FilterPruner:
    """
    Pruning de filtres complets dans les couches convolutionnelles
    """
    
    def __init__(self, model):
        self.model = model
        
    def compute_filter_importance(self, criterion='l1'):
        """
        Calcule l'importance de chaque filtre
        
        Crit√®res possibles:
        - 'l1': norme L1 des poids du filtre
        - 'l2': norme L2
        - 'apoz': average percentage of zeros dans les activations
        """
        importance = {}
        
        for name, module in self.model.named_modules():
            if isinstance(module, nn.Conv2d):
                weights = module.weight.data  # (out_channels, in_channels, kH, kW)
                
                if criterion == 'l1':
                    # Norme L1 par filtre
                    imp = weights.abs().sum(dim=(1, 2, 3))
                elif criterion == 'l2':
                    # Norme L2 par filtre
                    imp = (weights ** 2).sum(dim=(1, 2, 3)).sqrt()
                elif criterion == 'apoz':
                    # APoZ: n√©cessite forward pass sur donn√©es
                    # Pour l'instant, utilisons L1
                    imp = weights.abs().sum(dim=(1, 2, 3))
                
                importance[name] = imp
        
        return importance
    
    def prune_filters(self, module_name, n_filters_to_keep, importance):
        """
        Prune une couche conv en gardant les n_filters_to_keep filtres les plus importants
        """
        module = dict(self.model.named_modules())[module_name]
        
        if not isinstance(module, nn.Conv2d):
            raise ValueError(f"{module_name} n'est pas une Conv2d")
        
        # Indices des filtres √† garder
        _, indices = torch.topk(importance[module_name], n_filters_to_keep)
        indices = indices.sort()[0]
        
        # Cr√©e une nouvelle couche
        pruned_module = nn.Conv2d(
            module.in_channels,
            n_filters_to_keep,
            module.kernel_size,
            stride=module.stride,
            padding=module.padding,
            bias=module.bias is not None
        )
        
        # Copie les poids et biais
        pruned_module.weight.data = module.weight.data[indices].clone()
        if module.bias is not None:
            pruned_module.bias.data = module.bias.data[indices].clone()
        
        return pruned_module, indices

# Exemple
model = nn.Sequential(
    nn.Conv2d(3, 64, 3, padding=1),
    nn.ReLU(),
    nn.Conv2d(64, 128, 3, padding=1),
    nn.ReLU(),
    nn.Conv2d(128, 256, 3, padding=1)
)

pruner = FilterPruner(model)
importance = pruner.compute_filter_importance('l1')

print("Importance des filtres:")
for name, imp in importance.items():
    print(f"  {name}: {len(imp)} filtres, importance range: [{imp.min():.2f}, {imp.max():.2f}]")
```

### Pruning It√©ratif de Filtres

```python
def iterative_filter_pruning(model, train_loader, val_loader, 
                            target_sparsity=0.5, n_iterations=5):
    """
    Pruning it√©ratif de filtres avec fine-tuning
    """
    from copy import deepcopy
    
    current_sparsity = 0
    sparsity_per_iter = target_sparsity / n_iterations
    
    history = {'sparsity': [], 'accuracy': []}
    
    for iteration in range(n_iterations):
        # Calcule l'importance
        pruner = FilterPruner(model)
        importance = pruner.compute_filter_importance('l1')
        
        # Prune chaque couche
        for name, module in list(model.named_modules()):
            if isinstance(module, nn.Conv2d):
                current_filters = module.out_channels
                target_filters = int(current_filters * (1 - sparsity_per_iter))
                target_filters = max(1, target_filters)  # Garde au moins 1 filtre
                
                pruned_module, _ = pruner.prune_filters(
                    name, target_filters, importance
                )
                # Remplace dans le mod√®le (n√©cessite logique de remplacement)
        
        # Fine-tune
        fine_tune_model(model, train_loader, epochs=3)
        
        # √âvalue
        accuracy = evaluate(model, val_loader)
        current_sparsity = compute_model_sparsity(model)
        
        history['sparsity'].append(current_sparsity)
        history['accuracy'].append(accuracy)
        
        print(f"Iteration {iteration+1}: Sparsity={current_sparsity:.1%}, "
              f"Accuracy={accuracy:.2%}")
    
    return model, history
```

---

## 8.2.2 Channel Pruning

### Pruning de Canaux d'Entr√©e

```python
class ChannelPruner:
    """
    Pruning de canaux d'entr√©e dans les couches convolutionnelles
    
    N√©cessite de propager le pruning aux couches suivantes
    """
    
    def __init__(self, model):
        self.model = model
        
    def compute_channel_importance(self, layer_name, criterion='l1'):
        """
        Calcule l'importance des canaux d'entr√©e
        """
        module = dict(self.model.named_modules())[layer_name]
        
        if not isinstance(module, nn.Conv2d):
            raise ValueError("Module doit √™tre Conv2d")
        
        weights = module.weight.data  # (out_channels, in_channels, kH, kW)
        
        if criterion == 'l1':
            # Somme sur toutes les sorties et les dimensions spatiales
            importance = weights.abs().sum(dim=(0, 2, 3))
        elif criterion == 'l2':
            importance = (weights ** 2).sum(dim=(0, 2, 3)).sqrt()
        
        return importance
    
    def prune_input_channels(self, module_name, n_channels_to_keep, 
                            channel_indices=None):
        """
        Prune les canaux d'entr√©e d'une couche
        
        Si channel_indices est None, garde les n_channels_to_keep plus importants
        """
        module = dict(self.model.named_modules())[module_name]
        
        if channel_indices is None:
            importance = self.compute_channel_importance(module_name)
            _, channel_indices = torch.topk(importance, n_channels_to_keep)
            channel_indices = channel_indices.sort()[0]
        
        # Cr√©e nouvelle couche
        pruned_module = nn.Conv2d(
            n_channels_to_keep,
            module.out_channels,
            module.kernel_size,
            stride=module.stride,
            padding=module.padding,
            bias=module.bias is not None
        )
        
        # Copie les poids (s√©lectionne les canaux d'entr√©e)
        pruned_module.weight.data = module.weight.data[:, channel_indices].clone()
        if module.bias is not None:
            pruned_module.bias.data = module.bias.data.clone()
        
        return pruned_module, channel_indices
    
    def propagate_pruning(self, pruned_channels, next_layer_name):
        """
        Propage le pruning aux couches suivantes
        
        Si on prune les canaux de sortie d'une couche,
        il faut aussi pruner les canaux d'entr√©e de la suivante
        """
        # Impl√©mentation simplifi√©e
        pass

# Exemple avec propagation
def prune_resnet_block(block, target_channels):
    """
    Prune un bloc ResNet de mani√®re coh√©rente
    
    Les deux conv d'un bloc doivent avoir le m√™me nombre de canaux
    """
    conv1, bn1, relu, conv2, bn2 = block[0], block[1], block[2], block[3], block[4]
    
    # Prune conv1
    importance = conv1.weight.abs().sum(dim=(0, 2, 3))
    _, indices = torch.topk(importance, target_channels)
    
    # Cr√©e conv1 prun√©e
    new_conv1 = nn.Conv2d(conv1.in_channels, target_channels, conv1.kernel_size)
    new_conv1.weight.data = conv1.weight.data[indices]
    new_bn1 = nn.BatchNorm2d(target_channels)
    
    # Prune conv2 (doit correspondre aux sorties de conv1)
    new_conv2 = nn.Conv2d(target_channels, conv2.out_channels, conv2.kernel_size)
    new_conv2.weight.data = conv2.weight.data[:, indices]
    new_bn2 = nn.BatchNorm2d(conv2.out_channels)
    
    return nn.Sequential(new_conv1, new_bn1, relu, new_conv2, new_bn2)
```

---

## 8.2.3 Layer Pruning

### Suppression de Couches Enti√®res

```python
class LayerPruner:
    """
    Pruning de couches enti√®res dans un r√©seau
    """
    
    def __init__(self, model):
        self.model = model
        
    def compute_layer_importance(self, train_loader, criterion):
        """
        Calcule l'importance de chaque couche
        
        M√©thode: √©value la performance apr√®s suppression de chaque couche
        """
        baseline_accuracy = evaluate(self.model, train_loader)
        
        importance = {}
        layers = list(self.model.named_modules())
        
        for name, module in layers:
            if isinstance(module, (nn.Linear, nn.Conv2d)):
                # Sauvegarde originale
                original_state = module.state_dict().copy()
                
                # Remplace par Identity
                if isinstance(module, nn.Linear):
                    replacement = nn.Identity()
                    replacement.out_features = module.out_features
                else:
                    replacement = nn.Identity()
                
                # √âvalue
                # (n√©cessite logique de remplacement dans le mod√®le)
                # accuracy = evaluate(self.model, train_loader)
                # importance[name] = baseline_accuracy - accuracy
                
                # Restaure
                module.load_state_dict(original_state)
        
        return importance
    
    def remove_layer(self, layer_name):
        """
        Supprime une couche et reconnecte le r√©seau
        """
        # Logique complexe: n√©cessite de reconnecter les couches pr√©c√©dentes
        # et suivantes correctement
        pass

def identify_redundant_layers(model, train_loader, threshold=0.01):
    """
    Identifie les couches redondantes (qui peuvent √™tre supprim√©es
    sans perte significative de performance)
    """
    baseline_acc = evaluate(model, train_loader)
    redundant = []
    
    # Teste chaque couche
    for name, module in model.named_modules():
        if isinstance(module, (nn.Linear, nn.Conv2d)):
            # Simule la suppression (remplacement par Identity)
            # Mesure la perte de performance
            # Si < threshold, marque comme redondante
            pass
    
    return redundant
```

---

## Comparaison Filter vs Channel Pruning

```python
def compare_pruning_methods():
    """
    Compare les diff√©rentes m√©thodes de pruning structur√©
    """
    methods = {
        'Filter Pruning': {
            'Granularit√©': 'Filtre complet',
            'Acc√©l√©ration hardware': 'Bonne',
            'Facilit√© impl√©mentation': 'Facile',
            'Flexibilit√©': 'Moyenne'
        },
        'Channel Pruning': {
            'Granularit√©': 'Canal d\'entr√©e/sortie',
            'Acc√©l√©ration hardware': 'Tr√®s bonne',
            'Facilit√© impl√©mentation': 'Moyenne (propagation)',
            'Flexibilit√©': 'Bonne'
        },
        'Layer Pruning': {
            'Granularit√©': 'Couche enti√®re',
            'Acc√©l√©ration hardware': 'Excellente',
            'Facilit√© impl√©mentation': 'Difficile (reconnexion)',
            'Flexibilit√©': 'Faible'
        }
    }
    
    print("Comparaison des m√©thodes de pruning structur√©:")
    print(f"{'M√©thode':<20} | {'Granularit√©':<20} | {'Acc√©l√©ration':<15} | {'Facilit√©':<15}")
    print("-" * 75)
    for name, info in methods.items():
        print(f"{name:<20} | {info['Granularit√©']:<20} | {info['Acc√©l√©ration hardware']:<15} | {info['Facilit√© impl√©mentation']:<15}")

compare_pruning_methods()
```

---

## Pruning Structur√© pour Transformers

```python
class TransformerHeadPruner:
    """
    Pruning de t√™tes d'attention dans les Transformers
    """
    
    def compute_head_importance(self, model, train_loader):
        """
        Calcule l'importance de chaque t√™te d'attention
        """
        importance = {}
        
        for name, module in model.named_modules():
            if 'attention' in name.lower() and hasattr(module, 'num_heads'):
                # M√©thode: variance des scores d'attention
                # T√™tes avec faible variance sont moins importantes
                pass
        
        return importance
    
    def prune_heads(self, model, head_importance, n_heads_to_keep):
        """
        Prune les t√™tes d'attention les moins importantes
        """
        # N√©cessite modification de l'architecture attention
        # pour r√©duire num_heads dynamiquement
        pass

class TransformerLayerPruner:
    """
    Pruning de couches enti√®res dans un Transformer
    """
    
    def prune_transformer_layers(self, model, n_layers_to_remove):
        """
        Supprime les n derni√®res couches (ou les moins importantes)
        """
        # Les Transformers sont souvent sur-param√©tr√©s en profondeur
        # Supprimer quelques couches peut √™tre efficace
        pass
```

---

## Exercices

### Exercice 8.2.1
Impl√©mentez un algorithme de filter pruning qui privil√©gie les filtres produisant des activations proches de z√©ro.

### Exercice 8.2.2
Cr√©ez une fonction qui propage automatiquement le channel pruning √† travers un r√©seau s√©quentiel.

### Exercice 8.2.3
Comparez filter pruning et channel pruning sur un ResNet. Lequel pr√©serve mieux les performances ?

---

## Points Cl√©s √† Retenir

> üìå **Le pruning structur√© donne une acc√©l√©ration r√©elle sur hardware standard**

> üìå **Le filter pruning est plus simple mais le channel pruning peut √™tre plus efficace**

> üìå **La propagation du pruning entre couches est cruciale pour la coh√©rence**

> üìå **Le layer pruning donne les plus fortes acc√©l√©rations mais r√©duit la flexibilit√©**

---

*Section suivante : [8.3 Pruning Dynamique et Adaptatif](./08_03_Dynamique.md)*

