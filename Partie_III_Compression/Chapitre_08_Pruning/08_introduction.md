# Chapitre 8 : Techniques de Pruning (Ã‰lagage)

---

## Introduction

Le **pruning** (Ã©lagage) consiste Ã  supprimer les connexions ou neurones les moins importants d'un rÃ©seau de neurones. Cette technique peut rÃ©duire drastiquement la taille et le coÃ»t computationnel des modÃ¨les tout en prÃ©servant leurs performances.

---

## Plan du Chapitre

1. [Pruning Non StructurÃ©](./08_01_Non_Structure.md)
2. [Pruning StructurÃ©](./08_02_Structure.md)
3. [Pruning Dynamique et Adaptatif](./08_03_Dynamique.md)
4. [Lottery Ticket Hypothesis](./08_04_Lottery_Ticket.md)
5. [CritÃ¨res de SÃ©lection et Scheduling](./08_05_Criteres.md)

---

## Types de Pruning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Taxonomie du Pruning                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  NON STRUCTURÃ‰ (Fine-grained)                                  â”‚
â”‚  â”œâ”€â”€ Magnitude pruning                                         â”‚
â”‚  â”œâ”€â”€ Gradient-based pruning                                    â”‚
â”‚  â””â”€â”€ Second-order pruning (OBS, OBD)                          â”‚
â”‚                                                                 â”‚
â”‚  STRUCTURÃ‰ (Coarse-grained)                                    â”‚
â”‚  â”œâ”€â”€ Filter pruning (CNN)                                      â”‚
â”‚  â”œâ”€â”€ Channel pruning                                           â”‚
â”‚  â”œâ”€â”€ Head pruning (Transformers)                               â”‚
â”‚  â””â”€â”€ Layer pruning                                             â”‚
â”‚                                                                 â”‚
â”‚  DYNAMIQUE                                                      â”‚
â”‚  â”œâ”€â”€ Pruning pendant l'infÃ©rence                               â”‚
â”‚  â””â”€â”€ Input-dependent pruning                                   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Pruning par Magnitude

```python
import torch
import torch.nn as nn
import numpy as np

class MagnitudePruning:
    """
    Pruning basÃ© sur la magnitude des poids
    
    HypothÃ¨se: les poids de faible magnitude contribuent peu
    """
    
    def __init__(self, model):
        self.model = model
        self.masks = {}
        
    def compute_threshold(self, sparsity):
        """
        Calcule le seuil pour atteindre une sparsitÃ© donnÃ©e
        """
        all_weights = []
        for name, param in self.model.named_parameters():
            if 'weight' in name:
                all_weights.append(param.data.abs().flatten())
        
        all_weights = torch.cat(all_weights)
        threshold = torch.quantile(all_weights, sparsity)
        
        return threshold
    
    def create_masks(self, sparsity):
        """
        CrÃ©e les masques de pruning
        """
        threshold = self.compute_threshold(sparsity)
        
        for name, param in self.model.named_parameters():
            if 'weight' in name:
                mask = (param.data.abs() >= threshold).float()
                self.masks[name] = mask
        
        return self.masks
    
    def apply_masks(self):
        """
        Applique les masques aux poids
        """
        for name, param in self.model.named_parameters():
            if name in self.masks:
                param.data *= self.masks[name]
    
    def get_sparsity(self):
        """
        Calcule la sparsitÃ© rÃ©elle du modÃ¨le
        """
        total = 0
        zeros = 0
        
        for name, param in self.model.named_parameters():
            if 'weight' in name:
                total += param.numel()
                zeros += (param.data == 0).sum().item()
        
        return zeros / total

# Exemple d'utilisation
model = nn.Sequential(
    nn.Linear(784, 512),
    nn.ReLU(),
    nn.Linear(512, 256),
    nn.ReLU(),
    nn.Linear(256, 10)
)

pruner = MagnitudePruning(model)
pruner.create_masks(sparsity=0.9)  # 90% des poids Ã  zÃ©ro
pruner.apply_masks()

print(f"SparsitÃ© atteinte: {pruner.get_sparsity():.1%}")
```

---

## Pruning StructurÃ©

```python
class StructuredPruning:
    """
    Pruning structurÃ©: supprime des structures entiÃ¨res (filtres, neurones)
    
    Avantage: accÃ©lÃ©ration rÃ©elle sans hardware spÃ©cialisÃ©
    """
    
    @staticmethod
    def prune_filters(conv_layer, n_filters_to_keep, criterion='l1'):
        """
        Ã‰lague des filtres entiers d'une couche conv
        """
        weight = conv_layer.weight.data
        n_filters = weight.shape[0]
        
        # Calcul de l'importance de chaque filtre
        if criterion == 'l1':
            importance = weight.abs().sum(dim=(1, 2, 3))
        elif criterion == 'l2':
            importance = weight.pow(2).sum(dim=(1, 2, 3)).sqrt()
        
        # Garde les filtres les plus importants
        _, indices = torch.topk(importance, n_filters_to_keep)
        indices = indices.sort()[0]
        
        # CrÃ©e une nouvelle couche
        new_conv = nn.Conv2d(
            conv_layer.in_channels,
            n_filters_to_keep,
            conv_layer.kernel_size,
            stride=conv_layer.stride,
            padding=conv_layer.padding,
            bias=conv_layer.bias is not None
        )
        
        new_conv.weight.data = weight[indices]
        if conv_layer.bias is not None:
            new_conv.bias.data = conv_layer.bias.data[indices]
        
        return new_conv, indices
    
    @staticmethod
    def prune_neurons(linear_layer, n_neurons_to_keep, criterion='l1'):
        """
        Ã‰lague des neurones entiers d'une couche linÃ©aire
        """
        weight = linear_layer.weight.data
        
        # Importance basÃ©e sur les poids sortants
        if criterion == 'l1':
            importance = weight.abs().sum(dim=1)
        elif criterion == 'l2':
            importance = weight.pow(2).sum(dim=1).sqrt()
        
        # Garde les neurones les plus importants
        _, indices = torch.topk(importance, n_neurons_to_keep)
        indices = indices.sort()[0]
        
        # Nouvelle couche
        new_linear = nn.Linear(
            linear_layer.in_features,
            n_neurons_to_keep,
            bias=linear_layer.bias is not None
        )
        
        new_linear.weight.data = weight[indices]
        if linear_layer.bias is not None:
            new_linear.bias.data = linear_layer.bias.data[indices]
        
        return new_linear, indices

# Exemple
conv = nn.Conv2d(64, 128, 3, padding=1)
new_conv, kept_indices = StructuredPruning.prune_filters(conv, 64)

print(f"Filtres: {conv.weight.shape[0]} â†’ {new_conv.weight.shape[0]}")
print(f"ParamÃ¨tres: {conv.weight.numel():,} â†’ {new_conv.weight.numel():,}")
```

---

## Iterative Pruning avec Fine-tuning

```python
class IterativePruning:
    """
    Pruning itÃ©ratif: prune graduellement avec fine-tuning
    
    Plus stable que le pruning one-shot pour des sparsitÃ©s Ã©levÃ©es
    """
    
    def __init__(self, model, train_loader, val_loader, criterion):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.criterion = criterion
        
    def prune_and_finetune(self, 
                           target_sparsity, 
                           n_iterations=10,
                           finetune_epochs=5):
        """
        Pruning itÃ©ratif avec fine-tuning entre chaque Ã©tape
        """
        current_sparsity = 0
        sparsity_per_iter = target_sparsity / n_iterations
        
        history = {'sparsity': [], 'accuracy': []}
        
        for iteration in range(n_iterations):
            current_sparsity += sparsity_per_iter
            
            # Prune
            pruner = MagnitudePruning(self.model)
            pruner.create_masks(current_sparsity)
            pruner.apply_masks()
            
            # Fine-tune
            self._finetune(finetune_epochs, pruner.masks)
            
            # Ã‰value
            accuracy = self._evaluate()
            
            history['sparsity'].append(current_sparsity)
            history['accuracy'].append(accuracy)
            
            print(f"Iteration {iteration+1}: "
                  f"Sparsity={current_sparsity:.1%}, Accuracy={accuracy:.2%}")
        
        return history
    
    def _finetune(self, epochs, masks):
        """Fine-tune le modÃ¨le en gardant les masques"""
        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)
        
        for epoch in range(epochs):
            self.model.train()
            for x, y in self.train_loader:
                optimizer.zero_grad()
                output = self.model(x)
                loss = self.criterion(output, y)
                loss.backward()
                
                # Applique les masques aux gradients
                for name, param in self.model.named_parameters():
                    if name in masks and param.grad is not None:
                        param.grad *= masks[name]
                
                optimizer.step()
                
                # RÃ©applique les masques aux poids
                for name, param in self.model.named_parameters():
                    if name in masks:
                        param.data *= masks[name]
    
    def _evaluate(self):
        """Ã‰value la prÃ©cision"""
        self.model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for x, y in self.val_loader:
                output = self.model(x)
                pred = output.argmax(dim=1)
                correct += (pred == y).sum().item()
                total += y.size(0)
        
        return correct / total
```

---

## Lottery Ticket Hypothesis

```python
class LotteryTicketFinder:
    """
    ImplÃ©mentation de la Lottery Ticket Hypothesis
    
    HypothÃ¨se: Un rÃ©seau dense contient un sous-rÃ©seau sparse
    qui, entraÃ®nÃ© depuis les mÃªmes poids initiaux, atteint
    une performance comparable.
    """
    
    def __init__(self, model_fn):
        self.model_fn = model_fn
        
    def find_winning_ticket(self, 
                           train_fn,
                           target_sparsity=0.9,
                           n_rounds=10):
        """
        Trouve un "winning ticket" par pruning itÃ©ratif
        """
        # CrÃ©e le modÃ¨le et sauvegarde les poids initiaux
        model = self.model_fn()
        initial_weights = {name: param.clone() 
                          for name, param in model.named_parameters()}
        
        masks = {name: torch.ones_like(param) 
                for name, param in model.named_parameters() 
                if 'weight' in name}
        
        sparsity_per_round = 1 - (1 - target_sparsity) ** (1 / n_rounds)
        
        for round_idx in range(n_rounds):
            # EntraÃ®ne le modÃ¨le
            train_fn(model)
            
            # Prune les poids de plus faible magnitude
            for name, param in model.named_parameters():
                if name in masks:
                    # Poids actuels masquÃ©s
                    masked_weights = param.data * masks[name]
                    
                    # Seuil pour ce round
                    nonzero = masked_weights[masks[name] == 1]
                    threshold = torch.quantile(nonzero.abs(), sparsity_per_round)
                    
                    # Met Ã  jour le masque
                    new_mask = (masked_weights.abs() >= threshold).float()
                    masks[name] *= new_mask
            
            # RÃ©initialise aux poids initiaux
            for name, param in model.named_parameters():
                if name in initial_weights:
                    param.data = initial_weights[name].clone()
                    if name in masks:
                        param.data *= masks[name]
            
            current_sparsity = self._compute_sparsity(masks)
            print(f"Round {round_idx+1}: Sparsity = {current_sparsity:.1%}")
        
        return model, masks, initial_weights
    
    def _compute_sparsity(self, masks):
        total = sum(m.numel() for m in masks.values())
        zeros = sum((m == 0).sum().item() for m in masks.values())
        return zeros / total
```

---

## Pruning pour Physique des Particules

```python
class PhysicsPruning:
    """
    StratÃ©gies de pruning spÃ©cifiques Ã  la physique des particules
    """
    
    @staticmethod
    def sensitivity_analysis(model, val_loader, criterion):
        """
        Analyse de sensibilitÃ©: identifie les couches les plus importantes
        """
        sensitivities = {}
        
        baseline_loss = evaluate_loss(model, val_loader, criterion)
        
        for name, module in model.named_modules():
            if isinstance(module, (nn.Linear, nn.Conv2d)):
                # Sauvegarde les poids
                original_weight = module.weight.data.clone()
                
                # Teste diffÃ©rents niveaux de pruning
                for sparsity in [0.5, 0.7, 0.9]:
                    # Prune temporairement
                    threshold = torch.quantile(module.weight.abs(), sparsity)
                    mask = (module.weight.abs() >= threshold).float()
                    module.weight.data *= mask
                    
                    # Ã‰value
                    pruned_loss = evaluate_loss(model, val_loader, criterion)
                    
                    # Restaure
                    module.weight.data = original_weight.clone()
                    
                    key = f"{name}_sparsity_{sparsity}"
                    sensitivities[key] = pruned_loss - baseline_loss
        
        return sensitivities
    
    @staticmethod
    def latency_aware_pruning(model, target_latency, hardware_model):
        """
        Pruning guidÃ© par la latence cible
        
        Utilise un modÃ¨le de coÃ»t hardware pour optimiser
        """
        # Estime la latence de chaque couche
        layer_latencies = {}
        for name, module in model.named_modules():
            if isinstance(module, (nn.Linear, nn.Conv2d)):
                layer_latencies[name] = hardware_model.estimate_latency(module)
        
        # Optimise la sparsitÃ© par couche pour atteindre la latence cible
        # (Algorithme glouton ou programmation dynamique)
        pass
```

---

## Exercices

### Exercice 8.1
ImplÃ©mentez le pruning basÃ© sur le gradient (gradient-based pruning) et comparez-le au magnitude pruning.

### Exercice 8.2
CrÃ©ez une fonction qui prune automatiquement un modÃ¨le pour atteindre une latence cible sur GPU.

### Exercice 8.3
Reproduisez l'expÃ©rience Lottery Ticket sur un petit rÃ©seau pour MNIST.

---

## Points ClÃ©s Ã  Retenir

> ğŸ“Œ **Le pruning non structurÃ© atteint de hautes sparsitÃ©s mais nÃ©cessite du hardware spÃ©cialisÃ©**

> ğŸ“Œ **Le pruning structurÃ© donne des accÃ©lÃ©rations rÃ©elles sur hardware standard**

> ğŸ“Œ **Le fine-tuning aprÃ¨s pruning est crucial pour maintenir les performances**

> ğŸ“Œ **La Lottery Ticket Hypothesis suggÃ¨re que les rÃ©seaux sont sur-paramÃ©trÃ©s**

---

*Chapitre suivant : [Chapitre 9 - Quantification des RÃ©seaux de Neurones](../Chapitre_09_Quantification/09_introduction.md)*

