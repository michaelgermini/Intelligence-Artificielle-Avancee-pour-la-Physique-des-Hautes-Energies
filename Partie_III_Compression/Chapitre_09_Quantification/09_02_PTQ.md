# 9.2 Post-Training Quantization (PTQ)

---

## Introduction

La **Post-Training Quantization (PTQ)** quantifie un mod√®le d√©j√† entra√Æn√© sans n√©cessiter d'entra√Ænement suppl√©mentaire. C'est la m√©thode la plus rapide pour d√©ployer un mod√®le quantifi√©, mais peut entra√Æner une perte de pr√©cision.

---

## Principe

### √âtapes de PTQ

1. **Entra√Æner** le mod√®le en FP32
2. **Calibrer** les param√®tres de quantification sur un ensemble de calibration
3. **Quantifier** les poids et activations
4. **√âvaluer** la performance

---

## Calibration

### M√©thodes de Calibration

```python
import torch
import torch.nn as nn
import numpy as np
from collections import defaultdict

class CalibrationMethod:
    """M√©thodes de calibration pour PTQ"""
    
    @staticmethod
    def min_max(x):
        """
        Min-Max: utilise min et max observ√©s
        
        scale = (max - min) / (q_max - q_min)
        zero_point = q_min - round(min / scale)
        """
        x_min = x.min().item()
        x_max = x.max().item()
        return x_min, x_max
    
    @staticmethod
    def kl_divergence(x, n_bits=8, bins=2048):
        """
        Calibration par minimisation de la divergence KL
        
        Trouve les seuils qui minimisent la divergence entre
        distributions FP32 et quantifi√©e
        """
        # Histogramme des valeurs
        hist, bin_edges = np.histogram(x.cpu().numpy().flatten(), bins=bins)
        hist = hist / hist.sum()
        
        # Cherche le seuil optimal
        best_threshold = None
        best_kl = float('inf')
        
        for i in range(1, bins - 1):
            threshold = bin_edges[i]
            
            # Calcule la quantification avec ce seuil
            scale = threshold / (2 ** (n_bits - 1) - 1)
            
            # Quantifie
            x_q = torch.clamp(torch.round(x / scale), 
                            -2**(n_bits-1), 2**(n_bits-1)-1)
            x_recon = x_q * scale
            
            # Histogramme de la reconstruction
            hist_recon, _ = np.histogram(x_recon.cpu().numpy().flatten(), 
                                       bins=bin_edges)
            hist_recon = hist_recon / (hist_recon.sum() + 1e-10)
            
            # KL divergence
            kl = np.sum(hist * np.log(hist / (hist_recon + 1e-10) + 1e-10))
            
            if kl < best_kl:
                best_kl = kl
                best_threshold = threshold
        
        return -best_threshold, best_threshold
    
    @staticmethod
    def percentile(x, percentile=99.99):
        """
        Utilise un percentile au lieu du max absolu
        
        Plus robuste aux outliers
        """
        x_min = x.min().item()
        x_max = torch.quantile(x, percentile / 100.0).item()
        return x_min, x_max
    
    @staticmethod
    def moving_average(x, alpha=0.9):
        """
        Moyenne mobile exponentielle
        
        Utile pour le calibrage en ligne
        """
        # Simplifi√© - n√©cessite √©tat global
        return x.min().item(), x.max().item()

# Test des m√©thodes
x_test = torch.randn(1000) * 5.0

print("M√©thodes de Calibration:")
print(f"  Min-Max: {CalibrationMethod.min_max(x_test)}")
print(f"  Percentile 99.99: {CalibrationMethod.percentile(x_test)}")
```

---

## Impl√©mentation PTQ Compl√®te

```python
class PostTrainingQuantization:
    """
    Syst√®me complet de PTQ
    """
    
    def __init__(self, model, n_bits=8, calibration_method='min_max'):
        """
        Args:
            model: mod√®le PyTorch pr√©-entra√Æn√©
            n_bits: pr√©cision de quantification
            calibration_method: 'min_max', 'kl_divergence', 'percentile'
        """
        self.model = model
        self.n_bits = n_bits
        self.calibration_method = calibration_method
        
        self.activation_stats = defaultdict(list)
        self.weight_stats = {}
        self.quantization_params = {}
        
        # Hooks pour capturer les activations
        self.hooks = []
    
    def _register_hooks(self):
        """Enregistre les hooks pour capturer les activations"""
        def hook_fn(name):
            def hook(module, input, output):
                if isinstance(output, torch.Tensor):
                    self.activation_stats[name].append(output.detach())
            return hook
        
        for name, module in self.model.named_modules():
            if isinstance(module, (nn.Linear, nn.Conv2d)):
                hook = module.register_forward_hook(hook_fn(name))
                self.hooks.append(hook)
    
    def _remove_hooks(self):
        """Supprime les hooks"""
        for hook in self.hooks:
            hook.remove()
        self.hooks.clear()
    
    def calibrate(self, calibration_data, num_batches=None):
        """
        Calibre les param√®tres de quantification
        
        Args:
            calibration_data: DataLoader ou liste d'entr√©es
            num_batches: nombre de batches √† utiliser (None = tous)
        """
        self.model.eval()
        self._register_hooks()
        
        # Collecte les statistiques
        batch_count = 0
        with torch.no_grad():
            for batch in calibration_data:
                if isinstance(batch, (list, tuple)):
                    inputs = batch[0]
                else:
                    inputs = batch
                
                _ = self.model(inputs)
                
                batch_count += 1
                if num_batches is not None and batch_count >= num_batches:
                    break
        
        # Calcule les param√®tres de quantification pour les activations
        for name, activations in self.activation_stats.items():
            # Concat√®ne toutes les activations
            all_activations = torch.cat(activations, dim=0)
            
            # Calcule min/max selon la m√©thode choisie
            if self.calibration_method == 'min_max':
                x_min, x_max = CalibrationMethod.min_max(all_activations)
            elif self.calibration_method == 'percentile':
                x_min, x_max = CalibrationMethod.percentile(all_activations)
            elif self.calibration_method == 'kl_divergence':
                x_min, x_max = CalibrationMethod.kl_divergence(
                    all_activations, n_bits=self.n_bits
                )
            else:
                x_min, x_max = CalibrationMethod.min_max(all_activations)
            
            # Calcule scale et zero_point
            q_max = 2 ** (self.n_bits - 1) - 1
            scale = (x_max - x_min) / (2 * q_max)
            zero_point = 0  # Sym√©trique
            
            self.quantization_params[f'activation_{name}'] = {
                'scale': scale,
                'zero_point': zero_point,
                'min': x_min,
                'max': x_max
            }
        
        # Calcule les param√®tres pour les poids
        for name, param in self.model.named_parameters():
            if 'weight' in name:
                x_min, x_max = CalibrationMethod.min_max(param.data)
                
                q_max = 2 ** (self.n_bits - 1) - 1
                scale = max(abs(x_min), abs(x_max)) / q_max
                zero_point = 0
                
                self.quantization_params[f'weight_{name}'] = {
                    'scale': scale,
                    'zero_point': zero_point
                }
        
        self._remove_hooks()
        return self.quantization_params
    
    def quantize_model(self):
        """
        Cr√©e une version quantifi√©e du mod√®le
        """
        quantized_model = self.model  # En pratique, cr√©e une copie
        
        # Applique la quantification aux poids
        for name, module in quantized_model.named_modules():
            if isinstance(module, (nn.Linear, nn.Conv2d)):
                weight_params = self.quantization_params.get(f'weight_{name}')
                if weight_params:
                    # Quantifie les poids
                    scale = weight_params['scale']
                    zp = weight_params['zero_point']
                    
                    # Quantification (en pratique, stocke les poids quantifi√©s)
                    q_weight = torch.round(module.weight.data / scale + zp)
                    q_weight = torch.clamp(q_weight, -2**(self.n_bits-1), 
                                         2**(self.n_bits-1)-1)
                    
                    # Remplace les poids (simplifi√©)
                    # module.weight.data = (q_weight.float() - zp) * scale
        
        return quantized_model

# Exemple d'utilisation
"""
model = load_pretrained_model()
calibration_loader = get_calibration_dataloader(num_samples=500)

ptq = PostTrainingQuantization(model, n_bits=8, calibration_method='min_max')
params = ptq.calibrate(calibration_loader, num_batches=10)
quantized_model = ptq.quantize_model()
"""
```

---

## Optimisations Avanc√©es

### Cross-Layer Equalization

```python
def cross_layer_equalization(conv1, conv2):
    """
    Equalise les distributions entre couches adjacentes
    
    R√©duit la diff√©rence de ranges entre couches
    """
    # Trouve les scales pour √©galiser
    # (Simplifi√© - impl√©mentation compl√®te est complexe)
    
    # Id√©e: ajuste les poids de conv1 et conv2 pour que
    # les activations aient des ranges similaires
    
    # Scale factor pour conv1
    scale1 = torch.mean(torch.abs(conv1.weight))
    scale2 = torch.mean(torch.abs(conv2.weight))
    
    # √âgalise
    equalization_scale = torch.sqrt(scale2 / scale1)
    
    conv1.weight.data = conv1.weight.data * equalization_scale
    conv2.weight.data = conv2.weight.data / equalization_scale
    
    # Ajuste les biases si pr√©sents
    if conv1.bias is not None:
        conv1.bias.data = conv1.bias.data * equalization_scale

# Applique avant quantification
# cross_layer_equalization(conv_layer1, conv_layer2)
```

### Bias Correction

```python
def bias_correction(model, calibration_data, quantization_params):
    """
    Corrige les biais pour compenser l'erreur de quantification
    
    L'erreur de quantification introduit un biais syst√©matique
    """
    model.eval()
    
    # Collecte les erreurs de quantification par couche
    with torch.no_grad():
        for batch in calibration_data:
            inputs = batch[0] if isinstance(batch, (list, tuple)) else batch
            
            # Forward avec et sans quantification
            output_fp32 = model(inputs)
            # output_quantized = quantized_forward(model, inputs, quantization_params)
            
            # Calcule l'erreur moyenne
            # error = output_fp32 - output_quantized
            
            # Ajuste les biais pour compenser
            # (Simplifi√©)
            pass
```

---

## √âvaluation PTQ

```python
def evaluate_ptq(model_fp32, model_quantized, test_loader):
    """
    √âvalue la performance apr√®s PTQ
    """
    def evaluate_model(model):
        model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in test_loader:
                output = model(data)
                pred = output.argmax(dim=1)
                correct += pred.eq(target).sum().item()
                total += target.size(0)
        
        return 100.0 * correct / total
    
    acc_fp32 = evaluate_model(model_fp32)
    acc_quantized = evaluate_model(model_quantized)
    
    degradation = acc_fp32 - acc_quantized
    
    print(f"√âvaluation PTQ:")
    print(f"  FP32 Accuracy: {acc_fp32:.2f}%")
    print(f"  Quantized Accuracy: {acc_quantized:.2f}%")
    print(f"  Degradation: {degradation:.2f}%")
    
    return {
        'fp32_acc': acc_fp32,
        'quantized_acc': acc_quantized,
        'degradation': degradation
    }
```

---

## Limitations et Bonnes Pratiques

### Limitations

- **Perte de pr√©cision** : peut √™tre significative pour certains mod√®les
- **Pas d'adaptation** : le mod√®le ne s'adapte pas √† la quantification
- **D√©pend de la calibration** : qualit√© d√©pend des donn√©es de calibration

### Bonnes Pratiques

```python
def ptq_best_practices():
    """
    Recommandations pour PTQ
    """
    practices = [
        "Utiliser au moins 100-1000 √©chantillons de calibration",
        "Repr√©senter la distribution r√©elle des donn√©es",
        "Essayer diff√©rentes m√©thodes de calibration (min-max, KL, percentile)",
        "Quantifier poids ET activations (pas seulement les poids)",
        "Utiliser per-channel pour les poids de convolution",
        "Appliquer cross-layer equalization pour r√©duire la perte",
        "V√©rifier la performance sur un ensemble de validation",
        "Consid√©rer QAT si la d√©gradation est > 2-3%"
    ]
    
    print("Bonnes Pratiques PTQ:")
    for i, practice in enumerate(practices, 1):
        print(f"  {i}. {practice}")

ptq_best_practices()
```

---

## Exercices

### Exercice 9.2.1
Impl√©mentez une calibration par percentile adaptatif qui trouve automatiquement le meilleur percentile.

### Exercice 9.2.2
Comparez les r√©sultats de PTQ avec diff√©rentes m√©thodes de calibration sur un mod√®le r√©el (ResNet, MobileNet).

### Exercice 9.2.3
Impl√©mentez cross-layer equalization pour une s√©quence de couches convolutionnelles.

---

## Points Cl√©s √† Retenir

> üìå **PTQ est rapide mais peut entra√Æner une perte de pr√©cision significative**

> üìå **La calibration est cruciale - utilisez des donn√©es repr√©sentatives**

> üìå **Min-Max est simple mais KL divergence peut donner de meilleurs r√©sultats**

> üìå **Cross-layer equalization r√©duit la perte de pr√©cision**

> üìå **Per-channel quantization am√©liore g√©n√©ralement les r√©sultats**

---

*Section suivante : [9.3 Quantization-Aware Training (QAT)](./09_03_QAT.md)*

