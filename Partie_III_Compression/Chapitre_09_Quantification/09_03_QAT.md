# 9.3 Quantization-Aware Training (QAT)

---

## Introduction

La **Quantization-Aware Training (QAT)** intÃ¨gre la quantification dans le processus d'entraÃ®nement, permettant au modÃ¨le de s'adapter aux effets de la quantification et d'atteindre de meilleures performances que PTQ.

---

## Principe

### IdÃ©e Fondamentale

Au lieu de quantifier aprÃ¨s entraÃ®nement, on simule la quantification pendant l'entraÃ®nement :
- **Forward** : Quantifie puis dÃ©quantifie (simulation)
- **Backward** : Gradients passent Ã  travers (Straight-Through Estimator)

---

## Fake Quantization

### ImplÃ©mentation

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class FakeQuantize(torch.autograd.Function):
    """
    Fake quantization: simule la quantification
    
    Forward: quantize + dequantize
    Backward: Straight-Through Estimator (gradient passe tel quel)
    """
    
    @staticmethod
    def forward(ctx, x, scale, zero_point, q_min, q_max):
        """
        Args:
            x: tenseur Ã  quantifier
            scale: pas de quantification
            zero_point: point zÃ©ro
            q_min, q_max: bornes de la plage quantifiÃ©e
        """
        # Quantifie
        q = torch.round(x / scale + zero_point)
        q = torch.clamp(q, q_min, q_max)
        
        # DÃ©quantifie (c'est la valeur utilisÃ©e)
        x_q = (q - zero_point) * scale
        
        ctx.save_for_backward(x, scale, zero_point, q_min, q_max)
        
        return x_q
    
    @staticmethod
    def backward(ctx, grad_output):
        """
        Straight-Through Estimator: gradient passe inchangÃ©
        """
        x, scale, zero_point, q_min, q_max = ctx.saved_tensors
        
        # Gradient par rapport Ã  x: passe tel quel
        grad_x = grad_output.clone()
        
        # Pas de gradient pour les autres paramÃ¨tres
        return grad_x, None, None, None, None


class FakeQuantizeModule(nn.Module):
    """
    Module PyTorch pour fake quantization
    """
    
    def __init__(self, n_bits=8, symmetric=True, per_channel=False, channel_dim=0):
        super().__init__()
        
        self.n_bits = n_bits
        self.symmetric = symmetric
        self.per_channel = per_channel
        self.channel_dim = channel_dim
        
        if symmetric:
            self.q_min = -(2 ** (n_bits - 1))
            self.q_max = 2 ** (n_bits - 1) - 1
            self.zero_point = 0
        else:
            self.q_min = 0
            self.q_max = 2 ** n_bits - 1
        
        # Scale comme paramÃ¨tre apprenable ou fixe
        if per_channel:
            # Scale par canal (initialisÃ© plus tard selon la shape)
            self.register_buffer('scale', None)
        else:
            # Scale global
            self.register_buffer('scale', torch.tensor(1.0))
    
    def forward(self, x):
        if self.scale is None:
            # Initialise le scale si per_channel
            self._initialize_scale(x)
        
        # Calcule scale et zero_point
        if self.per_channel:
            # Scale par canal
            if self.channel_dim != 0:
                x = x.transpose(0, self.channel_dim)
            
            scales = self._compute_per_channel_scale(x)
            zero_points = torch.zeros(x.shape[0], device=x.device, dtype=torch.int32)
            
            # Quantifie canal par canal
            x_q = torch.zeros_like(x)
            for c in range(x.shape[0]):
                x_q[c] = FakeQuantize.apply(
                    x[c], scales[c], zero_points[c], 
                    self.q_min, self.q_max
                )
            
            if self.channel_dim != 0:
                x_q = x_q.transpose(0, self.channel_dim)
        else:
            # Scale global
            scale = self.scale if self.scale.numel() == 1 else self._compute_scale(x)
            x_q = FakeQuantize.apply(x, scale, self.zero_point, self.q_min, self.q_max)
        
        return x_q
    
    def _initialize_scale(self, x):
        """Initialise le scale pour per_channel"""
        if self.channel_dim != 0:
            num_channels = x.shape[self.channel_dim]
        else:
            num_channels = x.shape[0]
        self.scale = torch.ones(num_channels, device=x.device)
    
    def _compute_scale(self, x):
        """Calcule le scale optimal"""
        if self.symmetric:
            x_max = x.abs().max()
            return x_max / self.q_max
        else:
            x_min, x_max = x.min(), x.max()
            return (x_max - x_min) / (self.q_max - self.q_min)
    
    def _compute_per_channel_scale(self, x):
        """Calcule les scales par canal"""
        if self.symmetric:
            x_max = x.abs().max(dim=-1)[0]
            scales = x_max / self.q_max
        else:
            x_min = x.min(dim=-1)[0]
            x_max = x.max(dim=-1)[0]
            scales = (x_max - x_min) / (self.q_max - self.q_min)
        return scales.clamp(min=1e-8)
```

---

## Couches QAT

### Linear QAT

```python
class QATLinear(nn.Module):
    """
    Couche linÃ©aire avec QAT
    """
    
    def __init__(self, in_features, out_features, bias=True, 
                 n_bits_weight=8, n_bits_activation=8):
        super().__init__()
        
        self.linear = nn.Linear(in_features, out_features, bias=bias)
        
        # Fake quantization pour poids et activations
        self.weight_fake_quant = FakeQuantizeModule(
            n_bits=n_bits_weight, symmetric=True
        )
        self.activation_fake_quant = FakeQuantizeModule(
            n_bits=n_bits_activation, symmetric=False
        )
    
    def forward(self, x):
        # Quantifie les activations d'entrÃ©e
        x_q = self.activation_fake_quant(x)
        
        # Quantifie les poids
        w_q = self.weight_fake_quant(self.linear.weight)
        
        # Forward avec poids quantifiÃ©s
        output = F.linear(x_q, w_q, self.linear.bias)
        
        # Quantifie les activations de sortie
        output_q = self.activation_fake_quant(output)
        
        return output_q

# Exemple
qat_linear = QATLinear(784, 256, n_bits_weight=8, n_bits_activation=8)
```

### Conv2d QAT

```python
class QATConv2d(nn.Module):
    """
    Convolution 2D avec QAT
    """
    
    def __init__(self, in_channels, out_channels, kernel_size, 
                 stride=1, padding=0, bias=True,
                 n_bits_weight=8, n_bits_activation=8):
        super().__init__()
        
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size,
                             stride=stride, padding=padding, bias=bias)
        
        # Per-channel quantization pour les poids
        self.weight_fake_quant = FakeQuantizeModule(
            n_bits=n_bits_weight, symmetric=True, 
            per_channel=True, channel_dim=0
        )
        self.activation_fake_quant = FakeQuantizeModule(
            n_bits=n_bits_activation, symmetric=False
        )
    
    def forward(self, x):
        # Quantifie les activations d'entrÃ©e
        x_q = self.activation_fake_quant(x)
        
        # Quantifie les poids (per-channel)
        w_q = self.weight_fake_quant(self.conv.weight)
        
        # Forward avec poids quantifiÃ©s
        output = F.conv2d(x_q, w_q, self.conv.bias,
                         stride=self.conv.stride,
                         padding=self.conv.padding)
        
        # Quantifie les activations de sortie
        output_q = self.activation_fake_quant(output)
        
        return output_q
```

---

## Conversion de ModÃ¨le pour QAT

```python
def convert_to_qat(model, n_bits_weight=8, n_bits_activation=8):
    """
    Convertit un modÃ¨le standard en version QAT
    """
    qat_model = model  # En pratique, crÃ©er une copie
    
    for name, module in qat_model.named_modules():
        if isinstance(module, nn.Linear):
            # Remplace par QATLinear
            qat_module = QATLinear(
                module.in_features,
                module.out_features,
                bias=module.bias is not None,
                n_bits_weight=n_bits_weight,
                n_bits_activation=n_bits_activation
            )
            
            # Copie les poids
            qat_module.linear.weight.data = module.weight.data.clone()
            if module.bias is not None:
                qat_module.linear.bias.data = module.bias.data.clone()
            
            # Remplace dans le modÃ¨le (nÃ©cessite logique de remplacement rÃ©cursive)
            # setattr(parent_module, child_name, qat_module)
        
        elif isinstance(module, nn.Conv2d):
            # Remplace par QATConv2d
            qat_module = QATConv2d(
                module.in_channels,
                module.out_channels,
                module.kernel_size,
                stride=module.stride,
                padding=module.padding,
                bias=module.bias is not None,
                n_bits_weight=n_bits_weight,
                n_bits_activation=n_bits_activation
            )
            
            qat_module.conv.weight.data = module.weight.data.clone()
            if module.bias is not None:
                qat_module.conv.bias.data = module.bias.data.clone()
    
    return qat_model
```

---

## EntraÃ®nement QAT

### StratÃ©gie d'EntraÃ®nement

```python
class QATTrainer:
    """
    EntraÃ®neur pour QAT
    """
    
    def __init__(self, model, train_loader, val_loader, 
                 n_bits=8, lr=1e-3, epochs=20):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.n_bits = n_bits
        self.epochs = epochs
        
        # Convertit en QAT
        self.qat_model = convert_to_qat(model, n_bits_weight=n_bits,
                                        n_bits_activation=n_bits)
        
        self.optimizer = torch.optim.Adam(self.qat_model.parameters(), lr=lr)
        self.criterion = nn.CrossEntropyLoss()
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', patience=3
        )
    
    def train_epoch(self):
        """Un epoch d'entraÃ®nement"""
        self.qat_model.train()
        total_loss = 0.0
        
        for batch_idx, (data, target) in enumerate(self.train_loader):
            self.optimizer.zero_grad()
            
            output = self.qat_model(data)
            loss = self.criterion(output, target)
            loss.backward()
            
            # Gradient clipping pour stabilitÃ©
            torch.nn.utils.clip_grad_norm_(self.qat_model.parameters(), 1.0)
            
            self.optimizer.step()
            total_loss += loss.item()
        
        return total_loss / len(self.train_loader)
    
    def validate(self):
        """Validation"""
        self.qat_model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in self.val_loader:
                output = self.qat_model(data)
                pred = output.argmax(dim=1)
                correct += pred.eq(target).sum().item()
                total += target.size(0)
        
        return 100.0 * correct / total
    
    def train(self):
        """EntraÃ®nement complet"""
        best_acc = 0.0
        
        for epoch in range(self.epochs):
            train_loss = self.train_epoch()
            val_acc = self.validate()
            
            self.scheduler.step(train_loss)
            
            print(f'Epoch {epoch+1}/{self.epochs}: '
                  f'Loss={train_loss:.4f}, Acc={val_acc:.2f}%')
            
            if val_acc > best_acc:
                best_acc = val_acc
                torch.save(self.qat_model.state_dict(), 'best_qat_model.pt')
        
        return self.qat_model
```

---

## Freezing de BatchNorm

```python
def freeze_bn_for_qat(model):
    """
    Freeze les BatchNorm pour QAT
    
    Les BatchNorm peuvent interfÃ©rer avec la quantification
    """
    for module in model.modules():
        if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):
            module.eval()
            # DÃ©sactive les gradients
            for param in module.parameters():
                param.requires_grad = False
```

---

## Comparaison PTQ vs QAT

```python
def compare_ptq_vs_qat(fp32_model, train_loader, val_loader, n_bits=8):
    """
    Compare PTQ et QAT sur le mÃªme modÃ¨le
    """
    # PTQ
    calibration_data = [batch[0] for batch in list(train_loader)[:10]]
    ptq = PostTrainingQuantization(fp32_model, n_bits=n_bits)
    ptq.calibrate(calibration_data)
    ptq_model = ptq.quantize_model()
    
    ptq_acc = evaluate_accuracy(ptq_model, val_loader)
    
    # QAT
    qat_trainer = QATTrainer(fp32_model, train_loader, val_loader, 
                            n_bits=n_bits, epochs=10)
    qat_model = qat_trainer.train()
    qat_acc = evaluate_accuracy(qat_model, val_loader)
    
    # FP32 baseline
    fp32_acc = evaluate_accuracy(fp32_model, val_loader)
    
    print("Comparaison PTQ vs QAT:")
    print(f"  FP32: {fp32_acc:.2f}%")
    print(f"  PTQ:  {ptq_acc:.2f}% (degradation: {fp32_acc - ptq_acc:.2f}%)")
    print(f"  QAT:  {qat_acc:.2f}% (degradation: {fp32_acc - qat_acc:.2f}%)")
    
    return {
        'fp32': fp32_acc,
        'ptq': ptq_acc,
        'qat': qat_acc
    }

def evaluate_accuracy(model, data_loader):
    """Ã‰value l'accuracy d'un modÃ¨le"""
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data, target in data_loader:
            output = model(data)
            pred = output.argmax(dim=1)
            correct += pred.eq(target).sum().item()
            total += target.size(0)
    return 100.0 * correct / total
```

---

## Exercices

### Exercice 9.3.1
ImplÃ©mentez une variante de FakeQuantize qui utilise une fonction de gradient diffÃ©rente (pas seulement STE).

### Exercice 9.3.2
Comparez QAT avec et sans per-channel quantization pour les poids.

### Exercice 9.3.3
ExpÃ©rimentez avec diffÃ©rents learning rates pour QAT - comment affecte-t-il la convergence?

---

## Points ClÃ©s Ã  Retenir

> ğŸ“Œ **QAT simule la quantification pendant l'entraÃ®nement avec Straight-Through Estimator**

> ğŸ“Œ **QAT donne gÃ©nÃ©ralement de meilleurs rÃ©sultats que PTQ, surtout pour INT4/INT8**

> ğŸ“Œ **Per-channel quantization amÃ©liore les performances en QAT**

> ğŸ“Œ **Freezing BatchNorm peut amÃ©liorer la stabilitÃ©**

> ğŸ“Œ **QAT nÃ©cessite un entraÃ®nement supplÃ©mentaire mais vaut souvent le coÃ»t**

---

*Section suivante : [9.4 Quantification Mixte](./09_04_Mixed_Precision.md)*

