# 11.3 Low-Rank Adaptation (LoRA)

---

## Introduction

**LoRA** (Low-Rank Adaptation) est une mÃ©thode efficace pour adapter de grands modÃ¨les prÃ©-entraÃ®nÃ©s en n'entraÃ®nant que de petits adaptateurs de rang faible. Cela rÃ©duit drastiquement le nombre de paramÃ¨tres entraÃ®nables tout en prÃ©servant les performances.

---

## Principe

### IdÃ©e Fondamentale

Au lieu de fine-tuner tous les poids, LoRA ajoute une adaptation de rang faible :

$$\mathbf{W}_{\text{new}} = \mathbf{W}_0 + \alpha \cdot (\mathbf{B} \mathbf{A})$$

oÃ¹ :
- $\mathbf{W}_0$ : poids originaux (gelÃ©s)
- $\mathbf{B} \in \mathbb{R}^{d \times r}$, $\mathbf{A} \in \mathbb{R}^{r \times k}$ : adaptateurs (entraÃ®nables)
- $\alpha$ : facteur d'Ã©chelle

**ParamÃ¨tres entraÃ®nables** : $r(d + k)$ au lieu de $dk$

---

## ImplÃ©mentation de Base

```python
import torch
import torch.nn as nn
import numpy as np

class LoRALayer(nn.Module):
    """
    Couche avec adaptation LoRA
    """
    
    def __init__(self, original_layer, rank=16, alpha=1.0, dropout=0.0):
        """
        Args:
            original_layer: Couche originale (Linear, Conv2d, etc.)
            rank: Rang de l'adaptation (r)
            alpha: Facteur d'Ã©chelle
            dropout: Dropout pour les adaptateurs
        """
        super().__init__()
        
        self.original = original_layer
        self.rank = rank
        self.alpha = alpha
        
        # GÃ¨le les poids originaux
        for param in self.original.parameters():
            param.requires_grad = False
        
        # DÃ©termine les dimensions
        if isinstance(original_layer, nn.Linear):
            in_features = original_layer.in_features
            out_features = original_layer.out_features
            self.layer_type = 'linear'
        elif isinstance(original_layer, nn.Conv2d):
            in_features = original_layer.in_channels
            out_features = original_layer.out_channels
            self.layer_type = 'conv2d'
        else:
            raise ValueError(f"Unsupported layer type: {type(original_layer)}")
        
        # Adaptateurs LoRA
        # B @ A: (out_features, rank) @ (rank, in_features)
        self.lora_A = nn.Parameter(
            torch.randn(rank, in_features) / np.sqrt(in_features)
        )
        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))
        
        # Dropout
        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()
        
        # Initialisation
        # A: distribution normale
        # B: zÃ©ro (donc adaptation commence Ã  zÃ©ro)
    
    def forward(self, x):
        """
        Forward avec adaptation LoRA
        """
        # Forward original
        original_out = self.original(x)
        
        # Adaptation LoRA
        if self.layer_type == 'linear':
            # x @ A^T: (batch, rank)
            lora_intermediate = self.dropout(x) @ self.lora_A.T
            # lora_intermediate @ B^T: (batch, out_features)
            lora_out = lora_intermediate @ self.lora_B.T
        elif self.layer_type == 'conv2d':
            # Pour Conv2d, reshape nÃ©cessaire
            # (SimplifiÃ© - nÃ©cessite reshape appropriÃ©)
            lora_out = 0  # Placeholder
        
        # Combinaison
        return original_out + self.alpha * lora_out
    
    def merge_weights(self):
        """
        Fusionne les adaptateurs LoRA avec les poids originaux
        
        Utile pour infÃ©rence finale (Ã©vite de calculer sÃ©parÃ©ment)
        """
        if self.layer_type == 'linear':
            delta_W = self.alpha * (self.lora_B @ self.lora_A)
            self.original.weight.data += delta_W
        
        # AprÃ¨s fusion, les adaptateurs ne sont plus nÃ©cessaires
        self.lora_A = None
        self.lora_B = None
    
    def trainable_parameters(self):
        """Nombre de paramÃ¨tres entraÃ®nables"""
        return self.lora_A.numel() + self.lora_B.numel()
    
    def total_parameters(self):
        """Nombre total de paramÃ¨tres (originaux + LoRA)"""
        original_params = sum(p.numel() for p in self.original.parameters())
        return original_params + self.trainable_parameters()

# Exemple
base_layer = nn.Linear(768, 768)  # Type BERT
lora_layer = LoRALayer(base_layer, rank=16, alpha=1.0)

print("LoRA Layer:")
print(f"  ParamÃ¨tres originaux: {base_layer.weight.numel():,}")
print(f"  ParamÃ¨tres LoRA (entraÃ®nables): {lora_layer.trainable_parameters():,}")
print(f"  RÃ©duction: {1 - lora_layer.trainable_parameters() / base_layer.weight.numel():.2%}")
```

---

## Application Ã  un ModÃ¨le Complet

### Conversion de ModÃ¨le pour LoRA

```python
def convert_model_to_lora(model, target_modules=None, rank=16, alpha=1.0):
    """
    Convertit un modÃ¨le pour utiliser LoRA
    
    Args:
        model: ModÃ¨le Ã  convertir
        target_modules: Liste de noms de modules Ã  convertir (None = tous les Linear)
        rank: Rang LoRA
        alpha: Facteur d'Ã©chelle
    """
    if target_modules is None:
        target_modules = ['linear', 'dense']
    
    lora_model = model  # En pratique, crÃ©er une copie
    
    for name, module in lora_model.named_modules():
        # VÃ©rifie si le module doit Ãªtre converti
        should_convert = any(target in name.lower() for target in target_modules)
        
        if should_convert and isinstance(module, (nn.Linear, nn.Conv2d)):
            # Remplace par LoRA
            lora_module = LoRALayer(module, rank=rank, alpha=alpha)
            
            # Remplace dans le modÃ¨le (nÃ©cessite logique de remplacement rÃ©cursive)
            # setattr(parent_module, child_name, lora_module)
    
    return lora_model

# Exemple: conversion d'un transformer
class SimpleTransformer(nn.Module):
    def __init__(self, d_model=768, num_layers=12):
        super().__init__()
        self.layers = nn.ModuleList([
            nn.Linear(d_model, d_model) for _ in range(num_layers)
        ])
    
    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return x

model = SimpleTransformer(d_model=768, num_layers=12)
lora_model = convert_model_to_lora(model, rank=16)

print(f"ModÃ¨le Original: {sum(p.numel() for p in model.parameters()):,} params")
print(f"ModÃ¨le LoRA: {sum(p.numel() for p in lora_model.parameters() if p.requires_grad):,} trainable params")
```

---

## Fine-tuning avec LoRA

```python
class LoRATrainer:
    """
    EntraÃ®neur pour fine-tuning avec LoRA
    """
    
    def __init__(self, lora_model, train_loader, val_loader, lr=1e-4):
        self.model = lora_model
        self.train_loader = train_loader
        self.val_loader = val_loader
        
        # Optimiseur: seulement les paramÃ¨tres LoRA
        trainable_params = [p for p in lora_model.parameters() if p.requires_grad]
        self.optimizer = torch.optim.AdamW(trainable_params, lr=lr, weight_decay=0.01)
        self.criterion = nn.CrossEntropyLoss()
    
    def train_epoch(self):
        """Un epoch d'entraÃ®nement"""
        self.model.train()
        total_loss = 0.0
        
        for data, labels in self.train_loader:
            self.optimizer.zero_grad()
            
            outputs = self.model(data)
            loss = self.criterion(outputs, labels)
            loss.backward()
            
            # Gradient clipping (souvent utile pour LoRA)
            torch.nn.utils.clip_grad_norm_(
                [p for p in self.model.parameters() if p.requires_grad],
                max_norm=1.0
            )
            
            self.optimizer.step()
            total_loss += loss.item()
        
        return total_loss / len(self.train_loader)
    
    def validate(self):
        """Validation"""
        self.model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, labels in self.val_loader:
                outputs = self.model(data)
                pred = outputs.argmax(dim=1)
                correct += pred.eq(labels).sum().item()
                total += labels.size(0)
        
        return 100.0 * correct / total
    
    def train(self, epochs=10):
        """EntraÃ®nement complet"""
        best_acc = 0.0
        
        for epoch in range(epochs):
            train_loss = self.train_epoch()
            val_acc = self.validate()
            
            if epoch % 5 == 0:
                print(f'Epoch {epoch+1}/{epochs}: '
                      f'Loss={train_loss:.4f}, Acc={val_acc:.2f}%')
            
            if val_acc > best_acc:
                best_acc = val_acc
                # Sauvegarde seulement les poids LoRA
                lora_state = {
                    name: param for name, param in self.model.named_parameters()
                    if 'lora' in name and param.requires_grad
                }
                torch.save(lora_state, 'best_lora.pt')
        
        return self.model
```

---

## Variantes de LoRA

### LoRA avec Rang Adaptatif

```python
class AdaptiveLoRA(LoRALayer):
    """
    LoRA avec rang adaptatif (peut varier par couche)
    """
    
    def __init__(self, original_layer, ranks=[8, 16, 32], alpha=1.0):
        """
        Args:
            ranks: Liste de rangs possibles (choisit le meilleur)
        """
        # Pour simplifier, utilise le rang moyen
        avg_rank = int(np.mean(ranks))
        super().__init__(original_layer, rank=avg_rank, alpha=alpha)
        
        self.possible_ranks = ranks
```

### LoRA++ (AmÃ©liorations)

```python
class LoRAPlus(LoRALayer):
    """
    Variante amÃ©liorÃ©e de LoRA avec scaling adaptatif
    """
    
    def __init__(self, original_layer, rank=16, alpha_init=1.0):
        super().__init__(original_layer, rank=rank, alpha=alpha_init)
        
        # Alpha adaptatif par paramÃ¨tre
        # (SimplifiÃ© - en pratique, peut Ãªtre plus sophistiquÃ©)
        self.alpha_scaling = nn.Parameter(torch.ones(1))
    
    def forward(self, x):
        original_out = self.original(x)
        
        # Adaptation avec scaling adaptatif
        lora_intermediate = x @ self.lora_A.T
        lora_out = lora_intermediate @ self.lora_B.T
        
        adaptive_alpha = self.alpha * self.alpha_scaling
        
        return original_out + adaptive_alpha * lora_out
```

---

## Avantages et Limitations

### Avantages

- **EfficacitÃ©** : RÃ©duction massive des paramÃ¨tres entraÃ®nables
- **ModularitÃ©** : Peut ajouter/retirer LoRA facilement
- **Performance** : Presque aussi bon que fine-tuning complet
- **Multi-tÃ¢che** : DiffÃ©rents LoRA pour diffÃ©rentes tÃ¢ches

### Limitations

- **Rang fixe** : NÃ©cessite de choisir le rang
- **Architecture** : Principalement pour Linear/Conv2d
- **Fusion** : NÃ©cessite fusion pour infÃ©rence optimale

---

## Exercices

### Exercice 11.3.1
Comparez LoRA avec diffÃ©rents rangs (4, 8, 16, 32) sur un modÃ¨le prÃ©-entraÃ®nÃ©.

### Exercice 11.3.2
ImplÃ©mentez une version de LoRA qui ajuste automatiquement le rang selon l'importance des gradients.

### Exercice 11.3.3
CrÃ©ez un systÃ¨me multi-tÃ¢che utilisant diffÃ©rents LoRA pour diffÃ©rentes tÃ¢ches.

---

## Points ClÃ©s Ã  Retenir

> ğŸ“Œ **LoRA adapte un modÃ¨le en n'entraÃ®nant que de petits adaptateurs**

> ğŸ“Œ **W_new = W_0 + Î±(B @ A) oÃ¹ seulement A et B sont entraÃ®nables**

> ğŸ“Œ **RÃ©duction drastique: r(d+k) au lieu de dk paramÃ¨tres**

> ğŸ“Œ **Presque aussi performant que fine-tuning complet**

> ğŸ“Œ **Utile pour multi-tÃ¢che (diffÃ©rents LoRA par tÃ¢che)**

---

*Section suivante : [11.4 Combinaison Rang Faible + Quantification](./11_04_Combinaison.md)*

