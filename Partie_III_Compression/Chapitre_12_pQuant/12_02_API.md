# 12.2 API et Interfaces Principales

---

## Introduction

Cette section prÃ©sente l'**API principale** de pQuant, les interfaces utilisateur de haut niveau, et comment utiliser la bibliothÃ¨que pour compresser des modÃ¨les.

---

## API de Haut Niveau

### Fonction compress_model()

```python
import pquant
from pquant import compress_model

def compress_model_example():
    """
    Exemple d'utilisation de l'API principale
    """
    import torch
    import torch.nn as nn
    
    # ModÃ¨le Ã  compresser
    model = nn.Sequential(
        nn.Linear(784, 512),
        nn.ReLU(),
        nn.Linear(512, 256),
        nn.ReLU(),
        nn.Linear(256, 10)
    )
    
    # Configuration simple
    config = {
        'method': 'low_rank',
        'rank': 64,
        'train_after_compression': False
    }
    
    # Compression
    compressed_model = compress_model(model, config)
    
    return compressed_model

# Avec calibration
def compress_with_calibration(model, train_loader):
    """
    Compression avec calibration (pour quantification)
    """
    config = {
        'method': 'low_rank_quantization',
        'low_rank_rank': 64,
        'quantization_bits': 8,
        'calibration_samples': 100
    }
    
    compressed = compress_model(model, config, train_loader=train_loader)
    return compressed
```

---

## Classe CompressionPipeline

### Interface Principale

```python
from pquant import CompressionPipeline

class CompressionPipeline:
    """
    Pipeline principal pour compression multi-mÃ©thode
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        Args:
            config: Configuration du pipeline
                {
                    'methods': ['low_rank', 'quantization'],
                    'low_rank': {'rank': 64},
                    'quantization': {'bits': 8},
                    'order': ['low_rank', 'quantization'],  # Ordre d'application
                    'evaluate': True,
                    'save_path': 'compressed_model.pt'
                }
        """
        self.config = config
        self.methods = []
        
        # Initialise les mÃ©thodes selon l'ordre
        order = config.get('order', config.get('methods', []))
        
        for method_name in order:
            method_config = config.get(method_name, {})
            method = CompressionMethodRegistry.get(method_name, method_config)
            self.methods.append(method)
    
    def compress(self, model, train_loader=None, val_loader=None):
        """
        Compresse le modÃ¨le en appliquant toutes les mÃ©thodes
        
        Args:
            model: ModÃ¨le Ã  compresser
            train_loader: DataLoader pour entraÃ®nement/calibration
            val_loader: DataLoader pour validation
        
        Returns:
            ModÃ¨le compressÃ©
        """
        compressed = model
        
        print(f"Compression Pipeline: {len(self.methods)} methods")
        
        for i, method in enumerate(self.methods):
            print(f"  Step {i+1}/{len(self.methods)}: {method.name}")
            
            compressed = method.compress(
                compressed,
                train_loader=train_loader,
                val_loader=val_loader
            )
        
        return compressed
    
    def evaluate(self, original_model, compressed_model, test_loader):
        """
        Ã‰value les performances
        
        Returns:
            Dict avec mÃ©triques
        """
        results = {
            'original': self._evaluate_model(original_model, test_loader),
            'compressed': self._evaluate_model(compressed_model, test_loader),
            'compression_info': {}
        }
        
        # Informations de compression pour chaque mÃ©thode
        for method in self.methods:
            info = method.get_compression_info(original_model, compressed_model)
            results['compression_info'][method.name] = info
        
        # Calcul total
        orig_params = sum(p.numel() for p in original_model.parameters())
        comp_params = sum(p.numel() for p in compressed_model.parameters())
        
        results['total_compression'] = {
            'original_params': orig_params,
            'compressed_params': comp_params,
            'compression_ratio': orig_params / comp_params,
            'size_reduction_mb': (orig_params * 4 - comp_params * 4) / (1024**2)
        }
        
        return results
    
    def _evaluate_model(self, model, data_loader):
        """Ã‰value un modÃ¨le"""
        model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, labels in data_loader:
                outputs = model(data)
                pred = outputs.argmax(dim=1)
                correct += pred.eq(labels).sum().item()
                total += labels.size(0)
        
        return {
            'accuracy': 100.0 * correct / total,
            'correct': correct,
            'total': total
        }

# Exemple d'utilisation
config = {
    'methods': ['low_rank', 'quantization'],
    'low_rank': {'rank': 64},
    'quantization': {'bits': 8, 'method': 'ptq'},
    'order': ['low_rank', 'quantization']
}

pipeline = CompressionPipeline(config)
compressed = pipeline.compress(model, train_loader)
results = pipeline.evaluate(model, compressed, test_loader)

print(f"Results: {results}")
```

---

## API SpÃ©cialisÃ©e par MÃ©thode

### Low-Rank API

```python
from pquant.methods.low_rank import LowRankCompressor

def low_rank_api_example():
    """
    API spÃ©cialisÃ©e pour compression low-rank
    """
    compressor = LowRankCompressor(
        rank=64,
        method='svd',  # ou 'factorization', 'lora'
        target_layers=['linear']  # Types de couches Ã  compresser
    )
    
    compressed = compressor.compress(model)
    
    # Informations dÃ©taillÃ©es
    info = compressor.get_compression_info(model, compressed)
    print(f"Low-rank compression: {info['compression_ratio']:.2f}x")
    
    return compressed
```

### Quantization API

```python
from pquant.methods.quantization import QuantizationCompressor

def quantization_api_example():
    """
    API pour quantification
    """
    # PTQ
    ptq_compressor = QuantizationCompressor(
        bits=8,
        method='ptq',
        calibration_samples=500
    )
    
    compressed_ptq = ptq_compressor.compress(model, train_loader)
    
    # QAT
    qat_compressor = QuantizationCompressor(
        bits=8,
        method='qat',
        epochs=10
    )
    
    compressed_qat = qat_compressor.compress(model, train_loader, val_loader)
    
    return compressed_ptq, compressed_qat
```

### Tensor Network API

```python
from pquant.methods.tensor_networks import TensorTrainCompressor

def tensor_network_api_example():
    """
    API pour compression Tensor Train
    """
    tt_compressor = TensorTrainCompressor(
        rank=32,
        factorize_dimensions=True,  # Auto-factorisation
        train_after_compression=True,
        epochs=20
    )
    
    compressed = tt_compressor.compress(model, train_loader)
    
    return compressed
```

---

## API de Configuration

### Builder Pattern

```python
from pquant import CompressionBuilder

class CompressionBuilder:
    """
    Builder pour construire des pipelines de compression
    """
    
    def __init__(self):
        self.config = {'methods': [], 'order': []}
    
    def add_low_rank(self, rank=64, method='svd'):
        """Ajoute compression low-rank"""
        self.config['methods'].append('low_rank')
        self.config['order'].append('low_rank')
        self.config['low_rank'] = {'rank': rank, 'method': method}
        return self
    
    def add_quantization(self, bits=8, method='ptq'):
        """Ajoute quantification"""
        self.config['methods'].append('quantization')
        self.config['order'].append('quantization')
        self.config['quantization'] = {'bits': bits, 'method': method}
        return self
    
    def add_tensor_train(self, rank=32):
        """Ajoute Tensor Train"""
        self.config['methods'].append('tensor_train')
        self.config['order'].append('tensor_train')
        self.config['tensor_train'] = {'rank': rank}
        return self
    
    def set_training(self, epochs=10, lr=1e-4):
        """Configure l'entraÃ®nement"""
        self.config['training'] = {'epochs': epochs, 'lr': lr}
        return self
    
    def build(self):
        """Construit le pipeline"""
        return CompressionPipeline(self.config)

# Utilisation fluide
pipeline = (CompressionBuilder()
            .add_low_rank(rank=64)
            .add_quantization(bits=8)
            .set_training(epochs=20)
            .build())

compressed = pipeline.compress(model, train_loader)
```

---

## API d'Ã‰valuation

```python
from pquant.evaluation import evaluate_compression, compare_methods

def evaluation_api_example():
    """
    API pour Ã©valuation et comparaison
    """
    # Ã‰valuation simple
    results = evaluate_compression(
        original_model=model,
        compressed_model=compressed,
        test_loader=test_loader,
        metrics=['accuracy', 'latency', 'model_size']
    )
    
    print(f"Accuracy: {results['accuracy']['original']:.2f}% â†’ "
          f"{results['accuracy']['compressed']:.2f}%")
    print(f"Compression: {results['compression_ratio']:.2f}x")
    
    # Comparaison de mÃ©thodes
    methods = ['low_rank', 'quantization', 'tensor_train']
    comparison = compare_methods(
        model=model,
        methods=methods,
        test_loader=test_loader,
        configs={
            'low_rank': {'rank': 64},
            'quantization': {'bits': 8},
            'tensor_train': {'rank': 32}
        }
    )
    
    print("\nComparaison des mÃ©thodes:")
    for method, metrics in comparison.items():
        print(f"  {method}: {metrics['accuracy']:.2f}%, "
              f"compression={metrics['compression_ratio']:.2f}x")
    
    return results, comparison
```

---

## API de Sauvegarde/Chargement

```python
from pquant import save_compressed_model, load_compressed_model

def save_load_api():
    """
    API pour sauvegarder et charger des modÃ¨les compressÃ©s
    """
    # Sauvegarde avec mÃ©tadonnÃ©es
    save_compressed_model(
        model=compressed_model,
        path='compressed_model.pt',
        metadata={
            'compression_method': 'low_rank_quantization',
            'original_accuracy': 95.2,
            'compressed_accuracy': 94.8,
            'compression_ratio': 8.5,
            'config': pipeline.config
        }
    )
    
    # Chargement
    loaded_model, metadata = load_compressed_model('compressed_model.pt')
    
    print(f"Loaded model: {metadata['compression_method']}")
    print(f"Original accuracy: {metadata['original_accuracy']:.2f}%")
    
    return loaded_model
```

---

## API Utilitaire

```python
from pquant.utils import analyze_model, visualize_compression

def utility_api_example():
    """
    Utilitaires d'analyse et visualisation
    """
    # Analyse de modÃ¨le
    analysis = analyze_model(model)
    print(f"Total parameters: {analysis['total_params']:,}")
    print(f"Compressible layers: {len(analysis['compressible_layers'])}")
    
    # Visualisation
    visualize_compression(
        original_model=model,
        compressed_model=compressed,
        save_path='compression_report.html'
    )
```

---

## Exercices

### Exercice 12.2.1
CrÃ©ez un pipeline de compression combinant 3 mÃ©thodes avec le builder API.

### Exercice 12.2.2
ImplÃ©mentez une fonction utilitaire pour comparer automatiquement diffÃ©rentes configurations.

---

## Points ClÃ©s Ã  Retenir

> ğŸ“Œ **API de haut niveau simplifie l'utilisation**

> ğŸ“Œ **CompressionPipeline permet combinaison de mÃ©thodes**

> ğŸ“Œ **Builder pattern facilite la construction de configurations**

> ğŸ“Œ **API spÃ©cialisÃ©e pour chaque mÃ©thode de compression**

> ğŸ“Œ **Ã‰valuation et comparaison intÃ©grÃ©es**

---

*Section suivante : [12.3 ImplÃ©mentation des MÃ©thodes de Compression](./12_03_Implementation.md)*

