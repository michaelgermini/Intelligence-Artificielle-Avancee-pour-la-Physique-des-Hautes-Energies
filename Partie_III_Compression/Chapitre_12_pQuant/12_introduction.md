# Chapitre 12 : La BibliothÃ¨que pQuant

---

## Introduction

**pQuant** est une bibliothÃ¨que open-source dÃ©veloppÃ©e au CERN pour la compression de modÃ¨les deep learning utilisant des techniques de rang faible et de rÃ©seaux de tenseurs. Elle est conÃ§ue pour faciliter la recherche et le dÃ©ploiement de modÃ¨les compressÃ©s dans les applications de physique des hautes Ã©nergies.

---

## Plan du Chapitre

1. [Architecture et Conception](./12_01_Architecture.md)
2. [API et Interfaces Principales](./12_02_API.md)
3. [ImplÃ©mentation des MÃ©thodes de Compression](./12_03_Implementation.md)
4. [Pipelines de Compression AutomatisÃ©s](./12_04_Pipelines.md)
5. [Benchmarking et Ã‰valuation](./12_05_Benchmarking.md)
6. [Contribution Open-Source et Bonnes Pratiques](./12_06_Contribution.md)

---

## Vue d'Ensemble

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Architecture pQuant                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚              Interface Utilisateur                   â”‚      â”‚
â”‚  â”‚  (Compression Pipeline, Configuration)               â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                          â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚         MÃ©thodes de Compression                      â”‚      â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚      â”‚
â”‚  â”‚  â”‚  Low-Rankâ”‚ â”‚Tensor NN â”‚ â”‚Quantiz.  â”‚            â”‚      â”‚
â”‚  â”‚  â”‚  (SVD,   â”‚ â”‚(TT, CP)  â”‚ â”‚(INT8,    â”‚            â”‚      â”‚
â”‚  â”‚  â”‚   LoRA)  â”‚ â”‚          â”‚ â”‚  FP16)   â”‚            â”‚      â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                          â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚      Backends (PyTorch, TensorFlow, JAX)             â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Installation et Configuration

```python
# Installation (exemple)
# pip install pquant
# ou depuis source:
# git clone https://github.com/cern/pquant.git
# cd pquant
# pip install -e .

import pquant
import torch
import torch.nn as nn

print(f"pQuant version: {pquant.__version__}")

# VÃ©rification de l'installation
assert pquant is not None, "pQuant non installÃ©"
```

---

## Utilisation Basique

### Compression Simple

```python
from pquant import compress_model
from pquant.compression import LowRankCompression, TensorTrainCompression

# ModÃ¨le Ã  compresser
model = nn.Sequential(
    nn.Linear(784, 512),
    nn.ReLU(),
    nn.Linear(512, 256),
    nn.ReLU(),
    nn.Linear(256, 10)
)

# Compression par rang faible
compressed_model = compress_model(
    model,
    method='low_rank',
    rank=64,
    target_sparsity=0.5
)

print(f"ModÃ¨le original: {sum(p.numel() for p in model.parameters()):,} paramÃ¨tres")
print(f"ModÃ¨le compressÃ©: {sum(p.numel() for p in compressed_model.parameters()):,} paramÃ¨tres")
```

### Compression avec Tensor Train

```python
from pquant.compression import TensorTrainCompression

# Configuration
config = {
    'method': 'tensor_train',
    'rank': 32,
    'train_after_compression': True,
    'epochs': 10
}

# Compression
tt_compressor = TensorTrainCompression(config)
compressed_model = tt_compressor.compress(model)

# Ã‰valuation
original_accuracy = evaluate(model, test_loader)
compressed_accuracy = evaluate(compressed_model, test_loader)

print(f"Accuracy originale: {original_accuracy:.2%}")
print(f"Accuracy compressÃ©e: {compressed_accuracy:.2%}")
print(f"DÃ©gradation: {(original_accuracy - compressed_accuracy)*100:.2f}%")
```

---

## API Principale

### Classe de Compression

```python
class CompressionPipeline:
    """
    Pipeline de compression modulaire
    """
    
    def __init__(self, config):
        """
        Args:
            config: Dictionnaire de configuration
                {
                    'methods': ['low_rank', 'quantization'],
                    'low_rank_rank': 64,
                    'quantization_bits': 8,
                    ...
                }
        """
        self.config = config
        self.methods = []
        
        # Initialise les mÃ©thodes selon la config
        if 'low_rank' in config.get('methods', []):
            self.methods.append(LowRankCompression(config))
        
        if 'quantization' in config.get('methods', []):
            self.methods.append(QuantizationCompression(config))
    
    def compress(self, model, train_loader=None):
        """
        Compresse le modÃ¨le en appliquant toutes les mÃ©thodes
        
        Args:
            model: ModÃ¨le PyTorch
            train_loader: DataLoader pour calibration/fine-tuning
        
        Returns:
            ModÃ¨le compressÃ©
        """
        compressed = model
        
        for method in self.methods:
            compressed = method.compress(compressed, train_loader)
        
        return compressed
    
    def evaluate(self, original_model, compressed_model, test_loader):
        """
        Compare les performances original vs compressÃ©
        """
        results = {
            'original': evaluate_model(original_model, test_loader),
            'compressed': evaluate_model(compressed_model, test_loader),
            'compression_ratio': self._compute_compression_ratio(
                original_model, compressed_model
            )
        }
        
        return results
```

---

## IntÃ©gration avec les Workflows HEP

```python
class HEPModelCompression:
    """
    Compression spÃ©cialisÃ©e pour les modÃ¨les de physique des particules
    """
    
    @staticmethod
    def compress_jet_tagger(model, train_loader, val_loader):
        """
        Compresse un modÃ¨le de classification de jets
        
        OptimisÃ© pour prÃ©server les performances sur les jets rares
        """
        config = {
            'methods': ['low_rank', 'quantization'],
            'low_rank_rank': 64,
            'quantization_bits': 8,
            'preserve_rare_classes': True,  # Important pour HEP
            'fine_tune_epochs': 20
        }
        
        pipeline = CompressionPipeline(config)
        compressed = pipeline.compress(model, train_loader)
        
        # Ã‰valuation spÃ©ciale pour HEP
        results = pipeline.evaluate(model, compressed, val_loader)
        
        # MÃ©triques additionnelles pour HEP
        results['b_tag_efficiency'] = evaluate_b_tagging(
            model, compressed, val_loader
        )
        
        return compressed, results
    
    @staticmethod
    def compress_trigger_model(model, target_latency_ns=100):
        """
        Compresse un modÃ¨le pour le trigger L1
        
        Contraintes strictes de latence
        """
        config = {
            'methods': ['aggressive_quantization', 'structured_pruning'],
            'quantization_bits': 6,  # TrÃ¨s agressif
            'pruning_sparsity': 0.9,
            'target_latency_ns': target_latency_ns
        }
        
        pipeline = CompressionPipeline(config)
        compressed = pipeline.compress(model)
        
        # Validation de la latence
        latency = measure_latency(compressed)
        assert latency < target_latency_ns, f"Latence {latency}ns > cible {target_latency_ns}ns"
        
        return compressed
```

---

## Benchmarks et MÃ©triques

```python
from pquant.benchmarks import benchmark_compression

# Benchmark standard
results = benchmark_compression(
    model=model,
    dataset='CIFAR10',
    methods=['low_rank', 'quantization', 'pruning'],
    metrics=['accuracy', 'inference_time', 'model_size']
)

print("RÃ©sultats du benchmark:")
for method, metrics in results.items():
    print(f"\n{method}:")
    for metric, value in metrics.items():
        print(f"  {metric}: {value}")
```

---

## Points ClÃ©s Ã  Retenir

> ğŸ“Œ **pQuant fournit une interface unifiÃ©e pour diverses techniques de compression**

> ğŸ“Œ **La bibliothÃ¨que est optimisÃ©e pour les modÃ¨les utilisÃ©s en physique des particules**

> ğŸ“Œ **L'intÃ©gration avec les workflows existants est facilitÃ©e**

> ğŸ“Œ **Les contributions open-source sont encouragÃ©es pour amÃ©liorer la bibliothÃ¨que**

---

## RÃ©fÃ©rences

- Repository GitHub: https://github.com/cern/pquant
- Documentation: https://pquant.readthedocs.io/
- Exemples: https://github.com/cern/pquant/tree/main/examples

---

*Section suivante : [12.1 Architecture et Conception](./12_01_Architecture.md)*

