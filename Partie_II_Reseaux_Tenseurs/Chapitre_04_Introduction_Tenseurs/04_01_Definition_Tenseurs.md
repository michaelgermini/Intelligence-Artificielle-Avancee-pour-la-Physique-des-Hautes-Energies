# 4.1 D√©finition et Notation des Tenseurs

---

## Introduction

Cette section d√©finit rigoureusement les tenseurs, leur notation, et leurs propri√©t√©s fondamentales. Les tenseurs g√©n√©ralisent les concepts de scalaires, vecteurs et matrices √† des dimensions arbitraires.

---

## D√©finitions Formelles

### Tenseur d'Ordre d

Un **tenseur d'ordre d** (ou de rang d) sur un espace vectoriel $V$ de dimension $n$ est un √©l√©ment du produit tensoriel :

$$\mathcal{T} \in \underbrace{V \otimes V \otimes \cdots \otimes V}_{d \text{ fois}}$$

En coordonn√©es, c'est un tableau multidimensionnel :

$$T_{i_1, i_2, \ldots, i_d} \quad \text{o√π} \quad i_k \in \{1, 2, \ldots, n_k\}$$

```python
import numpy as np

class Tensor:
    """
    Classe de base pour manipuler des tenseurs
    """
    
    def __init__(self, data):
        """
        Cr√©e un tenseur √† partir d'un array NumPy
        
        Args:
            data: array NumPy de dimension arbitraire
        """
        self.data = np.array(data)
        self.order = self.data.ndim
        self.shape = self.data.shape
        self.size = self.data.size
        
    def __repr__(self):
        return (f"Tensor(order={self.order}, shape={self.shape}, "
                f"size={self.size})")
    
    def __getitem__(self, indices):
        """Acc√®s aux √©l√©ments"""
        return self.data[indices]
    
    def __setitem__(self, indices, value):
        """Modification des √©l√©ments"""
        self.data[indices] = value
    
    def reshape(self, new_shape):
        """Change la forme du tenseur"""
        return Tensor(self.data.reshape(new_shape))
    
    def transpose(self, axes=None):
        """Transpose le tenseur"""
        return Tensor(self.data.transpose(axes))
    
    def norm(self, p=2):
        """Calcule la norme Lp"""
        if p == 'fro':
            return np.linalg.norm(self.data, 'fro')
        return np.linalg.norm(self.data.flatten(), p)

# Exemples
T1 = Tensor(np.random.randn(3, 4))      # Ordre 2 (matrice)
T2 = Tensor(np.random.randn(2, 3, 4))   # Ordre 3
T3 = Tensor(np.random.randn(2, 3, 4, 5)) # Ordre 4

print("Exemples de tenseurs:")
print(f"T1: {T1}")
print(f"T2: {T2}")
print(f"T3: {T3}")
```

---

## Indexation et Notation

### Notation Indicielle

Les tenseurs utilisent la notation indicielle avec la convention d'Einstein :

```python
def einstein_sum_example():
    """
    Exemples de la notation d'Einstein
    
    Convention: indices r√©p√©t√©s impliquent sommation
    """
    # Exemple 1: Produit scalaire
    # v_i w_i = Œ£·µ¢ v·µ¢ w·µ¢
    v = np.array([1, 2, 3])
    w = np.array([4, 5, 6])
    dot_product = np.einsum('i,i->', v, w)
    print(f"Produit scalaire: {dot_product}")
    print(f"V√©rification: {np.dot(v, w)}")
    
    # Exemple 2: Produit matriciel
    # C_ij = A_ik B_kj = Œ£‚Çñ A·µ¢‚Çñ B‚Çñ‚±º
    A = np.random.randn(3, 4)
    B = np.random.randn(4, 5)
    C = np.einsum('ik,kj->ij', A, B)
    print(f"\nProduit matriciel: {C.shape}")
    print(f"V√©rification: {(A @ B).shape}")
    
    # Exemple 3: Contraction tensorielle
    # T_ijk U_jl = V_ikl = Œ£‚±º T·µ¢‚±º‚Çñ U‚±º‚Çó
    T = np.random.randn(3, 4, 5)
    U = np.random.randn(4, 6)
    V = np.einsum('ijk,jl->ikl', T, U)
    print(f"\nContraction: T{tuple(T.shape)} √ó U{tuple(U.shape)} = V{tuple(V.shape)}")
    
    # Exemple 4: Trace
    # tr(A) = A_ii = Œ£·µ¢ A·µ¢·µ¢
    M = np.random.randn(4, 4)
    trace = np.einsum('ii->', M)
    print(f"\nTrace: {trace:.4f}")
    print(f"V√©rification: {np.trace(M):.4f}")

einstein_sum_example()
```

### Acc√®s aux √âl√©ments

```python
class TensorIndexing:
    """
    Techniques d'indexation avanc√©es pour tenseurs
    """
    
    @staticmethod
    def basic_indexing(tensor):
        """Indexation basique"""
        print("Indexation basique:")
        print(f"  tensor[0]: {tensor[0].shape if tensor.ndim > 1 else tensor[0]}")
        print(f"  tensor[0, 1]: {tensor[0, 1] if tensor.ndim >= 2 else 'N/A'}")
        print(f"  tensor[0, :, 2]: {tensor[0, :, 2].shape if tensor.ndim >= 3 else 'N/A'}")
    
    @staticmethod
    def advanced_indexing(tensor):
        """Indexation avanc√©e"""
        # Boolean indexing
        mask = tensor > 0
        positive_values = tensor[mask]
        print(f"\nValeurs positives: {len(positive_values)}/{tensor.size}")
        
        # Fancy indexing
        indices = [0, 2, 4]
        if tensor.ndim >= 2:
            selected = tensor[indices, :]
            print(f"Lignes s√©lectionn√©es: {selected.shape}")
    
    @staticmethod
    def slicing(tensor):
        """Slicing multidimensionnel"""
        if tensor.ndim >= 2:
            slice_2d = tensor[1:3, :]
            print(f"\nSlice 2D: {slice_2d.shape}")
        
        if tensor.ndim >= 3:
            slice_3d = tensor[:, 1:3, ::2]
            print(f"Slice 3D: {slice_3d.shape}")

# Test
T = np.random.randn(5, 6, 7)
TensorIndexing.basic_indexing(T)
TensorIndexing.advanced_indexing(T)
TensorIndexing.slicing(T)
```

---

## Rang Tensoriel

### D√©finition du Rang

Le **rang tensoriel** (ou rang CP) est le nombre minimal $r$ tel que :

$$\mathcal{T} = \sum_{k=1}^{r} \lambda_k \mathbf{a}_k^{(1)} \otimes \mathbf{a}_k^{(2)} \otimes \cdots \otimes \mathbf{a}_k^{(d)}$$

o√π chaque $\mathbf{a}_k^{(i)}$ est un vecteur.

```python
def tensor_rank_analysis(tensor):
    """
    Analyse le rang d'un tenseur
    
    Le rang tensoriel est difficile √† calculer exactement (NP-hard),
    mais on peut estimer des bornes via les rangs des matricisations
    """
    # Matricisations (unfolding selon chaque mode)
    mode_ranks = []
    for mode in range(tensor.ndim):
        # D√©plie selon le mode
        shape = tensor.shape
        dim_mode = shape[mode]
        other_dims = np.prod([shape[i] for i in range(len(shape)) if i != mode])
        
        # Matricisation
        tensor_reshaped = np.moveaxis(tensor, mode, 0)
        matrix = tensor_reshaped.reshape(dim_mode, other_dims)
        
        # Rang de la matrice
        rank = np.linalg.matrix_rank(matrix)
        mode_ranks.append(rank)
    
    # Bornes sur le rang tensoriel
    lower_bound = max(mode_ranks)
    upper_bound = min(tensor.shape)
    
    return {
        'mode_ranks': mode_ranks,
        'lower_bound': lower_bound,
        'upper_bound': upper_bound,
        'mode_with_min_rank': np.argmin(mode_ranks),
        'mode_with_max_rank': np.argmax(mode_ranks)
    }

# Exemple
T = np.random.randn(10, 12, 8)
analysis = tensor_rank_analysis(T)
print("Analyse du rang tensoriel:")
print(f"  Rangs des modes: {analysis['mode_ranks']}")
print(f"  Borne inf√©rieure: {analysis['lower_bound']}")
print(f"  Borne sup√©rieure: {analysis['upper_bound']}")
print(f"  Rang tensoriel ‚àà [{analysis['lower_bound']}, {analysis['upper_bound']}]")
```

---

## Propri√©t√©s Fondamentales

### Sym√©trie

Un tenseur est **sym√©trique** si ses composantes sont invariantes sous permutation des indices :

```python
def check_symmetry(tensor):
    """
    V√©rifie si un tenseur est sym√©trique
    """
    if tensor.ndim < 2:
        return True  # Scalaires et vecteurs sont trivialement sym√©triques
    
    # Pour un tenseur d'ordre 2 (matrice)
    if tensor.ndim == 2:
        return np.allclose(tensor, tensor.T)
    
    # Pour les tenseurs d'ordre sup√©rieur
    # V√©rifie toutes les permutations possibles
    n = tensor.shape[0]
    if not all(s == n for s in tensor.shape):
        return False  # Doit √™tre cubique pour √™tre sym√©trique
    
    # Teste quelques permutations
    for perm in [(1, 0), (2, 1, 0)] if tensor.ndim >= 3 else [(1, 0)]:
        if tensor.ndim >= len(perm):
            permuted = np.transpose(tensor, perm)
            if not np.allclose(tensor, permuted):
                return False
    
    return True

# Test
symmetric_matrix = np.array([[1, 2, 3], [2, 4, 5], [3, 5, 6]])
print(f"Matrice sym√©trique: {check_symmetry(symmetric_matrix)}")

asymmetric_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
print(f"Matrice asym√©trique: {check_symmetry(asymmetric_matrix)}")
```

### D√©composabilit√©

Un tenseur est **s√©parable** (d√©composable) s'il peut s'√©crire comme un produit tensoriel :

```python
def is_separable(tensor):
    """
    V√©rifie si un tenseur est s√©parable (rang 1)
    """
    # Pour un tenseur d'ordre 2, c'est une matrice de rang 1
    if tensor.ndim == 2:
        rank = np.linalg.matrix_rank(tensor)
        return rank == 1
    
    # Pour les tenseurs d'ordre sup√©rieur, v√©rifie le rang tensoriel
    # Approximation: v√©rifie si toutes les matricisations sont de rang 1
    for mode in range(tensor.ndim):
        shape = tensor.shape
        dim_mode = shape[mode]
        other_dims = np.prod([shape[i] for i in range(len(shape)) if i != mode])
        
        tensor_reshaped = np.moveaxis(tensor, mode, 0)
        matrix = tensor_reshaped.reshape(dim_mode, other_dims)
        
        if np.linalg.matrix_rank(matrix) > 1:
            return False
    
    return True

# Test
# Tenseur s√©parable (produit ext√©rieur)
u = np.array([1, 2, 3])
v = np.array([4, 5])
T_sep = np.outer(u, v)
print(f"Tenseur s√©parable: {is_separable(T_sep)}")

# Tenseur non-s√©parable
T_nonsep = np.random.randn(3, 4)
print(f"Tenseur non-s√©parable: {is_separable(T_nonsep)}")
```

---

## Op√©rations de Base

### Addition et Multiplication

```python
def tensor_operations():
    """
    Op√©rations √©l√©mentaires sur les tenseurs
    """
    T1 = np.random.randn(3, 4, 5)
    T2 = np.random.randn(3, 4, 5)
    
    # Addition √©l√©ment par √©l√©ment
    T_sum = T1 + T2
    print(f"Addition: {T_sum.shape}")
    
    # Multiplication √©l√©ment par √©l√©ment (Hadamard)
    T_hadamard = T1 * T2
    print(f"Produit de Hadamard: {T_hadamard.shape}")
    
    # Multiplication par scalaire
    T_scaled = 2.5 * T1
    print(f"Multiplication par scalaire: {T_scaled.shape}")
    
    # Normes
    frobenius = np.linalg.norm(T1, 'fro')
    l1_norm = np.abs(T1).sum()
    l2_norm = np.linalg.norm(T1.flatten())
    
    print(f"\nNormes:")
    print(f"  Frobenius: {frobenius:.4f}")
    print(f"  L1: {l1_norm:.4f}")
    print(f"  L2: {l2_norm:.4f}")

tensor_operations()
```

### Produit Tensoriel (Outer Product)

```python
def outer_product(*vectors):
    """
    Produit tensoriel de plusieurs vecteurs
    
    Le r√©sultat a un ordre √©gal √† la somme des ordres
    """
    result = vectors[0]
    for v in vectors[1:]:
        result = np.tensordot(result, v, axes=0)
    return result

# Exemple
u = np.array([1, 2])
v = np.array([3, 4, 5])
w = np.array([6, 7])

T = outer_product(u, v, w)
print(f"u ‚äó v ‚äó w: shape = {T.shape}, order = {T.ndim}")
print(f"  u: shape {u.shape}")
print(f"  v: shape {v.shape}")
print(f"  w: shape {w.shape}")
print(f"  R√©sultat: shape {T.shape}")

# V√©rification: T[i, j, k] = u[i] * v[j] * w[k]
print(f"\nV√©rification:")
print(f"  T[0, 0, 0] = {T[0, 0, 0]}, u[0]*v[0]*w[0] = {u[0]*v[0]*w[0]}")
```

---

## Applications en Machine Learning

### Tenseurs dans les R√©seaux de Neurones

```python
class NeuralNetworkTensors:
    """
    Exemples de tenseurs dans les r√©seaux de neurones
    """
    
    @staticmethod
    def weight_tensors():
        """Tenseurs de poids dans diff√©rents types de couches"""
        
        # Couche Fully-Connected (Linear)
        # W: (out_features, in_features) - ordre 2
        W_fc = np.random.randn(256, 784)
        print(f"FC Layer weight: order {W_fc.ndim}, shape {W_fc.shape}")
        
        # Couche Convolutionnelle
        # W: (out_channels, in_channels, kernel_h, kernel_w) - ordre 4
        W_conv = np.random.randn(64, 32, 3, 3)
        print(f"Conv Layer weight: order {W_conv.ndim}, shape {W_conv.shape}")
        
        # Batch de donn√©es
        # Input: (batch_size, channels, height, width) - ordre 4
        X = np.random.randn(32, 3, 224, 224)
        print(f"Batch input: order {X.ndim}, shape {X.shape}")
        
        # Attention (Transformer)
        # Attention weights: (batch, heads, seq_len, seq_len) - ordre 4
        A = np.random.randn(8, 12, 512, 512)
        print(f"Attention weights: order {A.ndim}, shape {A.shape}")
    
    @staticmethod
    def parameter_count_example():
        """Compte les param√®tres en fonction de la repr√©sentation"""
        
        # Repr√©sentation dense
        W_dense = np.random.randn(1024, 512)
        params_dense = W_dense.size
        print(f"Repr√©sentation dense: {params_dense:,} param√®tres")
        
        # Repr√©sentation factoris√©e (low-rank)
        U = np.random.randn(1024, 64)
        V = np.random.randn(64, 512)
        W_factorized = U @ V
        params_factorized = U.size + V.size
        print(f"Repr√©sentation factoris√©e: {params_factorized:,} param√®tres")
        print(f"  Compression: {params_dense / params_factorized:.2f}x")
        print(f"  Erreur relative: {np.linalg.norm(W_dense - W_factorized) / np.linalg.norm(W_dense):.4f}")

NeuralNetworkTensors.weight_tensors()
print()
NeuralNetworkTensors.parameter_count_example()
```

---

## Exercices

### Exercice 4.1.1
Cr√©ez un tenseur d'ordre 5 de shape (2, 3, 4, 5, 6). Calculez toutes ses matricisations et leurs rangs.

### Exercice 4.1.2
Impl√©mentez une fonction qui v√©rifie si un tenseur d'ordre 3 est sym√©trique par rapport √† toutes ses permutations.

### Exercice 4.1.3
Cr√©ez un tenseur s√©parable (rang 1) et v√©rifiez que toutes ses matricisations sont de rang 1.

---

## Points Cl√©s √† Retenir

> üìå **Un tenseur d'ordre d est un tableau √† d dimensions**

> üìå **Le rang tensoriel est plus complexe que le rang matriciel**

> üìå **La notation d'Einstein simplifie les expressions avec indices r√©p√©t√©s**

> üìå **Les tenseurs dans les r√©seaux de neurones peuvent √™tre tr√®s grands**

---

*Section suivante : [4.2 Contraction de Tenseurs](./04_02_Contraction.md)*

