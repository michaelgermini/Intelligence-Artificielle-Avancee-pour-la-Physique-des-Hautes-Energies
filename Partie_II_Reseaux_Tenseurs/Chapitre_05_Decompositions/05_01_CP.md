# 5.1 D√©composition CP (CANDECOMP/PARAFAC)

---

## Introduction

La **d√©composition CP** (Canonical Polyadic ou CANDECOMP/PARAFAC) repr√©sente un tenseur comme une somme de produits tensoriels de rang 1. C'est la g√©n√©ralisation la plus directe de la SVD aux tenseurs d'ordre sup√©rieur.

---

## D√©finition Formelle

Pour un tenseur $\mathcal{T} \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N}$, la d√©composition CP de rang $R$ est :

$$\mathcal{T} \approx \sum_{r=1}^{R} \lambda_r \mathbf{a}_r^{(1)} \circ \mathbf{a}_r^{(2)} \circ \cdots \circ \mathbf{a}_r^{(N)}$$

o√π :
- $\lambda_r \in \mathbb{R}$ : poids de la composante $r$
- $\mathbf{a}_r^{(n)} \in \mathbb{R}^{I_n}$ : facteur du mode $n$ pour la composante $r$
- $\circ$ : produit tensoriel (outer product)

---

## Propri√©t√©s

```python
import numpy as np
from scipy.linalg import khatri_rao

class CPDecomposition:
    """
    D√©composition CP d'un tenseur
    """
    
    def __init__(self, tensor, rank):
        """
        Args:
            tensor: Tenseur √† d√©composer (array NumPy)
            rank: Rang CP (nombre de composantes)
        """
        self.tensor = np.array(tensor)
        self.rank = rank
        self.n_modes = tensor.ndim
        self.shape = tensor.shape
        
        # Facteurs (un par mode)
        self.factors = [
            np.random.randn(dim, rank) for dim in self.shape
        ]
        
        # Poids (optionnel, peut √™tre absorb√© dans les facteurs)
        self.weights = np.ones(rank)
    
    def reconstruct(self):
        """
        Reconstruit le tenseur depuis la d√©composition CP
        """
        result = np.zeros(self.shape)
        
        for r in range(self.rank):
            # Produit tensoriel des facteurs
            component = self.factors[0][:, r]
            for n in range(1, self.n_modes):
                component = np.outer(component, self.factors[n][:, r])
            
            # Ajoute avec poids
            result += self.weights[r] * component.reshape(self.shape)
        
        return result
    
    def compute_error(self):
        """Calcule l'erreur de reconstruction"""
        reconstructed = self.reconstruct()
        error = np.linalg.norm(self.tensor - reconstructed, 'fro')
        relative_error = error / np.linalg.norm(self.tensor, 'fro')
        return error, relative_error

# Exemple
tensor = np.random.randn(10, 12, 8)
cp = CPDecomposition(tensor, rank=5)

error, rel_error = cp.compute_error()
print(f"D√©composition CP: rank {cp.rank}")
print(f"  Erreur relative: {rel_error:.4f}")
```

---

## Algorithme ALS (Alternating Least Squares)

```python
def cp_als(tensor, rank, max_iter=100, tol=1e-6, normalize=True):
    """
    D√©composition CP par Alternating Least Squares
    
    Algorithm:
    1. Initialise al√©atoirement les facteurs
    2. Pour chaque mode n:
       - Fixe les autres facteurs
       - R√©sout un probl√®me de moindres carr√©s pour le mode n
    3. R√©p√®te jusqu'√† convergence
    """
    n_modes = tensor.ndim
    shape = tensor.shape
    
    # Initialisation al√©atoire
    factors = [
        np.random.randn(dim, rank) for dim in shape
    ]
    weights = np.ones(rank)
    
    # Normalisation
    for r in range(rank):
        norm = 1.0
        for n in range(n_modes):
            col_norm = np.linalg.norm(factors[n][:, r])
            factors[n][:, r] /= col_norm
            norm *= col_norm
        weights[r] = norm
    
    prev_error = float('inf')
    
    for iteration in range(max_iter):
        for mode in range(n_modes):
            # Matricisation selon le mode
            tensor_mat = unfold_tensor(tensor, mode)
            
            # Produit Khatri-Rao des autres facteurs
            kr_product = None
            for n in range(n_modes):
                if n != mode:
                    if kr_product is None:
                        kr_product = factors[n]
                    else:
                        kr_product = khatri_rao(kr_product, factors[n])
            
            # Moindres carr√©s: A^(n) = X_(n) @ (KR^T @ KR)^(-1) @ KR^T
            kr_kr = kr_product.T @ kr_product
            
            # √âvite singularit√©
            kr_kr += np.eye(kr_kr.shape[0]) * 1e-8
            
            factors[mode] = (tensor_mat @ kr_product) @ np.linalg.inv(kr_kr)
            
            # Normalise (optionnel)
            if normalize:
                for r in range(rank):
                    norm = np.linalg.norm(factors[mode][:, r])
                    if norm > 0:
                        factors[mode][:, r] /= norm
                        weights[r] *= norm
        
        # Calcule l'erreur
        reconstructed = cp_reconstruct_from_factors(factors, weights)
        error = np.linalg.norm(tensor - reconstructed, 'fro')
        rel_error = error / np.linalg.norm(tensor, 'fro')
        
        if iteration % 10 == 0:
            print(f"Iteration {iteration}: Error = {rel_error:.6f}")
        
        # V√©rifie convergence
        if abs(prev_error - error) < tol:
            print(f"Converged after {iteration+1} iterations")
            break
        
        prev_error = error
    
    return factors, weights, rel_error

def unfold_tensor(tensor, mode):
    """
    D√©plie un tenseur selon un mode
    
    Tensor.shape = (I‚ÇÅ, I‚ÇÇ, ..., I‚Çô)
    Matrix.shape = (I_mode, ‚àè_{i‚â†mode} I_i)
    """
    shape = tensor.shape
    n_modes = len(shape)
    
    # R√©arrange les axes
    perm = list(range(n_modes))
    perm[0], perm[mode] = perm[mode], perm[0]
    
    tensor_perm = np.transpose(tensor, perm)
    
    # Reshape
    matrix = tensor_perm.reshape(shape[mode], -1)
    
    return matrix

def cp_reconstruct_from_factors(factors, weights):
    """Reconstruit depuis les facteurs CP"""
    n_modes = len(factors)
    shape = tuple(f.shape[0] for f in factors)
    rank = factors[0].shape[1]
    
    result = np.zeros(shape)
    
    for r in range(rank):
        component = factors[0][:, r]
        for n in range(1, n_modes):
            component = np.outer(component, factors[n][:, r])
        result += weights[r] * component.reshape(shape)
    
    return result

# Test
tensor_test = np.random.randn(5, 6, 7)
factors, weights, error = cp_als(tensor_test, rank=3, max_iter=50)
print(f"\nErreur finale CP-ALS: {error:.6f}")
```

---

## Unicit√© de la D√©composition CP

### Conditions d'Unicit√©

La d√©composition CP est unique (sous permutation et scaling) si :

$$k_A^{(1)} + k_A^{(2)} + \cdots + k_A^{(N)} \geq 2R + (N-1)$$

o√π $k_A^{(n)}$ est le k-rang du facteur du mode $n$.

```python
def check_cp_uniqueness(factors):
    """
    V√©rifie si les conditions d'unicit√© sont satisfaites
    """
    k_ranks = []
    for factor in factors:
        # k-rank = plus grand k tel que tout ensemble de k colonnes soit lin√©airement ind√©pendant
        rank = np.linalg.matrix_rank(factor)
        k_ranks.append(rank)
    
    R = factors[0].shape[1]
    N = len(factors)
    
    sum_k_ranks = sum(k_ranks)
    condition = sum_k_ranks >= 2*R + (N-1)
    
    return condition, k_ranks, sum_k_ranks, 2*R + (N-1)

factors_test = [
    np.random.randn(10, 5),
    np.random.randn(12, 5),
    np.random.randn(8, 5)
]

is_unique, k_ranks, sum_k, threshold = check_cp_uniqueness(factors_test)
print(f"Conditions d'unicit√©:")
print(f"  k-ranks: {k_ranks}")
print(f"  Somme: {sum_k}, Seuil: {threshold}")
print(f"  Unique: {is_unique}")
```

---

## Limitations et D√©fis

### Probl√®me NP-hard

Trouver le rang CP exact est un probl√®me **NP-hard** pour les tenseurs d'ordre $\geq 3$.

```python
def cp_rank_estimation(tensor, max_rank=20, tolerance=0.01):
    """
    Estime le rang CP en testant diff√©rents rangs
    """
    errors = []
    
    for rank in range(1, max_rank + 1):
        factors, weights, error = cp_als(tensor, rank, max_iter=50)
        errors.append(error)
        
        if error < tolerance:
            print(f"Rang estim√©: {rank} (erreur < {tolerance})")
            return rank, factors, weights
    
    # Trouve le "coude" dans la courbe d'erreur
    errors = np.array(errors)
    improvements = np.diff(errors)
    
    # Le coude est o√π l'am√©lioration devient faible
    elbow = np.argmin(improvements) + 1
    
    return elbow, None, None

# Test sur un tenseur de rang connu
# Cr√©e un tenseur de rang 3
A = np.random.randn(10, 3)
B = np.random.randn(12, 3)
C = np.random.randn(8, 3)

tensor_rank3 = np.zeros((10, 12, 8))
for r in range(3):
    tensor_rank3 += np.outer(np.outer(A[:, r], B[:, r]), C[:, r])

estimated_rank, _, _ = cp_rank_estimation(tensor_rank3, max_rank=10)
print(f"Rang estim√©: {estimated_rank} (vrai rang: 3)")
```

### Probl√®me de D√©g√©n√©rescence

La d√©composition CP peut √™tre **d√©g√©n√©r√©e** (facteurs qui deviennent colin√©aires).

```python
def detect_cp_degeneracy(factors, threshold=0.99):
    """
    D√©tecte la d√©g√©n√©rescence: facteurs qui deviennent trop similaires
    """
    rank = factors[0].shape[1]
    n_modes = len(factors)
    
    degeneracy_scores = []
    
    for mode in range(n_modes):
        factor = factors[mode]
        
        # Similarit√© cosine entre colonnes
        similarities = []
        for i in range(rank):
            for j in range(i+1, rank):
                cos_sim = np.dot(factor[:, i], factor[:, j]) / (
                    np.linalg.norm(factor[:, i]) * np.linalg.norm(factor[:, j])
                )
                similarities.append(abs(cos_sim))
        
        max_sim = max(similarities) if similarities else 0
        degeneracy_scores.append(max_sim)
        
        if max_sim > threshold:
            print(f"Mode {mode}: D√©g√©n√©rescence d√©tect√©e (similarit√© max: {max_sim:.4f})")
    
    return degeneracy_scores

factors_degen = [
    np.random.randn(10, 5),
    np.random.randn(12, 5),
    np.random.randn(8, 5)
]

# Simule d√©g√©n√©rescence
factors_degen[0][:, 1] = 0.99 * factors_degen[0][:, 0] + 0.01 * np.random.randn(10)

scores = detect_cp_degeneracy(factors_degen)
```

---

## Applications en Machine Learning

### Compression de Couches Denses

```python
class CPCompressedLinear(nn.Module):
    """
    Couche lin√©aire compress√©e avec CP
    """
    
    def __init__(self, in_features, out_features, cp_rank):
        super().__init__()
        
        self.in_features = in_features
        self.out_features = out_features
        self.cp_rank = cp_rank
        
        # Factorisation: W = A @ B^T o√π A (out, rank), B (in, rank)
        self.A = nn.Parameter(torch.randn(out_features, cp_rank))
        self.B = nn.Parameter(torch.randn(in_features, cp_rank))
        
        nn.init.xavier_uniform_(self.A)
        nn.init.xavier_uniform_(self.B)
    
    def forward(self, x):
        # y = (A @ B^T) @ x = A @ (B^T @ x)
        return x @ self.B @ self.A.T
    
    @classmethod
    def from_linear(cls, linear_layer, cp_rank):
        """Cr√©e depuis une couche dense via CP"""
        W = linear_layer.weight.data.numpy()
        
        # D√©composition CP (pour matrice, c'est SVD)
        U, S, Vt = np.linalg.svd(W, full_matrices=False)
        
        # Troncature
        U_r = U[:, :cp_rank] * np.sqrt(S[:cp_rank])
        V_r = Vt[:cp_rank, :].T * np.sqrt(S[:cp_rank])
        
        # Cr√©e la couche
        compressed = cls(
            linear_layer.in_features,
            linear_layer.out_features,
            cp_rank
        )
        
        compressed.A.data = torch.from_numpy(U_r).float()
        compressed.B.data = torch.from_numpy(V_r).float()
        
        return compressed
    
    def compression_ratio(self):
        """Ratio de compression"""
        original = self.in_features * self.out_features
        compressed = self.cp_rank * (self.in_features + self.out_features)
        return original / compressed

# Exemple
original = nn.Linear(1024, 512)
compressed = CPCompressedLinear.from_linear(original, cp_rank=64)

print(f"Compression CP:")
print(f"  Original: {original.weight.numel():,} params")
print(f"  Compressed: {compressed.A.numel() + compressed.B.numel():,} params")
print(f"  Ratio: {compressed.compression_ratio():.2f}x")
```

---

## Exercices

### Exercice 5.1.1
Impl√©mentez la d√©composition CP pour un tenseur d'ordre 4 et comparez-la avec la d√©composition Tucker.

### Exercice 5.1.2
V√©rifiez exp√©rimentalement que la d√©composition CP n'est pas unique pour des rangs √©lev√©s (probl√®me de d√©g√©n√©rescence).

### Exercice 5.1.3
Utilisez CP pour compresser un r√©seau de neurones et mesurez la perte de performance.

---

## Points Cl√©s √† Retenir

> üìå **CP est la g√©n√©ralisation la plus directe de la SVD aux tenseurs**

> üìå **L'algorithme ALS est la m√©thode standard pour calculer CP**

> üìå **Le probl√®me de rang CP exact est NP-hard pour ordre ‚â• 3**

> üìå **CP peut √™tre utilis√© pour compresser les couches de r√©seaux de neurones**

---

*Section suivante : [5.2 D√©composition Tucker](./05_02_Tucker.md)*

