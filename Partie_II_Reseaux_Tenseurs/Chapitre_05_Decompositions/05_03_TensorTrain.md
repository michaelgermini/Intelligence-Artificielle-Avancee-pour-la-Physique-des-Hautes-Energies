# 5.3 Tensor Train (TT) / Matrix Product States (MPS)

---

## Introduction

Le **Tensor Train** (TT) ou **Matrix Product State** (MPS) √©vite la mal√©diction de la dimensionnalit√© en repr√©sentant un tenseur comme un produit de tenseurs 3D. C'est particuli√®rement efficace pour les tenseurs de grande dimension.

---

## D√©finition Formelle

Pour un tenseur $\mathcal{T} \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N}$, la d√©composition TT est :

$$\mathcal{T}[i_1, i_2, \ldots, i_N] = \sum_{r_0=1}^{R_0} \sum_{r_1=1}^{R_1} \cdots \sum_{r_{N-1}=1}^{R_{N-1}} G_1[r_0, i_1, r_1] \cdot G_2[r_1, i_2, r_2] \cdots G_N[r_{N-1}, i_N, r_N]$$

En notation matricielle :

$$\mathcal{T}[i_1, \ldots, i_N] = G_1[i_1] \cdot G_2[i_2] \cdots G_N[i_N]$$

o√π $G_k[i_k]$ est une matrice de taille $(R_{k-1}, R_k)$.

---

## Structure TT

```python
import numpy as np

class TensorTrain:
    """
    Repr√©sentation Tensor Train d'un tenseur
    """
    
    def __init__(self, cores):
        """
        Args:
            cores: Liste de tenseurs 3D [G‚ÇÅ, G‚ÇÇ, ..., G‚Çô]
                   G‚Çñ.shape = (r_{k-1}, i_k, r_k)
        """
        self.cores = cores
        self.n_modes = len(cores)
        self.local_dims = [core.shape[1] for core in cores]
        self.ranks = [cores[0].shape[0]] + [core.shape[2] for core in cores]
        
    def reconstruct(self):
        """
        Reconstruit le tenseur complet (co√ªteux!)
        """
        result = self.cores[0]  # Shape: (r‚ÇÄ, i‚ÇÅ, r‚ÇÅ)
        
        for core in self.cores[1:]:
            # Contracte: (..., r_{k-1}) √ó (r_{k-1}, i_k, r_k) ‚Üí (..., i_k, r_k)
            result = np.tensordot(result, core, axes=([-1], [0]))
        
        # Squeeze les dimensions de liaison aux bords
        return result.squeeze()
    
    def count_parameters(self):
        """Compte le nombre de param√®tres"""
        return sum(core.size for core in self.cores)
    
    def full_tensor_size(self):
        """Taille du tenseur complet"""
        return np.prod(self.local_dims)

# Exemple
cores = [
    np.random.randn(1, 5, 4),   # G‚ÇÅ: (1, 5, 4)
    np.random.randn(4, 6, 3),   # G‚ÇÇ: (4, 6, 3)
    np.random.randn(3, 7, 1)    # G‚ÇÉ: (3, 7, 1)
]

tt = TensorTrain(cores)
print(f"Tensor Train:")
print(f"  Cores: {len(cores)}")
print(f"  TT-ranks: {tt.ranks}")
print(f"  Param√®tres: {tt.count_parameters():,}")
print(f"  Taille compl√®te: {tt.full_tensor_size():,}")
print(f"  Compression: {tt.full_tensor_size() / tt.count_parameters():.1f}x")
```

---

## TT-SVD

```python
def tt_svd(tensor, max_rank=None, epsilon=1e-10):
    """
    D√©composition TT via SVD (TT-SVD)
    
    Algorithm:
    R√©p√®te pour chaque mode:
    1. Reshape en matrice
    2. SVD
    3. Tronque selon max_rank ou epsilon
    4. Continue avec le reste
    """
    shape = tensor.shape
    n_modes = len(shape)
    
    cores = []
    remainder = tensor.copy()
    rank_left = 1
    
    for k in range(n_modes - 1):
        # Reshape en matrice: (r_{k-1} √ó i_k, i_{k+1} √ó ... √ó i_N)
        remainder = remainder.reshape(rank_left * shape[k], -1)
        
        # SVD
        U, S, Vt = np.linalg.svd(remainder, full_matrices=False)
        
        # D√©termine le rang
        if max_rank is not None:
            if isinstance(max_rank, (list, tuple)):
                rank = min(max_rank[k] if k < len(max_rank) else len(S), len(S))
            else:
                rank = min(max_rank, len(S))
        else:
            # Par epsilon: garde les valeurs singuli√®res > epsilon * S[0]
            rank = np.sum(S > epsilon * S[0])
            rank = max(1, rank)
        
        # Tronque
        U = U[:, :rank]
        S = S[:rank]
        Vt = Vt[:rank, :]
        
        # Core k: (r_{k-1}, i_k, r_k)
        core = U.reshape(rank_left, shape[k], rank)
        cores.append(core)
        
        # Pr√©pare pour l'it√©ration suivante
        remainder = np.diag(S) @ Vt
        rank_left = rank
    
    # Dernier core
    cores.append(remainder.reshape(rank_left, shape[-1], 1))
    
    return cores

# Exemple
tensor = np.random.randn(10, 12, 8, 6)
cores = tt_svd(tensor, max_rank=[5, 5, 5])

tt = TensorTrain(cores)
print(f"TT-SVD d√©composition:")
print(f"  Tenseur original: {tensor.shape} ({tensor.size:,} √©l√©ments)")
print(f"  TT cores: {[c.shape for c in cores]}")
print(f"  Param√®tres TT: {tt.count_parameters():,}")
print(f"  Compression: {tt.full_tensor_size() / tt.count_parameters():.1f}x")

# Reconstruction
reconstructed = tt.reconstruct()
error = np.linalg.norm(tensor - reconstructed, 'fro') / np.linalg.norm(tensor, 'fro')
print(f"  Erreur relative: {error:.6f}")
```

---

## Op√©rations sur TT

### Addition

```python
def tt_add(tt1, tt2):
    """
    Addition de deux Tensor Trains
    
    Les rangs peuvent √™tre diff√©rents
    """
    # Concat√®ne les cores (augmente les rangs)
    new_cores = []
    
    for i, (c1, c2) in enumerate(zip(tt1.cores, tt2.cores)):
        if i == 0:
            # Premier core: concat√®ne verticalement
            new_core = np.concatenate([c1, c2], axis=2)  # Concat√®ne sur r_k
        elif i == len(tt1.cores) - 1:
            # Dernier core: concat√®ne horizontalement
            new_core = np.concatenate([c1, c2], axis=0)  # Concat√®ne sur r_{k-1}
        else:
            # Cores interm√©diaires: bloc diagonal
            r1_left, i, r1_right = c1.shape
            r2_left, _, r2_right = c2.shape
            
            new_core = np.zeros((r1_left + r2_left, i, r1_right + r2_right))
            new_core[:r1_left, :, :r1_right] = c1
            new_core[r1_left:, :, r1_right:] = c2
        
        new_cores.append(new_core)
    
    return TensorTrain(new_cores)
```

### Contraction

```python
def tt_contract(tt1, tt2, modes_to_contract):
    """
    Contraction de deux Tensor Trains
    
    Contracte sur certains modes sp√©cifi√©s
    """
    # Impl√©mentation simplifi√©e
    # La contraction TT est complexe car les rangs doivent √™tre compatibles
    pass
```

---

## Applications en ML

### Compression de Matrices en TT

```python
class TTLinear(nn.Module):
    """
    Couche lin√©aire avec poids en format Tensor Train
    """
    
    def __init__(self, input_dims, output_dims, tt_ranks):
        """
        Args:
            input_dims: tuple (d‚ÇÅ, d‚ÇÇ, ..., d‚Çô) pour reshape de l'input
            output_dims: tuple (d'‚ÇÅ, d'‚ÇÇ, ..., d'‚Çò) pour reshape de l'output
            tt_ranks: rangs TT
        """
        super().__init__()
        
        self.input_dims = input_dims
        self.output_dims = output_dims
        self.input_size = np.prod(input_dims)
        self.output_size = np.prod(output_dims)
        
        # Cr√©e les cores TT pour repr√©senter la matrice de poids
        # W: (output_size, input_size) = (‚àèd'·µ¢, ‚àèd‚±º)
        # Reshape en tenseur: (d'‚ÇÅ, ..., d'‚Çò, d‚ÇÅ, ..., d‚Çô)
        
        # Cores pour les dimensions d'entr√©e
        self.input_cores = nn.ModuleList()
        prev_rank = 1
        for i, dim in enumerate(input_dims):
            next_rank = tt_ranks[i] if i < len(tt_ranks) else 1
            core = nn.Parameter(torch.randn(prev_rank, dim, next_rank))
            self.input_cores.append(core)
            prev_rank = next_rank
        
        # Cores pour les dimensions de sortie
        self.output_cores = nn.ModuleList()
        for i, dim in enumerate(output_dims):
            next_rank = tt_ranks[len(input_dims) + i] if (len(input_dims) + i) < len(tt_ranks) else 1
            core = nn.Parameter(torch.randn(prev_rank, dim, next_rank))
            self.output_cores.append(core)
            prev_rank = next_rank
        
    def forward(self, x):
        """
        Forward pass avec contraction TT
        """
        batch_size = x.shape[0]
        
        # Reshape input
        x = x.view(batch_size, *self.input_dims)
        
        # Contracte avec les cores d'entr√©e
        result = x
        for i, core in enumerate(self.input_cores):
            # Contracte sur la dimension i+1 (apr√®s batch)
            result = torch.tensordot(result, core, dims=([i+1], [1]))
            # R√©arrange les dimensions
        
        # Contracte avec les cores de sortie
        for core in self.output_cores:
            result = torch.tensordot(result, core, dims=([1], [0]))
        
        # Reshape final
        result = result.view(batch_size, self.output_size)
        
        return result

# Exemple: compresser 1024 ‚Üí 512
# Reshape: 1024 = 16√ó16√ó4, 512 = 16√ó16√ó2
tt_linear = TTLinear(
    input_dims=(16, 16, 4),
    output_dims=(16, 16, 2),
    tt_ranks=[4, 4, 4, 4]
)

print(f"TT-Linear:")
print(f"  Input: {tt_linear.input_size}, Output: {tt_linear.output_size}")
total_params = sum(c.numel() for cores in [tt_linear.input_cores, tt_linear.output_cores] for c in cores)
print(f"  Param√®tres: {total_params:,}")
print(f"  vs dense: {tt_linear.input_size * tt_linear.output_size:,}")
print(f"  Compression: {tt_linear.input_size * tt_linear.output_size / total_params:.2f}x")
```

---

## Optimisation et Rounding

```python
def tt_rounding(tt, max_rank):
    """
    Arrondit un TT pour r√©duire les rangs
    
    Utilise SVD pour compresser chaque core
    """
    cores = tt.cores.copy()
    n_modes = len(cores)
    
    # Passe gauche-√†-droite: r√©duit les rangs
    for k in range(n_modes - 1):
        core = cores[k]  # (r_{k-1}, i_k, r_k)
        
        # Reshape en matrice: (r_{k-1} * i_k, r_k)
        matrix = core.reshape(-1, core.shape[2])
        
        # SVD
        U, S, Vt = np.linalg.svd(matrix, full_matrices=False)
        
        # Tronque
        rank = min(max_rank, len(S))
        U = U[:, :rank]
        S = S[:rank]
        Vt = Vt[:rank, :]
        
        # Mise √† jour
        cores[k] = U.reshape(core.shape[0], core.shape[1], rank)
        # S * Vt va dans le core suivant
        if k < n_modes - 1:
            cores[k+1] = np.tensordot(
                np.diag(S) @ Vt,
                cores[k+1],
                axes=([1], [0])
            )
    
    return TensorTrain(cores)
```

---

## Exercices

### Exercice 5.3.1
Impl√©mentez TT-SVD pour un tenseur d'ordre 5 et comparez l'erreur de reconstruction avec diff√©rents rangs.

### Exercice 5.3.2
Cr√©ez une couche lin√©aire TT et comparez ses performances avec une couche dense standard apr√®s entra√Ænement.

### Exercice 5.3.3
Impl√©mentez le rounding TT et testez son effet sur l'erreur de reconstruction.

---

## Points Cl√©s √† Retenir

> üìå **TT √©vite la mal√©diction de la dimensionnalit√© avec une complexit√© lin√©aire en d**

> üìå **TT-SVD donne la meilleure approximation de rang donn√©**

> üìå **Les op√©rations sur TT (addition, contraction) sont efficaces**

> üìå **TT est particuli√®rement adapt√© pour compresser les grandes matrices**

---

*Section suivante : [5.4 Hierarchical Tucker](./05_04_HierarchicalTucker.md)*

