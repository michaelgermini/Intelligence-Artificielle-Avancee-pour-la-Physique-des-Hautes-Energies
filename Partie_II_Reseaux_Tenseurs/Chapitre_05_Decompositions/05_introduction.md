# Chapitre 5 : DÃ©compositions Tensorielles Fondamentales

---

## Introduction

Les **dÃ©compositions tensorielles** sont au cÅ“ur de la compression de modÃ¨les par rÃ©seaux de tenseurs. Ce chapitre prÃ©sente les principales dÃ©compositions : CP, Tucker, Tensor Train, et leurs variantes.

---

## Plan du Chapitre

1. [DÃ©composition CP (CANDECOMP/PARAFAC)](./05_01_CP.md)
2. [DÃ©composition Tucker](./05_02_Tucker.md)
3. [Tensor Train / Matrix Product States](./05_03_TensorTrain.md)
4. [Hierarchical Tucker](./05_04_HierarchicalTucker.md)
5. [Tensor Ring Decomposition](./05_05_TensorRing.md)
6. [Comparaison et Choix de DÃ©composition](./05_06_Comparaison.md)

---

## Vue d'Ensemble des DÃ©compositions

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DÃ©compositions Tensorielles                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  CP (Canonical Polyadic):                                       â”‚
â”‚    T = Î£áµ£ aáµ£ âŠ— báµ£ âŠ— cáµ£                                         â”‚
â”‚    ParamÃ¨tres: R(nâ‚ + nâ‚‚ + nâ‚ƒ)                                 â”‚
â”‚                                                                 â”‚
â”‚  Tucker:                                                        â”‚
â”‚    T = G Ã—â‚ A Ã—â‚‚ B Ã—â‚ƒ C                                        â”‚
â”‚    ParamÃ¨tres: râ‚râ‚‚râ‚ƒ + nâ‚râ‚ + nâ‚‚râ‚‚ + nâ‚ƒrâ‚ƒ                    â”‚
â”‚                                                                 â”‚
â”‚  Tensor Train (TT):                                             â”‚
â”‚    T[iâ‚,...,iâ‚™] = Gâ‚[iâ‚] Gâ‚‚[iâ‚‚] ... Gâ‚™[iâ‚™]                    â”‚
â”‚    ParamÃ¨tres: Î£â‚– râ‚–â‚‹â‚ nâ‚– râ‚–                                   â”‚
â”‚                                                                 â”‚
â”‚  Tensor Ring (TR):                                              â”‚
â”‚    Comme TT mais avec trace finale                             â”‚
â”‚    Plus flexible, moins de contraintes aux bords               â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## DÃ©composition CP

```python
import numpy as np
from scipy.linalg import khatri_rao

def cp_decomposition_als(tensor, rank, max_iter=100, tol=1e-6):
    """
    DÃ©composition CP par Alternating Least Squares (ALS)
    
    T â‰ˆ Î£áµ£ Î»áµ£ aâ½Ê³â¾ âŠ— bâ½Ê³â¾ âŠ— câ½Ê³â¾
    """
    shape = tensor.shape
    n_modes = len(shape)
    
    # Initialisation alÃ©atoire des facteurs
    factors = [np.random.randn(shape[i], rank) for i in range(n_modes)]
    
    for iteration in range(max_iter):
        for mode in range(n_modes):
            # Matricisation selon le mode
            unfolding = unfold(tensor, mode)
            
            # Produit Khatri-Rao des autres facteurs
            kr_product = khatri_rao_except(factors, mode)
            
            # Mise Ã  jour du facteur (moindres carrÃ©s)
            factors[mode] = unfolding @ kr_product @ np.linalg.pinv(
                kr_product.T @ kr_product
            )
        
        # VÃ©rification de convergence
        reconstruction = cp_reconstruct(factors)
        error = np.linalg.norm(tensor - reconstruction) / np.linalg.norm(tensor)
        
        if error < tol:
            break
    
    return factors, error

def unfold(tensor, mode):
    """Matricisation selon un mode"""
    return np.moveaxis(tensor, mode, 0).reshape(tensor.shape[mode], -1)

def khatri_rao_except(factors, skip_mode):
    """Produit Khatri-Rao de tous les facteurs sauf un"""
    result = None
    for i, factor in enumerate(factors):
        if i != skip_mode:
            if result is None:
                result = factor
            else:
                result = khatri_rao(result, factor)
    return result

def cp_reconstruct(factors):
    """Reconstruit le tenseur Ã  partir des facteurs CP"""
    result = factors[0]
    for factor in factors[1:]:
        result = np.einsum('ir,jr->ijr', result, factor)
    return result.sum(axis=-1)
```

---

## DÃ©composition Tucker

```python
def tucker_decomposition(tensor, ranks, max_iter=100):
    """
    DÃ©composition Tucker par Higher-Order SVD (HOSVD)
    
    T â‰ˆ G Ã—â‚ Uâ‚ Ã—â‚‚ Uâ‚‚ Ã—â‚ƒ Uâ‚ƒ
    
    G: tenseur noyau de shape (râ‚, râ‚‚, râ‚ƒ)
    Uâ‚–: matrices facteurs de shape (nâ‚–, râ‚–)
    """
    n_modes = tensor.ndim
    
    # HOSVD: SVD de chaque matricisation
    factors = []
    for mode in range(n_modes):
        unfolding = unfold(tensor, mode)
        U, _, _ = np.linalg.svd(unfolding, full_matrices=False)
        factors.append(U[:, :ranks[mode]])
    
    # Calcul du noyau
    core = tensor.copy()
    for mode in range(n_modes):
        core = mode_n_product(core, factors[mode].T, mode)
    
    return core, factors

def mode_n_product(tensor, matrix, mode):
    """Produit mode-n : T Ã—â‚™ M"""
    # DÃ©place le mode en premiÃ¨re position
    tensor = np.moveaxis(tensor, mode, 0)
    shape = tensor.shape
    
    # Reshape pour multiplication matricielle
    tensor = tensor.reshape(shape[0], -1)
    result = matrix @ tensor
    
    # Reshape et remet le mode Ã  sa place
    new_shape = (matrix.shape[0],) + shape[1:]
    result = result.reshape(new_shape)
    result = np.moveaxis(result, 0, mode)
    
    return result

def tucker_reconstruct(core, factors):
    """Reconstruit le tenseur Ã  partir de la dÃ©composition Tucker"""
    result = core.copy()
    for mode, factor in enumerate(factors):
        result = mode_n_product(result, factor, mode)
    return result
```

---

## Tensor Train (TT)

```python
def tt_decomposition(tensor, max_rank=None, tol=1e-10):
    """
    DÃ©composition Tensor Train par TT-SVD
    
    T[iâ‚,...,iâ‚™] = Gâ‚[iâ‚] Ã— Gâ‚‚[iâ‚‚] Ã— ... Ã— Gâ‚™[iâ‚™]
    
    Gâ‚–: tenseur 3D de shape (râ‚–â‚‹â‚, nâ‚–, râ‚–)
    """
    shape = tensor.shape
    n_modes = len(shape)
    
    if max_rank is None:
        max_rank = [None] * (n_modes - 1)
    
    cores = []
    remainder = tensor.copy()
    rank_left = 1
    
    for k in range(n_modes - 1):
        # Reshape en matrice
        remainder = remainder.reshape(rank_left * shape[k], -1)
        
        # SVD tronquÃ©e
        U, S, Vt = np.linalg.svd(remainder, full_matrices=False)
        
        # DÃ©termine le rang (par seuil ou max_rank)
        if max_rank[k] is not None:
            rank = min(max_rank[k], len(S))
        else:
            rank = np.sum(S > tol * S[0])
        rank = max(1, rank)
        
        # Tronque
        U = U[:, :rank]
        S = S[:rank]
        Vt = Vt[:rank, :]
        
        # Core k
        core = U.reshape(rank_left, shape[k], rank)
        cores.append(core)
        
        # PrÃ©pare pour l'itÃ©ration suivante
        remainder = np.diag(S) @ Vt
        rank_left = rank
    
    # Dernier core
    cores.append(remainder.reshape(rank_left, shape[-1], 1))
    
    return cores

def tt_reconstruct(cores):
    """Reconstruit le tenseur Ã  partir des cores TT"""
    result = cores[0]
    for core in cores[1:]:
        # Contraction sur le dernier indice de result et le premier de core
        result = np.tensordot(result, core, axes=([-1], [0]))
    return result.squeeze()

def tt_ranks(cores):
    """Retourne les rangs TT"""
    return [core.shape[2] for core in cores[:-1]]

# Exemple
tensor = np.random.randn(10, 12, 8, 6)
cores = tt_decomposition(tensor, max_rank=[5, 5, 5])

print("Tensor Train Decomposition:")
print(f"  Original shape: {tensor.shape}")
print(f"  TT-ranks: {tt_ranks(cores)}")
print(f"  Core shapes: {[c.shape for c in cores]}")

# Compression
original_params = tensor.size
tt_params = sum(c.size for c in cores)
print(f"  ParamÃ¨tres: {original_params:,} â†’ {tt_params:,} ({tt_params/original_params:.1%})")

# Erreur
reconstruction = tt_reconstruct(cores)
error = np.linalg.norm(tensor - reconstruction) / np.linalg.norm(tensor)
print(f"  Erreur relative: {error:.2e}")
```

---

## Comparaison des DÃ©compositions

```python
def compare_decompositions(tensor, ranks):
    """
    Compare les diffÃ©rentes dÃ©compositions sur un mÃªme tenseur
    """
    results = {}
    
    # CP
    cp_rank = ranks.get('cp', 10)
    factors, cp_error = cp_decomposition_als(tensor, cp_rank)
    cp_params = sum(f.size for f in factors)
    results['CP'] = {'error': cp_error, 'params': cp_params}
    
    # Tucker
    tucker_ranks = ranks.get('tucker', [5, 5, 5])
    core, factors = tucker_decomposition(tensor, tucker_ranks)
    tucker_recon = tucker_reconstruct(core, factors)
    tucker_error = np.linalg.norm(tensor - tucker_recon) / np.linalg.norm(tensor)
    tucker_params = core.size + sum(f.size for f in factors)
    results['Tucker'] = {'error': tucker_error, 'params': tucker_params}
    
    # TT
    tt_max_ranks = ranks.get('tt', [5, 5])
    cores = tt_decomposition(tensor, max_rank=tt_max_ranks)
    tt_recon = tt_reconstruct(cores)
    tt_error = np.linalg.norm(tensor - tt_recon) / np.linalg.norm(tensor)
    tt_params = sum(c.size for c in cores)
    results['TT'] = {'error': tt_error, 'params': tt_params}
    
    # Affichage
    print("Comparaison des dÃ©compositions:")
    print(f"Tenseur original: {tensor.shape}, {tensor.size:,} Ã©lÃ©ments")
    print("\n{:10} | {:>12} | {:>12} | {:>10}".format(
        "MÃ©thode", "ParamÃ¨tres", "Compression", "Erreur"))
    print("-" * 50)
    
    for name, res in results.items():
        compression = tensor.size / res['params']
        print(f"{name:10} | {res['params']:>12,} | {compression:>10.1f}x | {res['error']:>10.2e}")
    
    return results

# Test
tensor = np.random.randn(20, 25, 30)
ranks = {'cp': 10, 'tucker': [8, 8, 8], 'tt': [10, 10]}
compare_decompositions(tensor, ranks)
```

---

## Points ClÃ©s Ã  Retenir

> ğŸ“Œ **CP donne la reprÃ©sentation la plus compacte mais est NP-hard Ã  calculer optimalement**

> ğŸ“Œ **Tucker est flexible mais le noyau peut Ãªtre grand (curse of dimensionality)**

> ğŸ“Œ **TT Ã©vite la malÃ©diction de la dimensionnalitÃ© avec une complexitÃ© linÃ©aire en d**

> ğŸ“Œ **Le choix dÃ©pend de la structure des donnÃ©es et des contraintes de l'application**

---

*Chapitre suivant : [Chapitre 6 - RÃ©seaux de Tenseurs en Physique Quantique](../Chapitre_06_Physique_Quantique/06_introduction.md)*

