# Chapitre 6 : RÃ©seaux de Tenseurs en Physique Quantique

---

## Introduction

Les **rÃ©seaux de tenseurs** ont Ã©tÃ© initialement dÃ©veloppÃ©s en physique quantique pour reprÃ©senter efficacement les Ã©tats quantiques. Ces techniques se sont rÃ©vÃ©lÃ©es extrÃªmement puissantes et ont trouvÃ© des applications naturelles en machine learning.

---

## Plan du Chapitre

1. [Ã‰tats Produits Matriciels (MPS)](./06_01_MPS.md)
2. [Ã‰tats ProjetÃ©s par Paires EntrelacÃ©es (PEPS)](./06_02_PEPS.md)
3. [MERA (Multi-scale Entanglement Renormalization Ansatz)](./06_03_MERA.md)
4. [Applications en MÃ©canique Quantique](./06_04_Mecanique_Quantique.md)
5. [Connexions avec l'Apprentissage Automatique](./06_05_ML_Connexions.md)

---

## Motivation : ProblÃ¨me de la Dimension Exponentielle

En mÃ©canique quantique, l'Ã©tat d'un systÃ¨me Ã  $n$ particules nÃ©cessite :

$$2^n \text{ coefficients} \quad \text{(pour des spins 1/2)}$$

Pour $n=100$, cela reprÃ©sente $2^{100} \approx 10^{30}$ nombres !

Les rÃ©seaux de tenseurs permettent de reprÃ©senter ces Ã©tats avec un nombre polynomial de paramÃ¨tres.

---

## Ã‰tats MPS (Matrix Product States)

### DÃ©finition

Un MPS reprÃ©sente un Ã©tat quantique comme :

$$|\psi\rangle = \sum_{i_1,\ldots,i_n} \text{Tr}(A^{[1]}_{i_1} A^{[2]}_{i_2} \cdots A^{[n]}_{i_n}) |i_1 i_2 \cdots i_n\rangle$$

oÃ¹ chaque $A^{[k]}_{i_k}$ est une matrice.

```python
import numpy as np

class MPSState:
    """
    ReprÃ©sente un Ã©tat quantique en format MPS
    """
    
    def __init__(self, local_dims, bond_dims):
        """
        Args:
            local_dims: dimensions locales (dâ‚, dâ‚‚, ..., dâ‚™)
            bond_dims: dimensions de liaison (Ï‡â‚, Ï‡â‚‚, ..., Ï‡â‚™â‚‹â‚)
        """
        self.n_sites = len(local_dims)
        self.local_dims = local_dims
        self.bond_dims = bond_dims
        
        # Initialise les matrices
        self.tensors = []
        for i in range(self.n_sites):
            bond_left = 1 if i == 0 else bond_dims[i-1]
            bond_right = 1 if i == self.n_sites-1 else bond_dims[i]
            physical = local_dims[i]
            
            # Matrices alÃ©atoires normalisÃ©es
            tensor = np.random.randn(bond_left, physical, bond_right)
            tensor = tensor / np.linalg.norm(tensor)
            self.tensors.append(tensor)
    
    def contract_to_full_state(self):
        """
        Contracte le MPS pour obtenir l'Ã©tat complet (coÃ»teux!)
        """
        result = self.tensors[0]  # Shape: (1, dâ‚, Ï‡â‚)
        
        for i in range(1, self.n_sites):
            # Contracte avec le tenseur suivant
            # result: (..., Ï‡_{i-1})
            # tensor: (Ï‡_{i-1}, d_i, Ï‡_i)
            result = np.tensordot(result, self.tensors[i], axes=([-1], [0]))
        
        # Squeeze les dimensions de liaison aux bords
        return result.squeeze()
    
    def compute_norm(self):
        """Calcule la norme de l'Ã©tat"""
        # Pour un MPS normalisÃ©, la norme devrait Ãªtre ~1
        state = self.contract_to_full_state()
        return np.linalg.norm(state.flatten())
    
    def count_parameters(self):
        """Compte le nombre de paramÃ¨tres"""
        total = 0
        for i, tensor in enumerate(self.tensors):
            total += tensor.size
        return total
    
    def full_state_size(self):
        """Taille de l'Ã©tat complet (non compressÃ©)"""
        return np.prod(self.local_dims)

# Exemple: 10 spins 1/2
mps = MPSState(local_dims=[2]*10, bond_dims=[4]*9)

print("Ã‰tat MPS:")
print(f"  Nombre de sites: {mps.n_sites}")
print(f"  ParamÃ¨tres MPS: {mps.count_parameters():,}")
print(f"  Taille Ã©tat complet: {mps.full_state_size():,}")
print(f"  Compression: {mps.full_state_size() / mps.count_parameters():.1f}x")
print(f"  Norme: {mps.compute_norm():.4f}")
```

---

## Ã‰volution Temporelle avec MPS

```python
class MPSEvolution:
    """
    Ã‰volution temporelle d'un Ã©tat MPS
    """
    
    @staticmethod
    def apply_operator_local(mps, operator, site):
        """
        Applique un opÃ©rateur local au site 'site'
        
        operator: matrice (d, d) agissant sur l'espace local
        """
        tensor = mps.tensors[site]  # (Ï‡_left, d, Ï‡_right)
        
        # Contracte l'opÃ©rateur avec le tenseur
        # Nouvelle forme: (Ï‡_left, d, Ï‡_right)
        new_tensor = np.tensordot(tensor, operator, axes=([1], [1]))
        new_tensor = np.moveaxis(new_tensor, -1, 1)
        
        mps.tensors[site] = new_tensor
        return mps
    
    @staticmethod
    def apply_two_site_operator(mps, operator, sites):
        """
        Applique un opÃ©rateur Ã  deux sites
        
        operator: tenseur (dâ‚, dâ‚‚, dâ‚', dâ‚‚')
        """
        i, j = sites
        
        # Contracte les deux tenseurs
        left_tensor = mps.tensors[i]  # (Ï‡_{i-1}, d_i, Ï‡_i)
        right_tensor = mps.tensors[j]  # (Ï‡_{j-1}, d_j, Ï‡_j)
        
        # Fusionne temporairement
        merged = np.tensordot(left_tensor, right_tensor, axes=([-1], [0]))
        # Shape: (Ï‡_{i-1}, d_i, Ï‡_i, d_j, Ï‡_j)
        
        # Applique l'opÃ©rateur
        # (Complexe, nÃ©cessite reshape appropriÃ©)
        
        # DÃ©compose avec SVD pour maintenir le format MPS
        
        return mps

# Exemple: rotation d'un spin
pauli_x = np.array([[0, 1], [1, 0]], dtype=complex)
pauli_y = np.array([[0, -1j], [1j, 0]], dtype=complex)
pauli_z = np.array([[1, 0], [0, -1]], dtype=complex)

# Rotation autour de l'axe X
theta = np.pi / 4
rotation = np.cos(theta/2) * np.eye(2) - 1j * np.sin(theta/2) * pauli_x

mps_test = MPSState([2]*5, [4]*4)
MPSEvolution.apply_operator_local(mps_test, rotation, site=2)
```

---

## PEPS (Projected Entangled Pair States)

### Introduction

Les **PEPS** gÃ©nÃ©ralisent les MPS aux dimensions supÃ©rieures (2D, 3D).

```python
class PEPSState:
    """
    PEPS pour systÃ¨mes 2D
    
    Chaque site a des connexions avec ses voisins (haut, bas, gauche, droite)
    """
    
    def __init__(self, lattice_shape, physical_dim, bond_dim):
        """
        Args:
            lattice_shape: (Lx, Ly) dimensions du rÃ©seau 2D
            physical_dim: dimension de l'espace local
            bond_dim: dimension des liens
        """
        self.Lx, self.Ly = lattice_shape
        self.physical_dim = physical_dim
        self.bond_dim = bond_dim
        
        # Tenseur par site: (bond_up, bond_down, bond_left, bond_right, physical)
        self.tensors = {}
        
        for x in range(self.Lx):
            for y in range(self.Ly):
                # Dimensions des liens (1 aux bords)
                bond_up = 1 if y == 0 else bond_dim
                bond_down = 1 if y == self.Ly-1 else bond_dim
                bond_left = 1 if x == 0 else bond_dim
                bond_right = 1 if x == self.Lx-1 else bond_dim
                
                tensor = np.random.randn(
                    bond_up, bond_down, bond_left, bond_right, physical_dim
                )
                self.tensors[(x, y)] = tensor
    
    def count_parameters(self):
        """Compte les paramÃ¨tres"""
        return sum(t.size for t in self.tensors.values())
    
    def full_state_size(self):
        """Taille de l'Ã©tat complet"""
        return self.physical_dim ** (self.Lx * self.Ly)

# Exemple: rÃ©seau 4Ã—4
peps = PEPSState(lattice_shape=(4, 4), physical_dim=2, bond_dim=3)

print("Ã‰tat PEPS:")
print(f"  RÃ©seau: {peps.Lx}Ã—{peps.Ly}")
print(f"  ParamÃ¨tres: {peps.count_parameters():,}")
print(f"  Ã‰tat complet: {peps.full_state_size():,}")
print(f"  Compression: {peps.full_state_size() / peps.count_parameters():.2e}x")
```

---

## MERA (Multi-scale Entanglement Renormalization Ansatz)

### Principe

MERA utilise une structure hiÃ©rarchique pour capturer l'intrication Ã  toutes les Ã©chelles.

```python
class MERAState:
    """
    MERA: structure hiÃ©rarchique pour l'intrication multi-Ã©chelle
    """
    
    def __init__(self, n_sites, bond_dim, n_layers):
        """
        Args:
            n_sites: nombre de sites physiques
            bond_dim: dimension de liaison
            n_layers: nombre de couches de rÃ©normalisation
        """
        self.n_sites = n_sites
        self.bond_dim = bond_dim
        self.n_layers = n_layers
        
        # Disentanglers et isometries pour chaque couche
        self.disentanglers = []
        self.isometries = []
        
        current_sites = n_sites
        
        for layer in range(n_layers):
            # Disentanglers: unitaires sur paires de sites
            n_pairs = current_sites // 2
            layer_disentanglers = []
            for _ in range(n_pairs):
                # Unitaire (bond_dimÂ², bond_dimÂ²)
                U = self._random_unitary(bond_dim ** 2)
                layer_disentanglers.append(U)
            self.disentanglers.append(layer_disentanglers)
            
            # Isometries: projection vers couche supÃ©rieure
            layer_isometries = []
            for _ in range(n_pairs):
                # IsomÃ©trie (bond_dim, bond_dim, bond_dim)
                V = self._random_isometry(bond_dim, bond_dim, bond_dim)
                layer_isometries.append(V)
            self.isometries.append(layer_isometries)
            
            current_sites = current_sites // 2
    
    @staticmethod
    def _random_unitary(n):
        """GÃ©nÃ¨re une matrice unitaire alÃ©atoire"""
        A = np.random.randn(n, n) + 1j * np.random.randn(n, n)
        Q, R = np.linalg.qr(A)
        return Q
    
    @staticmethod
    def _random_isometry(n_in1, n_in2, n_out):
        """GÃ©nÃ¨re une isomÃ©trie alÃ©atoire"""
        # V: (n_in1, n_in2, n_out) tel que Vâ€ V = I
        V = np.random.randn(n_in1, n_in2, n_out) + 1j * np.random.randn(n_in1, n_in2, n_out)
        # Normalisation (approximation)
        V = V / np.linalg.norm(V)
        return V

# Exemple
mera = MERAState(n_sites=8, bond_dim=2, n_layers=3)
print(f"MERA: {mera.n_sites} sites, {mera.n_layers} couches")
```

---

## Connexions avec le Machine Learning

### Analogies

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Analogies Physique Quantique â†” ML                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Physique Quantique          â”‚  Machine Learning              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Ã‰tat quantique |ÏˆâŸ©          â”‚  Vecteur de features           â”‚
â”‚  Intrication (entanglement)   â”‚  CorrÃ©lations complexes        â”‚
â”‚  RÃ©seau MPS/PEPS             â”‚  Architecture Tensor Train     â”‚
â”‚  Ã‰volution temporelle         â”‚  Forward pass                  â”‚
â”‚  RÃ©duction de dimension      â”‚  Compression de modÃ¨le         â”‚
â”‚  Ansatz variationnel         â”‚  Approximateur universel       â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Utilisation en Deep Learning

```python
def mps_as_neural_network(input_dim, output_dim, hidden_dims, bond_dim):
    """
    Utilise un MPS comme couche de rÃ©seau de neurones
    
    L'Ã©tat quantique devient le vecteur de features
    """
    n_sites = input_dim
    
    # MPS avec dimensions locales = dimensions d'entrÃ©e
    mps = MPSState(
        local_dims=hidden_dims,
        bond_dims=[bond_dim] * (len(hidden_dims) - 1)
    )
    
    # Pour l'infÃ©rence:
    # 1. Encode l'input dans les indices physiques
    # 2. Contracte le MPS
    # 3. Lit la sortie
    
    return mps

# Application: classification avec MPS
class MPSClassifier:
    """
    Classificateur utilisant un MPS
    """
    
    def __init__(self, input_dim, n_classes, bond_dim):
        self.input_dim = input_dim
        self.n_classes = n_classes
        self.bond_dim = bond_dim
        
        # MPS avec dimension locale = input_dim
        self.mps = MPSState(
            local_dims=[input_dim],
            bond_dims=[]
        )
        
        # Couche de sortie
        self.classifier = nn.Linear(input_dim, n_classes)
    
    def forward(self, x):
        """
        Forward pass
        
        x: (batch, input_dim)
        """
        # Encode l'input dans le MPS (simplifiÃ©)
        # Contracte le MPS
        # Classification
        return self.classifier(x)
```

---

## Exercices

### Exercice 6.1
ImplÃ©mentez l'application d'un opÃ©rateur Ã  deux sites sur un MPS avec dÃ©composition SVD pour maintenir le format.

### Exercice 6.2
Comparez le nombre de paramÃ¨tres d'un MPS vs Ã©tat complet pour 20 spins 1/2 avec diffÃ©rents rangs de liaison.

### Exercice 6.3
CrÃ©ez une fonction qui convertit un Ã©tat quantique complet (petit) en format MPS via SVD.

---

## Points ClÃ©s Ã  Retenir

> ðŸ“Œ **Les MPS Ã©vitent la malÃ©diction de la dimensionnalitÃ© pour les systÃ¨mes 1D**

> ðŸ“Œ **Les PEPS gÃ©nÃ©ralisent aux dimensions supÃ©rieures mais sont plus complexes**

> ðŸ“Œ **MERA capture l'intrication Ã  toutes les Ã©chelles**

> ðŸ“Œ **Les techniques de physique quantique inspirent directement le ML moderne**

---

*Section suivante : [6.1 Ã‰tats Produits Matriciels (MPS)](./06_01_MPS.md)*

