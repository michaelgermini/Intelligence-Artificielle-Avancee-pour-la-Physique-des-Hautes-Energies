# 16.5 Co-design Mod√®le-Hardware

---

## Introduction

Le **co-design mod√®le-hardware** consiste √† optimiser simultan√©ment l'architecture du mod√®le de machine learning et la configuration du hardware cible. Au lieu d'optimiser le mod√®le puis de l'adapter au hardware, cette approche optimise les deux en parall√®le pour obtenir de meilleures performances globales.

Cette section pr√©sente les principes du co-design, les m√©thodes pour optimiser simultan√©ment mod√®le et hardware, et des applications pratiques pour FPGA et syst√®mes embarqu√©s.

---

## Principes du Co-design

### Vue d'Ensemble

```python
class CoDesignPrinciples:
    """
    Principes fondamentaux du co-design mod√®le-hardware
    """
    
    def __init__(self):
        self.principles = {
            'joint_optimization': {
                'description': 'Optimisation simultan√©e mod√®le + hardware',
                'advantage': 'Meilleure solution globale que optimisation s√©par√©e',
                'challenge': 'Espace de recherche combin√© tr√®s large',
                'example': 'Optimiser architecture CNN + parall√©lisme FPGA simultan√©ment'
            },
            'hardware_aware_training': {
                'description': 'Entra√Ænement avec contraintes hardware',
                'advantage': 'Mod√®le appris pour √™tre efficace sur hardware cible',
                'challenge': 'Simulation hardware pendant entra√Ænement',
                'example': 'Entra√Æner avec latence/√©nergie comme r√©gularisation'
            },
            'adaptive_mapping': {
                'description': 'Mapping adaptatif du mod√®le sur hardware',
                'advantage': 'Utilisation optimale des ressources',
                'challenge': 'Trouver mapping optimal est complexe',
                'example': 'R√©partir couches sur diff√©rents PEs (Processing Elements)'
            },
            'heterogeneous_computation': {
                'description': 'Utilisation de diff√©rents types de compute units',
                'advantage': 'Exploite avantages de chaque type',
                'challenge': 'Scheduling et synchronisation complexes',
                'example': 'Conv sur DSP, activation sur LUT, pooling sur BRAM'
            },
            'memory_hierarchy_optimization': {
                'description': 'Optimisation de la hi√©rarchie m√©moire',
                'advantage': 'R√©duit latence et √©nergie m√©moire',
                'challenge': 'Trade-off complexe entre diff√©rentes m√©moires',
                'example': 'Weights en BRAM, activations en cache, buffers optimis√©s'
            }
        }
    
    def display_principles(self):
        """Affiche les principes"""
        print("\n" + "="*70)
        print("Principes du Co-design Mod√®le-Hardware")
        print("="*70)
        
        for principle, info in self.principles.items():
            print(f"\n{principle.replace('_', ' ').title()}:")
            print(f"  Description: {info['description']}")
            print(f"  Advantage: {info['advantage']}")
            print(f"  Challenge: {info['challenge']}")
            print(f"  Example: {info['example']}")

principles = CoDesignPrinciples()
principles.display_principles()
```

---

## Espace de Recherche Combin√©

### Mod√®le + Hardware

```python
import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Tuple, Optional

class CombinedSearchSpace:
    """
    Espace de recherche combin√©: architecture mod√®le + configuration hardware
    """
    
    def __init__(self, model_search_space, hardware_config_space):
        """
        Args:
            model_search_space: Espace de recherche d'architectures
            hardware_config_space: Espace de recherche de configs hardware
        """
        self.model_space = model_search_space
        self.hardware_space = hardware_config_space
        
        # Espace combin√©
        self.combined_space = {
            'model': self.model_space.base_space,
            'hardware': self.hardware_space.space
        }
    
    def sample_combined_config(self) -> Dict:
        """
        G√©n√®re une configuration combin√©e (mod√®le + hardware)
        """
        model_config = self.model_space.sample_random_config()
        hardware_config = self.hardware_space.sample_config()
        
        return {
            'model': model_config,
            'hardware': hardware_config
        }
    
    def create_model_from_combined_config(self, combined_config: Dict) -> nn.Module:
        """Cr√©e mod√®le depuis config combin√©e"""
        return self.model_space.create_model_from_config(combined_config['model'])
    
    def evaluate_combined_config(self, combined_config: Dict, 
                                input_shape: Tuple = (1, 784)) -> Dict:
        """
        √âvalue une configuration combin√©e
        
        Returns:
            M√©triques combin√©es (performance, latence, √©nergie, etc.)
        """
        # Cr√©er mod√®le
        model = self.create_model_from_combined_config(combined_config)
        
        # Simuler sur hardware configur√©
        simulator = HardwareSimulator(combined_config['hardware'])
        metrics = simulator.simulate(model, input_shape)
        
        # √âvaluer performance (proxy)
        performance = self._evaluate_performance_proxy(model, input_shape)
        
        return {
            'performance': performance,
            'latency_ns': metrics['latency_ns'],
            'energy_nj': metrics['energy_nj'],
            'hardware_utilization': metrics['utilization'],
            'combined_score': self._compute_combined_score(performance, metrics)
        }
    
    def _evaluate_performance_proxy(self, model: nn.Module, input_shape: Tuple) -> float:
        """Proxy de performance"""
        n_params = sum(p.numel() for p in model.parameters())
        return min(0.7 + (n_params / 1e6) * 0.2, 0.95)
    
    def _compute_combined_score(self, performance: float, metrics: Dict) -> float:
        """Score combin√©"""
        latency_penalty = (metrics['latency_ns'] / 1e6) * 0.3
        energy_penalty = (metrics['energy_nj'] / 1e3) * 0.2
        return performance - latency_penalty - energy_penalty


class HardwareConfigSpace:
    """
    Espace de recherche de configurations hardware
    """
    
    def __init__(self):
        self.space = {
            'parallelism': {
                'pe_count': [4, 8, 16, 32, 64],  # Processing Elements
                'dataflow': ['systolic', 'output_stationary', 'weight_stationary'],
                'tile_sizes': [(8, 8), (16, 16), (32, 32)]
            },
            'memory': {
                'buffer_size': [1024, 2048, 4096, 8192],  # bytes
                'memory_banks': [1, 2, 4, 8],
                'data_reuse': [True, False]
            },
            'precision': {
                'weight_bits': [8, 16],
                'activation_bits': [8, 16],
                'accumulator_bits': [16, 32]
            },
            'frequency': {
                'clock_mhz': [100, 150, 200, 250, 300]
            }
        }
    
    def sample_config(self) -> Dict:
        """G√©n√®re une configuration hardware"""
        config = {}
        
        # Parall√©lisme
        config['pe_count'] = np.random.choice(self.space['parallelism']['pe_count'])
        config['dataflow'] = np.random.choice(self.space['parallelism']['dataflow'])
        config['tile_size'] = np.random.choice(self.space['parallelism']['tile_sizes'])
        
        # M√©moire
        config['buffer_size'] = np.random.choice(self.space['memory']['buffer_size'])
        config['memory_banks'] = np.random.choice(self.space['memory']['memory_banks'])
        config['data_reuse'] = np.random.choice(self.space['memory']['data_reuse'])
        
        # Pr√©cision
        config['weight_bits'] = np.random.choice(self.space['precision']['weight_bits'])
        config['activation_bits'] = np.random.choice(self.space['precision']['activation_bits'])
        config['accumulator_bits'] = np.random.choice(self.space['precision']['accumulator_bits'])
        
        # Fr√©quence
        config['clock_mhz'] = np.random.choice(self.space['frequency']['clock_mhz'])
        
        return config


class HardwareSimulator:
    """
    Simulateur hardware pour √©valuer configurations
    """
    
    def __init__(self, hardware_config: Dict):
        self.config = hardware_config
        self.clock_period_ns = 1000.0 / hardware_config['clock_mhz']
    
    def simulate(self, model: nn.Module, input_shape: Tuple) -> Dict:
        """
        Simule l'ex√©cution du mod√®le sur hardware configur√©
        
        Returns:
            M√©triques hardware
        """
        # Estimer latence bas√©e sur parall√©lisme
        latency_ns = self._estimate_latency(model, input_shape)
        
        # Estimer √©nergie
        energy_nj = self._estimate_energy(model, input_shape)
        
        # Estimer utilisation ressources
        utilization = self._estimate_utilization(model, input_shape)
        
        return {
            'latency_ns': latency_ns,
            'latency_us': latency_ns / 1000.0,
            'energy_nj': energy_nj,
            'utilization': utilization
        }
    
    def _estimate_latency(self, model: nn.Module, input_shape: Tuple) -> float:
        """Estime latence avec parall√©lisme configur√©"""
        total_cycles = 0
        
        for module in model.modules():
            if isinstance(module, nn.Linear):
                # Op√©rations MAC
                n_mac = module.in_features * module.out_features
                
                # Cycles avec parall√©lisme
                cycles = np.ceil(n_mac / self.config['pe_count'])
                total_cycles += cycles
        
        latency_ns = total_cycles * self.clock_period_ns
        return latency_ns
    
    def _estimate_energy(self, model: nn.Module, input_shape: Tuple) -> float:
        """Estime √©nergie avec pr√©cision configur√©e"""
        # √ânergie par op√©ration d√©pend de pr√©cision
        energy_per_mult = {
            8: 4.6,   # pJ
            16: 18.0
        }
        
        energy_mult = energy_per_mult.get(self.config['weight_bits'], 4.6)
        
        total_energy_pj = 0
        for module in model.modules():
            if isinstance(module, nn.Linear):
                n_mac = module.in_features * module.out_features
                total_energy_pj += n_mac * energy_mult
        
        return total_energy_pj / 1e3  # nJ
    
    def _estimate_utilization(self, model: nn.Module, input_shape: Tuple) -> Dict:
        """Estime utilisation des ressources"""
        # Simplifi√©: utilisation bas√©e sur PE
        total_ops = sum(module.in_features * module.out_features 
                       for module in model.modules() 
                       if isinstance(module, nn.Linear))
        
        pe_utilization = min(1.0, total_ops / (self.config['pe_count'] * 1000))
        
        return {
            'pe_utilization': pe_utilization,
            'memory_utilization': 0.6,  # approximation
            'overall': pe_utilization * 0.7 + 0.6 * 0.3
        }
```

---

## Optimisation Jointe

### Algorithme de Co-optimisation

```python
class JointOptimizationNAS:
    """
    NAS avec optimisation jointe mod√®le-hardware
    """
    
    def __init__(self, combined_search_space, constraints: Dict):
        self.search_space = combined_search_space
        self.constraints = constraints
        
        self.evaluation_history = []
    
    def optimize(self, n_iterations: int = 200, 
                 input_shape: Tuple = (1, 784)) -> Dict:
        """
        Optimise simultan√©ment mod√®le et hardware
        
        Strat√©gie: recherche altern√©e avec coordination
        """
        # Initialisation
        best_combined = None
        best_score = -float('inf')
        
        # Phase 1: Exploration large
        print("Phase 1: Exploration large...")
        for i in range(n_iterations // 2):
            combined_config = self.search_space.sample_combined_config()
            
            # √âvaluer
            metrics = self.search_space.evaluate_combined_config(combined_config, input_shape)
            
            self.evaluation_history.append({
                'config': combined_config,
                'metrics': metrics
            })
            
            if metrics['combined_score'] > best_score:
                best_score = metrics['combined_score']
                best_combined = {
                    'config': combined_config,
                    'metrics': metrics
                }
            
            if (i + 1) % 20 == 0:
                print(f"  Iteration {i+1}/{n_iterations//2}: Best score = {best_score:.4f}")
        
        # Phase 2: Raffinement local
        print("\nPhase 2: Raffinement local...")
        current_config = best_combined['config']
        
        for i in range(n_iterations // 2):
            # Alterner entre optimisation mod√®le et hardware
            if i % 2 == 0:
                # Optimiser mod√®le autour de config hardware actuelle
                new_config = self._optimize_model_local(current_config)
            else:
                # Optimiser hardware autour de mod√®le actuel
                new_config = self._optimize_hardware_local(current_config)
            
            # √âvaluer
            metrics = self.search_space.evaluate_combined_config(new_config, input_shape)
            
            if metrics['combined_score'] > best_score:
                best_score = metrics['combined_score']
                best_combined = {
                    'config': new_config,
                    'metrics': metrics
                }
                current_config = new_config
            
            if (i + 1) % 20 == 0:
                print(f"  Iteration {i+1}/{n_iterations//2}: Best score = {best_score:.4f}")
        
        # Cr√©er mod√®le final
        final_model = self.search_space.create_model_from_combined_config(best_combined['config'])
        
        return {
            'model': final_model,
            'model_config': best_combined['config']['model'],
            'hardware_config': best_combined['config']['hardware'],
            'metrics': best_combined['metrics'],
            'score': best_score
        }
    
    def _optimize_model_local(self, current_config: Dict) -> Dict:
        """Mutation locale du mod√®le"""
        new_config = current_config.copy()
        
        # Mutation du mod√®le
        model_config = new_config['model'].copy()
        
        if 'num_layers' in model_config:
            current_layers = model_config['num_layers']
            model_config['num_layers'] = np.random.choice([
                max(3, current_layers - 1),
                current_layers,
                min(8, current_layers + 1)
            ])
        
        new_config['model'] = model_config
        return new_config
    
    def _optimize_hardware_local(self, current_config: Dict) -> Dict:
        """Mutation locale du hardware"""
        new_config = current_config.copy()
        
        # Mutation du hardware
        hw_config = new_config['hardware'].copy()
        
        if 'pe_count' in hw_config:
            current_pe = hw_config['pe_count']
            options = [max(4, current_pe - 8), current_pe, min(64, current_pe + 8)]
            hw_config['pe_count'] = np.random.choice(options)
        
        if 'clock_mhz' in hw_config:
            current_freq = hw_config['clock_mhz']
            options = [max(100, current_freq - 50), current_freq, min(300, current_freq + 50)]
            hw_config['clock_mhz'] = np.random.choice(options)
        
        new_config['hardware'] = hw_config
        return new_config
```

---

## Hardware-Aware Training

### Entra√Ænement avec Contraintes Hardware

```python
class HardwareAwareTraining:
    """
    Entra√Ænement de mod√®le avec r√©gularisation hardware
    """
    
    def __init__(self, model: nn.Module, hardware_simulator, 
                 hardware_weight: float = 0.1):
        """
        Args:
            model: Mod√®le √† entra√Æner
            hardware_simulator: HardwareSimulator
            hardware_weight: Poids de la r√©gularisation hardware
        """
        self.model = model
        self.hardware_sim = hardware_simulator
        self.hardware_weight = hardware_weight
    
    def train(self, train_loader, val_loader, epochs: int = 50,
              lr: float = 0.001, input_shape: Tuple = (1, 784)):
        """
        Entra√Æne avec r√©gularisation hardware
        """
        import torch.optim as optim
        import torch.nn.functional as F
        
        optimizer = optim.Adam(self.model.parameters(), lr=lr)
        criterion = nn.CrossEntropyLoss()
        
        for epoch in range(epochs):
            # Entra√Ænement
            self.model.train()
            train_loss = 0
            
            for batch_idx, (data, target) in enumerate(train_loader):
                optimizer.zero_grad()
                
                # Forward
                output = self.model(data)
                
                # Loss standard
                loss_standard = criterion(output, target)
                
                # P√©nalit√© hardware (tous les N batches pour efficacit√©)
                if batch_idx % 10 == 0:
                    hardware_metrics = self.hardware_sim.simulate(self.model, input_shape)
                    hardware_penalty = self._compute_hardware_penalty(hardware_metrics)
                else:
                    hardware_penalty = 0
                
                # Loss combin√©e
                loss = loss_standard + self.hardware_weight * hardware_penalty
                
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
            
            # Validation
            self.model.eval()
            val_loss = 0
            correct = 0
            total = 0
            
            with torch.no_grad():
                for data, target in val_loader:
                    output = self.model(data)
                    loss = criterion(output, target)
                    val_loss += loss.item()
                    
                    pred = output.argmax(dim=1)
                    correct += (pred == target).sum().item()
                    total += target.size(0)
            
            if (epoch + 1) % 10 == 0:
                acc = 100 * correct / total
                print(f"Epoch {epoch+1}/{epochs}: "
                      f"Train Loss = {train_loss/len(train_loader):.4f}, "
                      f"Val Loss = {val_loss/len(val_loader):.4f}, "
                      f"Val Acc = {acc:.2f}%")
    
    def _compute_hardware_penalty(self, metrics: Dict) -> torch.Tensor:
        """
        Calcule p√©nalit√© hardware (plus bas = mieux)
        """
        latency_penalty = metrics['latency_us'] / 100.0  # normalis√©
        energy_penalty = metrics['energy_nj'] / 1000.0  # normalis√©
        
        return torch.tensor(latency_penalty + energy_penalty)
```

---

## Mapping Adaptatif sur FPGA

### Optimisation du Mapping

```python
class AdaptiveFPGAMapping:
    """
    Mapping adaptatif de mod√®le sur FPGA avec optimisation
    """
    
    def __init__(self, model: nn.Module, fpga_constraints: Dict):
        """
        Args:
            model: Mod√®le √† mapper
            fpga_constraints: Contraintes FPGA (LUT, DSP, BRAM)
        """
        self.model = model
        self.fpga_constraints = fpga_constraints
        
        # Strat√©gies de mapping
        self.mapping_strategies = {
            'layer_wise': self._map_layer_wise,
            'tensor_slicing': self._map_tensor_slicing,
            'pipelined': self._map_pipelined
        }
    
    def find_optimal_mapping(self, input_shape: Tuple = (1, 784)) -> Dict:
        """
        Trouve le mapping optimal
        
        Returns:
            Configuration de mapping optimale
        """
        best_mapping = None
        best_latency = float('inf')
        
        # Tester diff√©rentes strat√©gies
        for strategy_name, strategy_fn in self.mapping_strategies.items():
            mapping = strategy_fn(input_shape)
            
            # √âvaluer mapping
            latency = self._evaluate_mapping(mapping, input_shape)
            
            if latency < best_latency:
                best_latency = latency
                best_mapping = {
                    'strategy': strategy_name,
                    'mapping': mapping,
                    'latency_ns': latency
                }
        
        return best_mapping
    
    def _map_layer_wise(self, input_shape: Tuple) -> Dict:
        """
        Mapping couche par couche (s√©quentiel)
        """
        mapping = {
            'type': 'layer_wise',
            'layers': []
        }
        
        current_shape = input_shape
        for name, module in self.model.named_modules():
            if isinstance(module, nn.Linear):
                layer_mapping = {
                    'name': name,
                    'type': 'linear',
                    'pe_count': min(64, module.out_features),  # PEs pour cette couche
                    'tile_size': (8, 8),
                    'input_shape': current_shape,
                    'output_shape': (current_shape[0], module.out_features)
                }
                mapping['layers'].append(layer_mapping)
                current_shape = layer_mapping['output_shape']
        
        return mapping
    
    def _map_tensor_slicing(self, input_shape: Tuple) -> Dict:
        """
        Mapping avec d√©coupage de tenseurs (parall√©lisme spatial)
        """
        mapping = {
            'type': 'tensor_slicing',
            'slices': 4,  # Nombre de slices
            'layers': []
        }
        
        # Similar √† layer_wise mais avec slicing
        current_shape = input_shape
        for name, module in self.model.named_modules():
            if isinstance(module, nn.Linear):
                layer_mapping = {
                    'name': name,
                    'type': 'linear_sliced',
                    'slices': mapping['slices'],
                    'pe_per_slice': 16,
                    'input_shape': current_shape,
                    'output_shape': (current_shape[0], module.out_features)
                }
                mapping['layers'].append(layer_mapping)
                current_shape = layer_mapping['output_shape']
        
        return mapping
    
    def _map_pipelined(self, input_shape: Tuple) -> Dict:
        """
        Mapping pipelin√© (overlapping computation)
        """
        mapping = {
            'type': 'pipelined',
            'pipeline_stages': 3,
            'layers': []
        }
        
        # Mapping avec pipeline stages
        current_shape = input_shape
        for name, module in self.model.named_modules():
            if isinstance(module, nn.Linear):
                layer_mapping = {
                    'name': name,
                    'type': 'linear_pipelined',
                    'pipeline_stage': len(mapping['layers']) % mapping['pipeline_stages'],
                    'pe_count': 32,
                    'input_shape': current_shape,
                    'output_shape': (current_shape[0], module.out_features)
                }
                mapping['layers'].append(layer_mapping)
                current_shape = layer_mapping['output_shape']
        
        return mapping
    
    def _evaluate_mapping(self, mapping: Dict, input_shape: Tuple) -> float:
        """
        √âvalue la latence d'un mapping
        
        Returns:
            Latence en nanosecondes
        """
        # Simplifi√©: estimation bas√©e sur type de mapping
        total_latency = 0
        
        if mapping['type'] == 'layer_wise':
            # Latence s√©quentielle
            for layer in mapping['layers']:
                if layer['type'] == 'linear':
                    n_ops = layer['input_shape'][1] * layer['output_shape'][1]
                    cycles = np.ceil(n_ops / layer['pe_count'])
                    total_latency += cycles * 5  # 5 ns par cycle (200 MHz)
        
        elif mapping['type'] == 'tensor_slicing':
            # Latence avec parall√©lisme
            for layer in mapping['layers']:
                n_ops = layer['input_shape'][1] * layer['output_shape'][1]
                cycles = np.ceil(n_ops / (layer['pe_per_slice'] * layer['slices']))
                total_latency += cycles * 5
        
        elif mapping['type'] == 'pipelined':
            # Latence pipelin√©e (overlap)
            max_stage_latency = 0
            for layer in mapping['layers']:
                n_ops = layer['input_shape'][1] * layer['output_shape'][1]
                cycles = np.ceil(n_ops / layer['pe_count'])
                stage_latency = cycles * 5
                max_stage_latency = max(max_stage_latency, stage_latency)
            total_latency = max_stage_latency * mapping['pipeline_stages']
        
        return total_latency
```

---

## Applications Pratiques

### Cas d'Usage: Trigger HEP

```python
class HEPTriggerCoDesign:
    """
    Co-design pour syst√®me de trigger HEP
    """
    
    def __init__(self):
        self.requirements = {
            'max_latency_ns': 100000,  # 100 Œºs
            'max_energy_nj': 500,      # 500 nJ
            'target_accuracy': 0.95,    # 95%
            'fpga_family': 'Xilinx Zynq UltraScale+'
        }
    
    def design_trigger_system(self):
        """
        Con√ßoit un syst√®me de trigger optimis√©
        """
        print("\n" + "="*70)
        print("Co-design Syst√®me de Trigger HEP")
        print("="*70)
        
        # 1. D√©finir espace de recherche
        print("\n1. D√©finition de l'espace de recherche...")
        model_space = ConstrainedSearchSpace(self.requirements, None)  # simplifi√©
        hw_space = HardwareConfigSpace()
        combined_space = CombinedSearchSpace(model_space, hw_space)
        
        # 2. Optimisation jointe
        print("\n2. Optimisation jointe mod√®le-hardware...")
        optimizer = JointOptimizationNAS(combined_space, self.requirements)
        result = optimizer.optimize(n_iterations=100)
        
        # 3. R√©sultats
        print("\n3. R√©sultats du co-design:")
        print(f"  Architecture mod√®le: {result['model_config']}")
        print(f"  Configuration hardware:")
        print(f"    - PE count: {result['hardware_config']['pe_count']}")
        print(f"    - Clock: {result['hardware_config']['clock_mhz']} MHz")
        print(f"    - Dataflow: {result['hardware_config']['dataflow']}")
        print(f"  M√©triques:")
        print(f"    - Latence: {result['metrics']['latency_us']:.2f} Œºs")
        print(f"    - √ânergie: {result['metrics']['energy_nj']:.2f} nJ")
        print(f"    - Performance: {result['metrics']['performance']:.4f}")
        print(f"    - Score combin√©: {result['score']:.4f}")
        
        # 4. V√©rification contraintes
        print("\n4. V√©rification des contraintes:")
        constraints_met = (
            result['metrics']['latency_ns'] <= self.requirements['max_latency_ns'] and
            result['metrics']['energy_nj'] <= self.requirements['max_energy_nj']
        )
        print(f"  Contraintes respect√©es: {constraints_met}")
        
        return result
```

---

## Exercices

### Exercice 16.5.1
Impl√©mentez un syst√®me de co-design pour un mod√®le de classification d'images avec contraintes FPGA. Comparez avec optimisation s√©par√©e mod√®le/hardware.

### Exercice 16.5.2
Cr√©ez un entra√Ænement hardware-aware qui int√®gre la latence FPGA comme r√©gularisation et comparez avec entra√Ænement standard.

### Exercice 16.5.3
D√©veloppez un algorithme de mapping adaptatif qui trouve automatiquement la meilleure r√©partition des couches sur les ressources FPGA.

### Exercice 16.5.4
Analysez le trade-off entre diff√©rentes strat√©gies de mapping (layer-wise, tensor slicing, pipelined) pour un mod√®le donn√©.

---

## Points Cl√©s √† Retenir

> üìå **Le co-design optimise simultan√©ment mod√®le et hardware pour meilleure solution globale**

> üìå **L'entra√Ænement hardware-aware int√®gre contraintes hardware pendant l'apprentissage**

> üìå **Le mapping adaptatif optimise l'utilisation des ressources hardware**

> üìå **Les strat√©gies de mapping (layer-wise, slicing, pipelined) offrent diff√©rents trade-offs**

> üìå **Le co-design est particuli√®rement important pour applications avec contraintes strictes (trigger HEP)**

> üìå **L'espace de recherche combin√© est tr√®s large mais peut √™tre explor√© efficacement avec bonnes strat√©gies**

---

*Section pr√©c√©dente : [16.4 Algorithmes de Recherche Efficaces](./16_04_Algorithmes.md)*

