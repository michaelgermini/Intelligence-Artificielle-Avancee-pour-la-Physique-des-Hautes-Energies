# 2.2 DÃ©composition en Valeurs SinguliÃ¨res (SVD)

---

## Introduction

La **DÃ©composition en Valeurs SinguliÃ¨res** (SVD - Singular Value Decomposition) est l'un des outils les plus puissants de l'algÃ¨bre linÃ©aire. Elle gÃ©nÃ©ralise la diagonalisation aux matrices rectangulaires et constitue la base de nombreuses techniques de compression.

---

## DÃ©finition et ThÃ©orÃ¨me

### Ã‰noncÃ© du ThÃ©orÃ¨me SVD

Pour toute matrice $\mathbf{A} \in \mathbb{R}^{m \times n}$, il existe une factorisation :

$$\mathbf{A} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^T$$

oÃ¹ :
- $\mathbf{U} \in \mathbb{R}^{m \times m}$ : matrice orthogonale (vecteurs singuliers gauches)
- $\mathbf{\Sigma} \in \mathbb{R}^{m \times n}$ : matrice diagonale (valeurs singuliÃ¨res)
- $\mathbf{V} \in \mathbb{R}^{n \times n}$ : matrice orthogonale (vecteurs singuliers droits)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Structure de la SVD                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚    A          =      U          Î£           V^T                â”‚
â”‚  (m Ã— n)          (m Ã— m)    (m Ã— n)      (n Ã— n)              â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚     â”‚        â”‚     â”‚    â”‚Ïƒâ‚   â”‚      â”‚     â”‚               â”‚
â”‚  â”‚     â”‚   =    â”‚     â”‚    â”‚  Ïƒâ‚‚ â”‚      â”‚     â”‚               â”‚
â”‚  â”‚     â”‚        â”‚     â”‚    â”‚   â‹± â”‚      â”‚     â”‚               â”‚
â”‚  â”‚     â”‚        â”‚     â”‚    â”‚    Ïƒáµ£â”‚      â”‚     â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                                 â”‚
â”‚  Ïƒâ‚ â‰¥ Ïƒâ‚‚ â‰¥ ... â‰¥ Ïƒáµ£ > 0 (valeurs singuliÃ¨res non nulles)      â”‚
â”‚  r = rang(A)                                                    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ImplÃ©mentation et Calcul

```python
import numpy as np
import matplotlib.pyplot as plt

def full_svd(A):
    """
    Calcul de la SVD complÃ¨te avec analyse
    """
    m, n = A.shape
    
    # SVD via NumPy
    U, S, Vt = np.linalg.svd(A, full_matrices=True)
    
    # Reconstruction de Sigma
    Sigma = np.zeros((m, n))
    np.fill_diagonal(Sigma, S)
    
    # VÃ©rification
    A_reconstructed = U @ Sigma @ Vt
    reconstruction_error = np.linalg.norm(A - A_reconstructed)
    
    return {
        'U': U,
        'S': S,
        'Vt': Vt,
        'Sigma': Sigma,
        'rank': np.sum(S > 1e-10),
        'reconstruction_error': reconstruction_error
    }

def compact_svd(A):
    """
    SVD compacte (Ã©conomique) - ne garde que les composantes non nulles
    """
    U, S, Vt = np.linalg.svd(A, full_matrices=False)
    
    # Filtre les valeurs singuliÃ¨res nÃ©gligeables
    tol = 1e-10
    rank = np.sum(S > tol)
    
    return U[:, :rank], S[:rank], Vt[:rank, :]

# Exemple
A = np.array([
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 9],
    [10, 11, 12]
])

result = full_svd(A)
print(f"Dimensions de A: {A.shape}")
print(f"Rang de A: {result['rank']}")
print(f"Valeurs singuliÃ¨res: {result['S']}")
print(f"Erreur de reconstruction: {result['reconstruction_error']:.2e}")
```

---

## InterprÃ©tation GÃ©omÃ©trique

### Transformation en Trois Ã‰tapes

La SVD dÃ©compose toute transformation linÃ©aire en :

1. **Rotation/RÃ©flexion** ($\mathbf{V}^T$) : Aligne les donnÃ©es avec les axes principaux
2. **Mise Ã  l'Ã©chelle** ($\mathbf{\Sigma}$) : Ã‰tire/compresse le long de chaque axe
3. **Rotation/RÃ©flexion** ($\mathbf{U}$) : Rotation finale dans l'espace d'arrivÃ©e

```python
def visualize_svd_2d(A):
    """
    Visualise la transformation SVD en 2D
    """
    U, S, Vt = np.linalg.svd(A)
    
    # Cercle unitÃ©
    theta = np.linspace(0, 2*np.pi, 100)
    circle = np.array([np.cos(theta), np.sin(theta)])
    
    # Ã‰tapes de transformation
    step1 = Vt @ circle           # Rotation V^T
    step2 = np.diag(S) @ step1    # Mise Ã  l'Ã©chelle
    step3 = U @ step2             # Rotation U
    
    # Visualisation
    fig, axes = plt.subplots(1, 4, figsize=(16, 4))
    
    titles = ['Cercle original', 'AprÃ¨s V^T', 'AprÃ¨s Î£', 'AprÃ¨s U (= Ax)']
    data = [circle, step1, step2, step3]
    
    for ax, title, d in zip(axes, titles, data):
        ax.plot(d[0], d[1], 'b-', linewidth=2)
        ax.set_xlim(-3, 3)
        ax.set_ylim(-3, 3)
        ax.set_aspect('equal')
        ax.grid(True)
        ax.set_title(title)
        ax.axhline(y=0, color='k', linewidth=0.5)
        ax.axvline(x=0, color='k', linewidth=0.5)
    
    plt.tight_layout()
    return fig

# Exemple avec une matrice 2Ã—2
A_2d = np.array([[2, 1], [1, 2]])
# visualize_svd_2d(A_2d)
```

### Vecteurs Singuliers comme Directions Principales

```python
def principal_directions(A):
    """
    Analyse des directions principales via SVD
    """
    U, S, Vt = np.linalg.svd(A)
    
    print("Directions principales dans l'espace d'entrÃ©e (colonnes de V):")
    V = Vt.T
    for i, (s, v) in enumerate(zip(S, V.T)):
        print(f"  Direction {i+1}: {v}, Ïƒ = {s:.4f}")
    
    print("\nDirections principales dans l'espace de sortie (colonnes de U):")
    for i, (s, u) in enumerate(zip(S, U.T)):
        print(f"  Direction {i+1}: {u}, Ïƒ = {s:.4f}")
    
    return U, S, V

# Analyse
U, S, V = principal_directions(A_2d)
```

---

## PropriÃ©tÃ©s Fondamentales

### Relations avec d'Autres Concepts

```python
def svd_properties(A):
    """
    DÃ©montre les propriÃ©tÃ©s de la SVD
    """
    U, S, Vt = np.linalg.svd(A)
    V = Vt.T
    
    properties = {}
    
    # 1. Rang = nombre de valeurs singuliÃ¨res non nulles
    properties['rank'] = np.sum(S > 1e-10)
    
    # 2. Normes
    properties['frobenius_norm'] = np.sqrt(np.sum(S**2))
    properties['spectral_norm'] = S[0]  # Plus grande valeur singuliÃ¨re
    properties['nuclear_norm'] = np.sum(S)  # Somme des valeurs singuliÃ¨res
    
    # 3. Condition number
    if S[-1] > 1e-10:
        properties['condition_number'] = S[0] / S[-1]
    else:
        properties['condition_number'] = np.inf
    
    # 4. Valeurs propres de A^T A et A A^T
    AtA = A.T @ A
    AAt = A @ A.T
    
    eig_AtA = np.sqrt(np.abs(np.linalg.eigvalsh(AtA)))
    eig_AAt = np.sqrt(np.abs(np.linalg.eigvalsh(AAt)))
    
    properties['eigenvalues_AtA'] = np.sort(eig_AtA)[::-1]
    properties['eigenvalues_AAt'] = np.sort(eig_AAt)[::-1]
    properties['singular_values'] = S
    
    # VÃ©rification : Ïƒáµ¢Â² = Î»áµ¢(A^T A) = Î»áµ¢(A A^T)
    
    # 5. Image et noyau
    properties['image_basis'] = U[:, :properties['rank']]
    properties['kernel_basis'] = V[:, properties['rank']:]
    
    return properties

props = svd_properties(A)
print("PropriÃ©tÃ©s de la SVD:")
for key, value in props.items():
    if isinstance(value, np.ndarray) and value.size > 10:
        print(f"  {key}: array of shape {value.shape}")
    else:
        print(f"  {key}: {value}")
```

### ThÃ©orÃ¨me d'Eckart-Young-Mirsky

La **meilleure approximation de rang k** (au sens des normes de Frobenius et spectrale) est donnÃ©e par la SVD tronquÃ©e :

$$\mathbf{A}_k = \sum_{i=1}^{k} \sigma_i \mathbf{u}_i \mathbf{v}_i^T$$

```python
def truncated_svd(A, k):
    """
    SVD tronquÃ©e au rang k
    
    ThÃ©orÃ¨me d'Eckart-Young:
    A_k = argmin_{rank(B)=k} ||A - B||_F
    """
    U, S, Vt = np.linalg.svd(A, full_matrices=False)
    
    # Troncature
    U_k = U[:, :k]
    S_k = S[:k]
    Vt_k = Vt[:k, :]
    
    # Reconstruction
    A_k = U_k @ np.diag(S_k) @ Vt_k
    
    # Erreur d'approximation
    # ||A - A_k||_F = sqrt(sum_{i>k} Ïƒáµ¢Â²)
    error_frobenius = np.sqrt(np.sum(S[k:]**2))
    error_spectral = S[k] if k < len(S) else 0
    
    return {
        'A_k': A_k,
        'U_k': U_k,
        'S_k': S_k,
        'Vt_k': Vt_k,
        'error_frobenius': error_frobenius,
        'error_spectral': error_spectral,
        'relative_error': error_frobenius / np.linalg.norm(A, 'fro')
    }

# DÃ©monstration sur une matrice plus grande
np.random.seed(42)
A_large = np.random.randn(100, 50)

print("Erreur d'approximation en fonction du rang:")
print("k  | Erreur relative | Ã‰nergie capturÃ©e")
print("-" * 45)

for k in [1, 5, 10, 20, 30, 40, 50]:
    result = truncated_svd(A_large, k)
    
    # Ã‰nergie capturÃ©e = sum(Ïƒáµ¢Â²) / sum(Ïƒâ±¼Â²)
    U, S, Vt = np.linalg.svd(A_large, full_matrices=False)
    energy = np.sum(S[:k]**2) / np.sum(S**2)
    
    print(f"{k:2} | {result['relative_error']:.6f}      | {energy:.4f}")
```

---

## Applications Ã  la Compression

### Compression d'Images

```python
def compress_image_svd(image, k):
    """
    Compresse une image en niveaux de gris par SVD
    """
    U, S, Vt = np.linalg.svd(image, full_matrices=False)
    
    # Reconstruction avec k composantes
    compressed = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]
    
    # Calcul du taux de compression
    m, n = image.shape
    original_size = m * n
    compressed_size = k * (m + n + 1)
    compression_ratio = original_size / compressed_size
    
    # Erreur
    psnr = 10 * np.log10(255**2 / np.mean((image - compressed)**2))
    
    return {
        'compressed': compressed,
        'compression_ratio': compression_ratio,
        'psnr': psnr,
        'k': k
    }

# CrÃ©ation d'une image test (gradient + bruit)
def create_test_image(size=256):
    x = np.linspace(0, 1, size)
    y = np.linspace(0, 1, size)
    X, Y = np.meshgrid(x, y)
    
    # Image avec structure de rang faible + bruit
    image = 128 + 50 * np.sin(2*np.pi*X) * np.cos(2*np.pi*Y)
    image += 10 * np.random.randn(size, size)
    
    return np.clip(image, 0, 255)

image = create_test_image()

print("Compression d'image par SVD:")
print("k   | Compression | PSNR (dB)")
print("-" * 35)

for k in [5, 10, 20, 50, 100]:
    result = compress_image_svd(image, k)
    print(f"{k:3} | {result['compression_ratio']:6.2f}x    | {result['psnr']:.1f}")
```

### Compression de Matrices de Poids

```python
class SVDCompressedLinear:
    """
    Couche linÃ©aire compressÃ©e par SVD
    
    Au lieu de stocker W (mÃ—n), on stocke:
    - U_k (mÃ—k)
    - S_k (k)
    - V_k (nÃ—k)
    
    RÃ©duction: mÃ—n â†’ kÃ—(m+n+1)
    """
    
    def __init__(self, W, rank):
        self.original_shape = W.shape
        self.rank = rank
        
        # DÃ©composition
        U, S, Vt = np.linalg.svd(W, full_matrices=False)
        
        self.U = U[:, :rank]
        self.S = S[:rank]
        self.V = Vt[:rank, :].T  # Stocke V, pas V^T
        
    def forward(self, x):
        """
        Calcul efficace: y = (U @ diag(S) @ V^T) @ x
                           = U @ (S * (V^T @ x))
        """
        # Ã‰tape 1: V^T @ x
        temp = self.V.T @ x
        
        # Ã‰tape 2: Mise Ã  l'Ã©chelle par S
        temp = self.S[:, np.newaxis] * temp
        
        # Ã‰tape 3: U @ temp
        return self.U @ temp
    
    def reconstruct_weight(self):
        """Reconstruit la matrice de poids"""
        return self.U @ np.diag(self.S) @ self.V.T
    
    def compression_stats(self):
        m, n = self.original_shape
        k = self.rank
        
        original_params = m * n
        compressed_params = k * (m + n + 1)
        
        return {
            'original_params': original_params,
            'compressed_params': compressed_params,
            'compression_ratio': original_params / compressed_params,
            'rank': k
        }

# DÃ©monstration
W = np.random.randn(1024, 512)
compressed_layer = SVDCompressedLinear(W, rank=64)

stats = compressed_layer.compression_stats()
print("Statistiques de compression:")
for key, value in stats.items():
    print(f"  {key}: {value}")

# VÃ©rification de la reconstruction
W_reconstructed = compressed_layer.reconstruct_weight()
relative_error = np.linalg.norm(W - W_reconstructed) / np.linalg.norm(W)
print(f"\nErreur relative de reconstruction: {relative_error:.4f}")

# Test du forward pass
x = np.random.randn(512, 32)  # Batch de 32
y_original = W @ x
y_compressed = compressed_layer.forward(x)
forward_error = np.linalg.norm(y_original - y_compressed) / np.linalg.norm(y_original)
print(f"Erreur relative du forward: {forward_error:.4f}")
```

---

## Algorithmes de Calcul

### SVD par ItÃ©ration de Puissance

```python
def power_iteration_svd(A, k, n_iter=100, tol=1e-10):
    """
    Calcul approchÃ© de la SVD par itÃ©ration de puissance
    
    Utile pour les trÃ¨s grandes matrices oÃ¹ seules
    les premiÃ¨res composantes sont nÃ©cessaires
    """
    m, n = A.shape
    
    # Initialisation alÃ©atoire
    V = np.random.randn(n, k)
    V, _ = np.linalg.qr(V)
    
    for iteration in range(n_iter):
        # Ã‰tape de puissance
        U = A @ V
        U, _ = np.linalg.qr(U)
        
        V_new = A.T @ U
        V_new, R = np.linalg.qr(V_new)
        
        # VÃ©rification de convergence
        if np.linalg.norm(V - V_new) < tol:
            print(f"Convergence aprÃ¨s {iteration+1} itÃ©rations")
            break
        
        V = V_new
    
    # Calcul des valeurs singuliÃ¨res
    S = np.diag(R)[:k]
    
    return U[:, :k], np.abs(S), V[:, :k]

# Comparaison avec SVD exacte
A_test = np.random.randn(500, 300)
k = 20

# SVD exacte
U_exact, S_exact, Vt_exact = np.linalg.svd(A_test, full_matrices=False)

# SVD approchÃ©e
U_approx, S_approx, V_approx = power_iteration_svd(A_test, k)

print(f"\nComparaison des {k} premiÃ¨res valeurs singuliÃ¨res:")
print("Exacte    | ApprochÃ©e | Erreur relative")
print("-" * 45)
for i in range(min(5, k)):
    error = abs(S_exact[i] - S_approx[i]) / S_exact[i]
    print(f"{S_exact[i]:.4f}   | {S_approx[i]:.4f}    | {error:.2e}")
```

### Randomized SVD

```python
def randomized_svd(A, k, n_oversamples=10, n_iter=2):
    """
    SVD randomisÃ©e pour grandes matrices
    
    ComplexitÃ©: O(mnÂ·k) au lieu de O(mnÂ·min(m,n))
    """
    m, n = A.shape
    
    # Ã‰tape 1: Projection alÃ©atoire
    l = k + n_oversamples
    Omega = np.random.randn(n, l)
    
    # Ã‰tape 2: ItÃ©rations de puissance pour amÃ©liorer la prÃ©cision
    Y = A @ Omega
    for _ in range(n_iter):
        Y = A @ (A.T @ Y)
    
    # Ã‰tape 3: Orthonormalisation
    Q, _ = np.linalg.qr(Y)
    
    # Ã‰tape 4: SVD du problÃ¨me rÃ©duit
    B = Q.T @ A
    U_tilde, S, Vt = np.linalg.svd(B, full_matrices=False)
    
    # Ã‰tape 5: Reconstruction de U
    U = Q @ U_tilde
    
    return U[:, :k], S[:k], Vt[:k, :]

# Test sur une grande matrice
A_large = np.random.randn(2000, 1000)

import time

# SVD exacte
start = time.time()
U_exact, S_exact, Vt_exact = np.linalg.svd(A_large, full_matrices=False)
time_exact = time.time() - start

# SVD randomisÃ©e
start = time.time()
U_rand, S_rand, Vt_rand = randomized_svd(A_large, k=50)
time_rand = time.time() - start

print(f"Temps SVD exacte: {time_exact:.3f}s")
print(f"Temps SVD randomisÃ©e: {time_rand:.3f}s")
print(f"AccÃ©lÃ©ration: {time_exact/time_rand:.1f}x")

# Erreur
error = np.linalg.norm(S_exact[:50] - S_rand) / np.linalg.norm(S_exact[:50])
print(f"Erreur relative sur les 50 premiÃ¨res valeurs singuliÃ¨res: {error:.2e}")
```

---

## Exercices

### Exercice 2.2.1
Calculez la SVD de la matrice :
$$\mathbf{A} = \begin{pmatrix} 3 & 2 & 2 \\ 2 & 3 & -2 \end{pmatrix}$$

VÃ©rifiez que $\mathbf{U}^T\mathbf{U} = \mathbf{I}$ et $\mathbf{V}^T\mathbf{V} = \mathbf{I}$.

### Exercice 2.2.2
Une matrice de poids $\mathbf{W} \in \mathbb{R}^{1000 \times 500}$ a des valeurs singuliÃ¨res qui dÃ©croissent comme $\sigma_i = 100/i$. Quel rang $k$ faut-il pour capturer 99% de l'Ã©nergie (norme de Frobenius au carrÃ©) ?

### Exercice 2.2.3
ImplÃ©mentez une fonction qui trouve automatiquement le rang optimal pour une compression SVD, Ã©tant donnÃ© une erreur relative maximale tolÃ©rÃ©e.

---

## Points ClÃ©s Ã  Retenir

> ðŸ“Œ **La SVD existe pour toute matrice et fournit la meilleure approximation de rang k**

> ðŸ“Œ **Les valeurs singuliÃ¨res mesurent l'importance de chaque composante**

> ðŸ“Œ **La SVD randomisÃ©e permet de traiter des matrices trÃ¨s grandes efficacement**

> ðŸ“Œ **La compression par SVD est optimale au sens de la norme de Frobenius**

---

## RÃ©fÃ©rences

1. Golub, G., Van Loan, C. "Matrix Computations." 4th Edition, 2013
2. Halko, N. et al. "Finding Structure with Randomness." SIAM Review, 2011
3. Trefethen, L., Bau, D. "Numerical Linear Algebra." SIAM, 1997

---

*Section suivante : [2.3 Approximations de Rang Faible](./02_03_Low_Rank.md)*

