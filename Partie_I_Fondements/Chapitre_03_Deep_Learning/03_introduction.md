# Chapitre 3 : Deep Learning - Architectures et Principes

---

## Introduction

Le **Deep Learning** a r√©volutionn√© l'intelligence artificielle et trouve des applications cruciales en physique des hautes √©nergies. Ce chapitre pr√©sente les architectures fondamentales des r√©seaux de neurones, les principes d'optimisation, et les techniques de r√©gularisation essentielles pour la compression de mod√®les.

---

## Objectifs d'Apprentissage

√Ä la fin de ce chapitre, vous serez capable de :

- Comprendre l'architecture des r√©seaux de neurones modernes
- Impl√©menter et entra√Æner des CNN, RNN et Transformers
- Ma√Ætriser les techniques d'optimisation et de r√©gularisation
- Identifier les opportunit√©s de compression dans chaque architecture

---

## Plan du Chapitre

1. [R√©seaux de Neurones Feedforward](./03_01_Feedforward.md)
2. [R√©seaux Convolutionnels (CNN)](./03_02_CNN.md)
3. [R√©seaux R√©currents et Transformers](./03_03_RNN_Transformers.md)
4. [Fonctions de Perte et Optimisation](./03_04_Optimisation.md)
5. [R√©gularisation et G√©n√©ralisation](./03_05_Regularisation.md)

---

## Contexte Historique

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              √âvolution du Deep Learning                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  1943  ‚îÇ McCulloch & Pitts : Neurone artificiel                ‚îÇ
‚îÇ  1958  ‚îÇ Rosenblatt : Perceptron                               ‚îÇ
‚îÇ  1986  ‚îÇ Rumelhart et al. : Backpropagation                    ‚îÇ
‚îÇ  1989  ‚îÇ LeCun : CNN pour reconnaissance de chiffres           ‚îÇ
‚îÇ  1997  ‚îÇ Hochreiter & Schmidhuber : LSTM                       ‚îÇ
‚îÇ  2006  ‚îÇ Hinton : Deep Belief Networks                         ‚îÇ
‚îÇ  2012  ‚îÇ Krizhevsky : AlexNet (r√©volution ImageNet)            ‚îÇ
‚îÇ  2014  ‚îÇ Goodfellow : GANs                                      ‚îÇ
‚îÇ  2015  ‚îÇ He : ResNet (r√©seaux tr√®s profonds)                   ‚îÇ
‚îÇ  2017  ‚îÇ Vaswani : Transformer (Attention is All You Need)     ‚îÇ
‚îÇ  2018  ‚îÇ BERT, GPT : Mod√®les de langage pr√©-entra√Æn√©s          ‚îÇ
‚îÇ  2020+ ‚îÇ Scaling laws, Foundation models                       ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Le Neurone Artificiel

### Mod√®le Math√©matique

Un neurone artificiel calcule :

$$y = \sigma\left(\sum_{i=1}^{n} w_i x_i + b\right) = \sigma(\mathbf{w}^T \mathbf{x} + b)$$

o√π $\sigma$ est une fonction d'activation non-lin√©aire.

```python
import numpy as np
import torch
import torch.nn as nn

class Neuron:
    """
    Impl√©mentation d'un neurone artificiel
    """
    
    def __init__(self, n_inputs, activation='relu'):
        # Initialisation Xavier/Glorot
        self.weights = np.random.randn(n_inputs) / np.sqrt(n_inputs)
        self.bias = 0.0
        self.activation = activation
        
    def forward(self, x):
        """Propagation avant"""
        z = np.dot(self.weights, x) + self.bias
        return self._activate(z)
    
    def _activate(self, z):
        """Applique la fonction d'activation"""
        if self.activation == 'relu':
            return np.maximum(0, z)
        elif self.activation == 'sigmoid':
            return 1 / (1 + np.exp(-np.clip(z, -500, 500)))
        elif self.activation == 'tanh':
            return np.tanh(z)
        elif self.activation == 'linear':
            return z
        else:
            raise ValueError(f"Activation inconnue: {self.activation}")

# D√©monstration
neuron = Neuron(5, activation='relu')
x = np.array([1.0, -0.5, 0.3, 0.8, -0.2])
output = neuron.forward(x)
print(f"Entr√©e: {x}")
print(f"Sortie: {output:.4f}")
```

### Fonctions d'Activation

```python
import matplotlib.pyplot as plt

def plot_activations():
    """Visualise les fonctions d'activation courantes"""
    x = np.linspace(-5, 5, 1000)
    
    activations = {
        'Sigmoid': (1 / (1 + np.exp(-x)), 'Sortie ‚àà (0, 1)'),
        'Tanh': (np.tanh(x), 'Sortie ‚àà (-1, 1)'),
        'ReLU': (np.maximum(0, x), 'Sortie ‚àà [0, ‚àû)'),
        'Leaky ReLU': (np.where(x > 0, x, 0.01 * x), '√âvite les neurones morts'),
        'GELU': (x * 0.5 * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3))), 
                 'Utilis√© dans Transformers'),
        'Swish': (x / (1 + np.exp(-x)), 'Auto-gated')
    }
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 8))
    axes = axes.flatten()
    
    for ax, (name, (y, desc)) in zip(axes, activations.items()):
        ax.plot(x, y, 'b-', linewidth=2)
        ax.axhline(y=0, color='k', linewidth=0.5)
        ax.axvline(x=0, color='k', linewidth=0.5)
        ax.set_xlim(-5, 5)
        ax.set_ylim(-2, 5)
        ax.set_title(f'{name}\n{desc}', fontsize=10)
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig

# Propri√©t√©s des activations pour la compression
activation_properties = {
    'ReLU': {
        'sparsity': 'Haute (50% des neurones inactifs en moyenne)',
        'quantization': 'Excellente (sortie non-n√©gative)',
        'pruning': 'Facile (neurones morts identifiables)'
    },
    'Sigmoid': {
        'sparsity': 'Faible',
        'quantization': 'Bonne (sortie born√©e)',
        'pruning': 'Mod√©r√©e'
    },
    'GELU': {
        'sparsity': 'Mod√©r√©e',
        'quantization': 'Plus difficile (lisse)',
        'pruning': 'Mod√©r√©e'
    }
}
```

---

## Architectures de Base

### R√©seaux Fully-Connected (MLP)

```python
class MLP(nn.Module):
    """
    Multi-Layer Perceptron
    
    Architecture: Input ‚Üí [Hidden]* ‚Üí Output
    """
    
    def __init__(self, layer_sizes, activation='relu', dropout=0.0):
        super().__init__()
        
        self.layers = nn.ModuleList()
        self.dropouts = nn.ModuleList()
        
        # Cr√©e les couches
        for i in range(len(layer_sizes) - 1):
            self.layers.append(
                nn.Linear(layer_sizes[i], layer_sizes[i+1])
            )
            if i < len(layer_sizes) - 2:  # Pas de dropout sur la derni√®re couche
                self.dropouts.append(nn.Dropout(dropout))
        
        # Activation
        self.activation = {
            'relu': nn.ReLU(),
            'gelu': nn.GELU(),
            'tanh': nn.Tanh(),
            'sigmoid': nn.Sigmoid()
        }[activation]
        
    def forward(self, x):
        for i, layer in enumerate(self.layers[:-1]):
            x = layer(x)
            x = self.activation(x)
            if i < len(self.dropouts):
                x = self.dropouts[i](x)
        
        # Derni√®re couche sans activation
        x = self.layers[-1](x)
        return x
    
    def count_parameters(self):
        """Compte le nombre de param√®tres"""
        return sum(p.numel() for p in self.parameters())
    
    def layer_analysis(self):
        """Analyse de chaque couche"""
        analysis = []
        for i, layer in enumerate(self.layers):
            if isinstance(layer, nn.Linear):
                analysis.append({
                    'layer': i,
                    'type': 'Linear',
                    'input_dim': layer.in_features,
                    'output_dim': layer.out_features,
                    'parameters': layer.weight.numel() + layer.bias.numel(),
                    'weight_shape': tuple(layer.weight.shape)
                })
        return analysis

# Exemple
mlp = MLP([784, 512, 256, 128, 10], activation='relu', dropout=0.2)
print(f"Nombre total de param√®tres: {mlp.count_parameters():,}")

print("\nAnalyse des couches:")
for info in mlp.layer_analysis():
    print(f"  Couche {info['layer']}: {info['input_dim']} ‚Üí {info['output_dim']} "
          f"({info['parameters']:,} params)")
```

---

## Pourquoi la Profondeur ?

### Avantages des R√©seaux Profonds

```python
def depth_vs_width_analysis():
    """
    Compare r√©seaux profonds vs larges
    """
    # M√™me nombre de param√®tres, architectures diff√©rentes
    
    # R√©seau large et peu profond
    wide_shallow = MLP([100, 1000, 10])
    
    # R√©seau √©troit et profond
    narrow_deep = MLP([100, 100, 100, 100, 100, 100, 100, 100, 100, 10])
    
    print("Comparaison profondeur vs largeur:")
    print(f"  Large/Shallow: {wide_shallow.count_parameters():,} params, 2 couches")
    print(f"  Narrow/Deep: {narrow_deep.count_parameters():,} params, 9 couches")
    
    # Le r√©seau profond peut apprendre des repr√©sentations hi√©rarchiques
    # mais est plus difficile √† entra√Æner (vanishing gradients)

depth_vs_width_analysis()
```

### Repr√©sentations Hi√©rarchiques

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Hi√©rarchie des Repr√©sentations                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  Couche 1: Features bas niveau (bords, textures)               ‚îÇ
‚îÇ      ‚Üì                                                          ‚îÇ
‚îÇ  Couche 2: Combinaisons simples (coins, formes simples)        ‚îÇ
‚îÇ      ‚Üì                                                          ‚îÇ
‚îÇ  Couche 3: Motifs (parties d'objets)                           ‚îÇ
‚îÇ      ‚Üì                                                          ‚îÇ
‚îÇ  Couche N: Concepts abstraits (objets, cat√©gories)             ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Implications pour la Compression

### Redondance dans les R√©seaux

```python
def analyze_redundancy(model):
    """
    Analyse la redondance dans un mod√®le
    """
    results = {}
    
    for name, param in model.named_parameters():
        if 'weight' in name:
            W = param.data.numpy()
            
            # SVD pour analyser le rang effectif
            U, S, Vt = np.linalg.svd(W, full_matrices=False)
            
            # Rang effectif (99% de l'√©nergie)
            cumsum = np.cumsum(S**2) / np.sum(S**2)
            effective_rank = np.searchsorted(cumsum, 0.99) + 1
            
            # Sparsit√©
            sparsity = np.mean(np.abs(W) < 0.01)
            
            results[name] = {
                'shape': W.shape,
                'full_rank': min(W.shape),
                'effective_rank': effective_rank,
                'rank_ratio': effective_rank / min(W.shape),
                'sparsity': sparsity,
                'condition_number': S[0] / S[-1] if S[-1] > 1e-10 else np.inf
            }
    
    return results

# Analyse d'un MLP entra√Æn√©
mlp = MLP([256, 512, 256, 64, 10])

# Simulation d'entra√Ænement (les poids r√©els seraient diff√©rents)
print("Analyse de redondance (poids al√©atoires):")
redundancy = analyze_redundancy(mlp)
for name, info in redundancy.items():
    print(f"\n{name}:")
    print(f"  Shape: {info['shape']}")
    print(f"  Rang effectif: {info['effective_rank']} / {info['full_rank']} "
          f"({info['rank_ratio']:.1%})")
    print(f"  Sparsit√©: {info['sparsity']:.1%}")
```

### Opportunit√©s de Compression par Architecture

| Architecture | Technique Principale | Ratio Typique |
|--------------|---------------------|---------------|
| MLP | Low-rank factorization | 2-10x |
| CNN | Filter pruning | 2-5x |
| Transformer | Attention pruning + quantization | 4-8x |
| RNN/LSTM | Structured pruning | 2-4x |

---

## Framework PyTorch : Rappels Essentiels

```python
# Structure de base d'un entra√Ænement PyTorch
def training_loop(model, train_loader, optimizer, criterion, device, epochs=10):
    """
    Boucle d'entra√Ænement standard
    """
    model.to(device)
    model.train()
    
    history = {'loss': [], 'accuracy': []}
    
    for epoch in range(epochs):
        total_loss = 0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            # Forward pass
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            
            # Backward pass
            loss.backward()
            optimizer.step()
            
            # Statistiques
            total_loss += loss.item()
            pred = output.argmax(dim=1)
            correct += (pred == target).sum().item()
            total += target.size(0)
        
        avg_loss = total_loss / len(train_loader)
        accuracy = correct / total
        
        history['loss'].append(avg_loss)
        history['accuracy'].append(accuracy)
        
        print(f"Epoch {epoch+1}/{epochs}: Loss={avg_loss:.4f}, Acc={accuracy:.4f}")
    
    return history

# Exemple d'utilisation
"""
model = MLP([784, 256, 128, 10])
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
criterion = nn.CrossEntropyLoss()

history = training_loop(model, train_loader, optimizer, criterion, 'cuda')
"""
```

---

## Exercices Pr√©liminaires

### Exercice 3.0.1
Calculez le nombre de param√®tres d'un MLP avec architecture [1000, 500, 200, 50, 10].

### Exercice 3.0.2
Un r√©seau a 10 millions de param√®tres en float32. Quelle est sa taille en m√©moire ? En int8 ?

### Exercice 3.0.3
Impl√©mentez une fonction qui calcule le nombre de FLOPs pour un forward pass d'un MLP.

---

## Points Cl√©s √† Retenir

> üìå **Les r√©seaux profonds apprennent des repr√©sentations hi√©rarchiques**

> üìå **La redondance dans les poids permet la compression**

> üìå **Chaque architecture a ses opportunit√©s de compression sp√©cifiques**

> üìå **L'activation ReLU favorise la sparsit√© et facilite la compression**

---

*Commen√ßons par la premi√®re section : [3.1 R√©seaux de Neurones Feedforward](./03_01_Feedforward.md)*

