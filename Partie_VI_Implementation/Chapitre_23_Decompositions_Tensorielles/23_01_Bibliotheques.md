# 23.1 BibliothÃ¨ques Python (tensorly, tntorch)

---

## Introduction

Les bibliothÃ¨ques **tensorly** et **tntorch** fournissent des implÃ©mentations efficaces des dÃ©compositions tensorielles avec support pour diffÃ©rents backends (NumPy, PyTorch, TensorFlow). Cette section prÃ©sente comment utiliser ces bibliothÃ¨ques pour effectuer des dÃ©compositions tensorielles.

---

## TensorLy

### Installation et Configuration

```python
"""
Installation:
pip install tensorly

Backends supportÃ©s:
- NumPy (dÃ©faut)
- PyTorch
- TensorFlow
- JAX
- CuPy (GPU)
"""

import tensorly as tl
import numpy as np
import torch

# Changer backend
tl.set_backend('numpy')  # NumPy (dÃ©faut)
tl.set_backend('pytorch')  # PyTorch
tl.set_backend('tensorflow')  # TensorFlow

print(f"Backend actuel: {tl.get_backend()}")
```

---

## DÃ©compositions avec TensorLy

### CP Decomposition

```python
import tensorly as tl
from tensorly.decomposition import parafac
import numpy as np

# CrÃ©er tenseur 3D
tensor = np.random.randn(10, 20, 30)

# DÃ©composition CP avec rank R
rank = 5
factors = parafac(tensor, rank=rank)

# factors est liste de matrices
print(f"Nombre de facteurs: {len(factors)}")
for i, factor in enumerate(factors):
    print(f"Facteur {i} shape: {factor.shape}")

# Reconstruire tenseur
reconstructed = tl.cp_to_tensor(factors)
print(f"Erreur reconstruction: {np.linalg.norm(tensor - reconstructed):.6f}")
```

### Tucker Decomposition

```python
from tensorly.decomposition import tucker

# DÃ©composition Tucker
tucker_rank = [5, 10, 8]  # Ranks pour chaque mode
core, factors = tucker(tensor, rank=tucker_rank)

print(f"Core shape: {core.shape}")
print(f"Nombre de facteurs: {len(factors)}")

# Reconstruire
reconstructed = tl.tucker_to_tensor((core, factors))
print(f"Erreur reconstruction: {np.linalg.norm(tensor - reconstructed):.6f}")
```

### Tensor Train

```python
from tensorly.decomposition import matrix_product_state

# DÃ©composition Tensor Train
tt_rank = [1, 5, 5, 1]  # Bond dimensions
factors = matrix_product_state(tensor, rank=tt_rank)

# Reconstruire
reconstructed = tl.tt_to_tensor(factors)
print(f"Erreur reconstruction: {np.linalg.norm(tensor - reconstructed):.6f}")
```

---

## TensorLy avec PyTorch

### Backend PyTorch

```python
import tensorly as tl
import torch

# Changer backend PyTorch
tl.set_backend('pytorch')

# CrÃ©er tenseur PyTorch
tensor = torch.randn(10, 20, 30, requires_grad=True)

# DÃ©composition CP
rank = 5
factors = parafac(tensor, rank=rank)

# Les facteurs sont aussi des tenseurs PyTorch avec gradients
print(f"Facteur 0 requires_grad: {factors[0].requires_grad}")

# Reconstruire
reconstructed = tl.cp_to_tensor(factors)

# Utiliser dans loss
loss = torch.nn.functional.mse_loss(reconstructed, tensor)
loss.backward()
print(f"Gradients calculÃ©s pour facteurs")
```

---

## tntorch (Tensor Train)

### Installation et Utilisation

```python
"""
Installation:
pip install tntorch

Focus sur Tensor Train et formats compressÃ©s
"""

import tntorch as tn
import torch

# CrÃ©er tenseur PyTorch
tensor = torch.randn(10, 10, 10, 10)

# Compression en Tensor Train
tt = tn.TensorTrain(tensor, ranks_tt=5)  # Bond dimension 5

print(f"Tenseur original: {tensor.numel()} paramÃ¨tres")
print(f"Tensor Train: {tt.numel()} paramÃ¨tres")
print(f"Compression: {tensor.numel() / tt.numel():.2f}Ã—")

# OpÃ©rations sur Tensor Train
tt_sum = tt + tt  # Addition
tt_prod = tt * 2  # Multiplication scalaire
tt_dot = tn.dot(tt, tt)  # Produit scalaire

# Reconstruire tenseur complet
reconstructed = tt.torch()
print(f"Erreur reconstruction: {torch.norm(tensor - reconstructed):.6f}")
```

---

## OpÃ©rations avec tntorch

### Manipulation de Tensor Trains

```python
import tntorch as tn
import torch

# CrÃ©er Tensor Train
tt = tn.TensorTrain(tensor, ranks_tt=5)

# AccÃ©der aux cores
cores = tt.cores
print(f"Nombre de cores: {len(cores)}")
for i, core in enumerate(cores):
    print(f"Core {i} shape: {core.shape}")

# Modifier bond dimension
tt_compressed = tt.round(eps=1e-4)  # Compression additionnelle
print(f"Nouvelle compression: {tt_compressed.numel()} paramÃ¨tres")

# Slicing et indexing
tt_slice = tt[0:5, :, :, :]  # Slice premiÃ¨re dimension

# Fonctions mathÃ©matiques
tt_exp = tn.exp(tt)
tt_log = tn.log(tt + 1e-8)
```

---

## Comparaison TensorLy vs tntorch

### CaractÃ©ristiques

```python
class LibraryComparison:
    """
    Comparaison des bibliothÃ¨ques
    """
    
    def __init__(self):
        self.comparison = {
            'tensorly': {
                'strengths': [
                    'Multiple dÃ©compositions (CP, Tucker, TT)',
                    'Plusieurs backends (NumPy, PyTorch, TensorFlow)',
                    'Interface unifiÃ©e',
                    'Bien documentÃ©'
                ],
                'weaknesses': [
                    'Performance parfois limitÃ©e',
                    'Moins optimisÃ© que bibliothÃ¨ques spÃ©cialisÃ©es'
                ],
                'best_for': 'Prototypage, expÃ©rimentation, comparaison mÃ©thodes'
            },
            'tntorch': {
                'strengths': [
                    'OptimisÃ© pour Tensor Train',
                    'Interface PyTorch native',
                    'OpÃ©rations sur TT compressÃ©s',
                    'Meilleures performances'
                ],
                'weaknesses': [
                    'Focus sur TT uniquement',
                    'Backend PyTorch seulement'
                ],
                'best_for': 'Production, modÃ¨les TT compressÃ©s, PyTorch'
            }
        }
    
    def display_comparison(self):
        """Affiche comparaison"""
        print("\n" + "="*70)
        print("Comparaison TensorLy vs tntorch")
        print("="*70)
        
        for lib, info in self.comparison.items():
            print(f"\n{lib.upper()}:")
            print("  Forces:")
            for strength in info['strengths']:
                print(f"    + {strength}")
            print("  Faiblesses:")
            for weakness in info['weaknesses']:
                print(f"    - {weakness}")
            print(f"  IdÃ©al pour: {info['best_for']}")

comparison = LibraryComparison()
comparison.display_comparison()
```

---

## Exemple Complet: Compression de Poids

### Application RÃ©elle

```python
import tensorly as tl
import torch
from tensorly.decomposition import parafac

# Simuler poids d'une couche dense
original_weights = torch.randn(100, 50)  # Couche 100 â†’ 50

# Tensoriser en 3D: (10, 10) Ã— (10, 5)
weights_tensorized = original_weights.reshape(10, 10, 10, 5)

# DÃ©composition CP
rank = 8
factors = parafac(weights_tensorized, rank=rank)

# Reconstruire
reconstructed = tl.cp_to_tensor(factors)
reconstructed_weights = reconstructed.reshape(100, 50)

# Comparaison
original_params = original_weights.numel()  # 5000
compressed_params = sum(f.numel() for f in factors)  # ~800 (dÃ©pend rank)

compression_ratio = original_params / compressed_params
error = torch.norm(original_weights - reconstructed_weights) / torch.norm(original_weights)

print(f"\nCompression de Poids:")
print(f"  ParamÃ¨tres originaux: {original_params}")
print(f"  ParamÃ¨tres compressÃ©s: {compressed_params}")
print(f"  Ratio compression: {compression_ratio:.2f}Ã—")
print(f"  Erreur relative: {error:.4f}")
```

---

## Exercices

### Exercice 23.1.1
Installez tensorly et testez dÃ©composition CP, Tucker, et TT sur un tenseur 4D.

### Exercice 23.1.2
Comparez performance entre backend NumPy et PyTorch dans tensorly pour mÃªme dÃ©composition.

### Exercice 23.1.3
Utilisez tntorch pour compresser un tenseur 5D et comparez erreur de reconstruction vs compression ratio.

### Exercice 23.1.4
ImplÃ©mentez compression de couche dense avec dÃ©composition CP en utilisant tensorly avec backend PyTorch.

---

## Points ClÃ©s Ã  Retenir

> ğŸ“Œ **TensorLy supporte multiple dÃ©compositions et backends**

> ğŸ“Œ **tntorch est optimisÃ© pour Tensor Train avec PyTorch**

> ğŸ“Œ **Le choix de backend impact performance et fonctionnalitÃ©s**

> ğŸ“Œ **Les bibliothÃ¨ques simplifient utilisation mais comprendre implÃ©mentation reste important**

> ğŸ“Œ **La compression peut rÃ©duire paramÃ¨tres significativement avec faible erreur**

---

*Section prÃ©cÃ©dente : [23.0 Introduction](./23_introduction.md) | Section suivante : [23.2 DÃ©composition CP](./23_02_Decomposition_CP.md)*

