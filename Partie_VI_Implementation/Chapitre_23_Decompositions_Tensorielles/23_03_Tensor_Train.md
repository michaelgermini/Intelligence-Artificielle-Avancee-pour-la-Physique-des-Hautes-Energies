# 23.3 ImplÃ©mentation du Tensor Train

---

## Introduction

Le **Tensor Train (TT)** est une dÃ©composition particuliÃ¨rement efficace pour les tenseurs de haute dimension. Cette section prÃ©sente l'implÃ©mentation du Tensor Train, incluant l'algorithme de construction, les opÃ©rations sur les TT, et les optimisations.

---

## Formulation MathÃ©matique

### Structure Tensor Train

```python
"""
Tensor Train d'un tenseur T de shape (Iâ‚, Iâ‚‚, ..., Iâ‚™):

T[iâ‚, iâ‚‚, ..., iâ‚™] = Gâ‚[iâ‚] Gâ‚‚[iâ‚‚] ... Gâ‚™[iâ‚™]

oÃ¹ Gâ‚–[iâ‚–] est une matrice (râ‚–â‚‹â‚ Ã— râ‚–) pour chaque valeur iâ‚–.

Les cores sont des tenseurs:
- Gâ‚: (Iâ‚ Ã— râ‚)
- Gâ‚–: (râ‚–â‚‹â‚ Ã— Iâ‚– Ã— râ‚–) pour k = 2, ..., n-1
- Gâ‚™: (râ‚™â‚‹â‚ Ã— Iâ‚™)

Bond dimensions: [1, râ‚, râ‚‚, ..., râ‚™â‚‹â‚, 1]
"""
```

---

## ImplÃ©mentation Basique

### Construction depuis Tenseur

```python
import numpy as np

class TensorTrain:
    """
    ReprÃ©sentation Tensor Train
    """
    
    def __init__(self, cores=None, ranks=None):
        """
        Args:
            cores: Liste des cores [Gâ‚, Gâ‚‚, ..., Gâ‚™]
            ranks: Bond dimensions [1, râ‚, râ‚‚, ..., râ‚™â‚‹â‚, 1]
        """
        self.cores = cores
        self.ranks = ranks if ranks else self._compute_ranks()
    
    def _compute_ranks(self):
        """Calcule bond dimensions depuis cores"""
        if self.cores is None:
            return None
        
        ranks = [1]
        for i in range(len(self.cores) - 1):
            ranks.append(self.cores[i].shape[-1])
        ranks.append(1)
        return ranks
    
    def from_tensor(self, tensor, max_rank=None, eps=1e-6):
        """
        Construit TT depuis tenseur avec SVD
        
        Args:
            tensor: Tenseur Ã  dÃ©composer
            max_rank: Rank maximum (truncation)
            eps: TolÃ©rance pour truncation
        """
        shape = tensor.shape
        n_modes = len(shape)
        
        cores = []
        remaining = tensor
        
        for mode in range(n_modes - 1):
            # Reshape en matrice
            I_current = shape[mode]
            I_remaining = np.prod(shape[mode+1:])
            
            matrix = remaining.reshape(I_current, I_remaining)
            
            # SVD
            U, s, Vt = np.linalg.svd(matrix, full_matrices=False)
            
            # Truncation
            if max_rank:
                rank = min(max_rank, len(s))
            else:
                # Truncation basÃ©e sur eps
                cumsum_sq = np.cumsum(s**2)
                total = cumsum_sq[-1]
                rank = np.searchsorted(cumsum_sq, total * (1 - eps)) + 1
                rank = min(rank, len(s))
            
            # Prendre r premiÃ¨res composantes
            U = U[:, :rank]
            s = s[:rank]
            Vt = Vt[:rank, :]
            
            # Core actuel
            core = U.reshape(I_current, rank)
            cores.append(core)
            
            # PrÃ©parer pour prochaine itÃ©ration
            remaining = (np.diag(s) @ Vt).reshape(rank, *shape[mode+1:])
        
        # Dernier core
        cores.append(remaining.reshape(remaining.shape[0], remaining.shape[1]))
        
        self.cores = cores
        self.ranks = self._compute_ranks()
        
        return self
    
    def to_tensor(self):
        """Reconstruit tenseur depuis TT"""
        if self.cores is None:
            raise ValueError("No cores available")
        
        result = self.cores[0]
        
        for core in self.cores[1:]:
            # Contracter: result (..., r) avec core (r, I, r')
            # RÃ©sultat: (..., I, r')
            
            # Reshape pour contraction
            result = np.tensordot(result, core, axes=([-1], [0]))
        
        # Squeeze dimensions unitaires
        result = np.squeeze(result)
        
        return result
    
    def numel(self):
        """Nombre de paramÃ¨tres dans TT"""
        if self.cores is None:
            return 0
        
        total = 0
        for core in self.cores:
            total += core.size
        return total

# Test
tt = TensorTrain()
tensor = np.random.randn(5, 6, 7, 8)
tt.from_tensor(tensor, max_rank=5)

print(f"Tenseur original: {tensor.size} paramÃ¨tres")
print(f"Tensor Train: {tt.numel()} paramÃ¨tres")
print(f"Compression: {tensor.size / tt.numel():.2f}Ã—")

reconstructed = tt.to_tensor()
error = np.linalg.norm(tensor - reconstructed) / np.linalg.norm(tensor)
print(f"Erreur relative: {error:.6f}")
```

---

## OpÃ©rations sur Tensor Train

### Addition et Multiplication

```python
class TensorTrainOperations(TensorTrain):
    """
    OpÃ©rations sur Tensor Trains
    """
    
    def add(self, other):
        """Addition de deux Tensor Trains"""
        if len(self.cores) != len(other.cores):
            raise ValueError("TT must have same number of modes")
        
        new_cores = []
        for i, (core1, core2) in enumerate(zip(self.cores, other.cores)):
            if i == 0:
                # Premier core: concatÃ©ner colonnes
                new_core = np.concatenate([core1, core2], axis=1)
            elif i == len(self.cores) - 1:
                # Dernier core: concatÃ©ner lignes
                new_core = np.concatenate([core1, core2], axis=0)
            else:
                # Cores intermÃ©diaires: blocs diagonaux
                r1_prev, I, r1_next = core1.shape
                r2_prev, _, r2_next = core2.shape
                
                # CrÃ©er bloc diagonal
                new_core = np.zeros((r1_prev + r2_prev, I, r1_next + r2_next))
                new_core[:r1_prev, :, :r1_next] = core1
                new_core[r1_prev:, :, r1_next:] = core2
                
                new_cores.append(new_core)
        
        return TensorTrain(cores=new_cores)
    
    def multiply_scalar(self, scalar):
        """Multiplication par scalaire"""
        new_cores = [core.copy() for core in self.cores]
        # Multiplier premier ou dernier core
        new_cores[0] *= scalar
        return TensorTrain(cores=new_cores)
    
    def round(self, eps=1e-6, max_rank=None):
        """Compression du TT"""
        # RÃ©appliquer SVD sur chaque liaison
        # SimplifiÃ©: reconstruire et redÃ©composer
        tensor = self.to_tensor()
        compressed = TensorTrain()
        compressed.from_tensor(tensor, max_rank=max_rank, eps=eps)
        return compressed

# Test opÃ©rations
tt1 = TensorTrain()
tt1.from_tensor(np.random.randn(5, 6, 7), max_rank=3)

tt2 = TensorTrain()
tt2.from_tensor(np.random.randn(5, 6, 7), max_rank=3)

tt_sum = tt1.add(tt2)
print(f"TT1: {tt1.numel()} paramÃ¨tres")
print(f"TT2: {tt2.numel()} paramÃ¨tres")
print(f"TT sum: {tt_sum.numel()} paramÃ¨tres")
```

---

## Version PyTorch

### Support GPU et Gradients

```python
import torch

class PyTorchTensorTrain:
    """
    Tensor Train avec PyTorch
    """
    
    def __init__(self, cores=None, device='cpu'):
        self.cores = cores
        self.device = device
    
    def from_tensor(self, tensor, max_rank=None, eps=1e-6):
        """Construit TT depuis tenseur PyTorch"""
        tensor = tensor.to(self.device)
        shape = tensor.shape
        n_modes = len(shape)
        
        cores = []
        remaining = tensor
        
        for mode in range(n_modes - 1):
            I_current = shape[mode]
            I_remaining = torch.prod(torch.tensor(shape[mode+1:]))
            
            matrix = remaining.reshape(I_current, int(I_remaining))
            
            # SVD
            U, s, Vt = torch.linalg.svd(matrix, full_matrices=False)
            
            # Truncation
            if max_rank:
                rank = min(max_rank, len(s))
            else:
                cumsum_sq = torch.cumsum(s**2, dim=0)
                total = cumsum_sq[-1]
                rank = torch.searchsorted(cumsum_sq, total * (1 - eps)) + 1
                rank = min(int(rank), len(s))
            
            U = U[:, :rank]
            s = s[:rank]
            Vt = Vt[:rank, :]
            
            core = U.reshape(I_current, rank)
            cores.append(core)
            
            remaining = (torch.diag(s) @ Vt).reshape(rank, *shape[mode+1:])
        
        cores.append(remaining.reshape(remaining.shape[0], remaining.shape[1]))
        
        self.cores = cores
        return self
    
    def to_tensor(self):
        """Reconstruit tenseur"""
        result = self.cores[0]
        
        for core in self.cores[1:]:
            result = torch.tensordot(result, core, dims=([-1], [0]))
        
        return result.squeeze()
    
    def numel(self):
        """Nombre de paramÃ¨tres"""
        return sum(core.numel() for core in self.cores)

# Test PyTorch
device = 'cuda' if torch.cuda.is_available() else 'cpu'
tt_torch = PyTorchTensorTrain(device=device)

tensor_torch = torch.randn(5, 6, 7, 8, device=device, requires_grad=True)
tt_torch.from_tensor(tensor_torch, max_rank=5)

print(f"TT PyTorch: {tt_torch.numel()} paramÃ¨tres")
print(f"Compression: {tensor_torch.numel() / tt_torch.numel():.2f}Ã—")

# Gradients
reconstructed = tt_torch.to_tensor()
loss = reconstructed.sum()
loss.backward()
print(f"Gradients calculÃ©s pour cores")
```

---

## Exercices

### Exercice 23.3.1
ImplÃ©mentez construction TT depuis tenseur avec SVD et testez sur tenseurs de diffÃ©rentes dimensions.

### Exercice 23.3.2
Comparez compression ratio vs erreur pour diffÃ©rents max_rank.

### Exercice 23.3.3
ImplÃ©mentez opÃ©rations (addition, multiplication) sur Tensor Trains.

### Exercice 23.3.4
Utilisez version PyTorch pour entraÃ®ner modÃ¨le avec contraintes TT.

---

## Points ClÃ©s Ã  Retenir

> ğŸ“Œ **Tensor Train construit via SVD sÃ©quentiel**

> ğŸ“Œ **Truncation contrÃ´le trade-off compression/prÃ©cision**

> ğŸ“Œ **OpÃ©rations sur TT peuvent Ãªtre faites sans reconstruire**

> ğŸ“Œ **Support PyTorch permet gradients et GPU**

> ğŸ“Œ **TT est particuliÃ¨rement efficace pour haute dimension**

---

*Section prÃ©cÃ©dente : [23.2 DÃ©composition CP](./23_02_Decomposition_CP.md) | Section suivante : [23.4 Optimisation](./23_04_Optimisation.md)*

