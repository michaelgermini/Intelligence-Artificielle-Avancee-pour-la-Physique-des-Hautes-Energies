# 18.3 High-Level Trigger (HLT)

---

## Introduction

Le **High-Level Trigger (HLT)** est le second niveau de s√©lection apr√®s le L1. Contrairement au L1 qui fonctionne en hardware, le HLT utilise une **farm de processeurs (CPU/GPU)** et dispose d'environ **300 millisecondes** par √©v√©nement pour prendre une d√©cision. Ce temps permet l'utilisation d'algorithmes de reconstruction complets et de mod√®les de machine learning sophistiqu√©s.

Cette section d√©taille l'architecture HLT, les algorithmes de reconstruction utilis√©s, l'int√©gration du ML, et l'optimisation des performances.

---

## Architecture HLT

### Farm de Processeurs

```python
import numpy as np
import torch
import torch.nn as nn
from typing import Dict, List, Tuple

class HLTArchitecture:
    """
    Architecture du High-Level Trigger
    """
    
    def __init__(self):
        self.config = {
            'input_rate_khz': 100,  # Depuis L1
            'target_output_rate_hz': 1000,  # 1 kHz
            'average_latency_ms': 250,
            'max_latency_ms': 300,
            'event_size_mb': 1.5,
            'n_nodes': 1000,
            'cores_per_node': 32,
            'gpus_per_node': 4
        }
        
        self.total_cores = self.config['n_nodes'] * self.config['cores_per_node']
        self.total_gpus = self.config['n_nodes'] * self.config['gpus_per_node']
    
    def compute_throughput_requirements(self):
        """Calcule les besoins en throughput"""
        input_rate = self.config['input_rate_khz'] * 1000  # Hz
        
        # Throughput n√©cessaire
        events_per_sec = input_rate
        bandwidth_gbps = events_per_sec * self.config['event_size_mb'] * 8 / 1000
        
        # CPU cores n√©cessaires (si un core traite un √©v√©nement √† la fois)
        avg_time_per_event_s = self.config['average_latency_ms'] / 1000
        cores_needed = events_per_sec * avg_time_per_event_s
        
        # Avec parall√©lisme et overhead
        cores_with_overhead = cores_needed / 0.7  # 70% d'efficacit√©
        
        return {
            'input_rate_hz': input_rate,
            'bandwidth_gbps': bandwidth_gbps,
            'cores_needed': cores_needed,
            'cores_with_overhead': cores_with_overhead,
            'utilization': cores_with_overhead / self.total_cores
        }
    
    def display_architecture(self):
        """Affiche l'architecture"""
        print("\n" + "="*70)
        print("Architecture High-Level Trigger")
        print("="*70)
        
        print(f"\nConfiguration:")
        print(f"  Input rate: {self.config['input_rate_khz']} kHz")
        print(f"  Target output: {self.config['target_output_rate_hz']} Hz")
        print(f"  Latence moyenne: {self.config['average_latency_ms']} ms")
        print(f"  Latence max: {self.config['max_latency_ms']} ms")
        
        print(f"\nRessources:")
        print(f"  N≈ìuds: {self.config['n_nodes']}")
        print(f"  Cores par n≈ìud: {self.config['cores_per_node']}")
        print(f"  Total cores: {self.total_cores:,}")
        print(f"  GPUs par n≈ìud: {self.config['gpus_per_node']}")
        print(f"  Total GPUs: {self.total_gpus:,}")
        
        requirements = self.compute_throughput_requirements()
        print(f"\nBesoins:")
        print(f"  Bandwidth: {requirements['bandwidth_gbps']:.1f} Gbps")
        print(f"  Cores n√©cessaires: {requirements['cores_with_overhead']:.0f}")
        print(f"  Utilisation: {requirements['utilization']*100:.1f}%")

hlt_arch = HLTArchitecture()
hlt_arch.display_architecture()
```

---

## Pipeline de Traitement HLT

### Stages de Reconstruction

```python
class HLTProcessingPipeline:
    """
    Pipeline de traitement HLT
    """
    
    def __init__(self):
        self.stages = [
            {
                'name': 'Event Building',
                'time_ms': 10,
                'description': 'Reconstruction √©v√©nement depuis fragments',
                'parallelizable': True
            },
            {
                'name': 'Track Reconstruction',
                'time_ms': 80,
                'description': 'Reconstruction traces dans tracker',
                'parallelizable': True,
                'ml_used': False
            },
            {
                'name': 'Calorimeter Reconstruction',
                'time_ms': 50,
                'description': 'Reconstruction clusters calorim√©triques',
                'parallelizable': True,
                'ml_used': True
            },
            {
                'name': 'Muon Reconstruction',
                'time_ms': 30,
                'description': 'Reconstruction muons',
                'parallelizable': True,
                'ml_used': False
            },
            {
                'name': 'Jet Reconstruction',
                'time_ms': 40,
                'description': 'Jet clustering (anti-kT)',
                'parallelizable': True,
                'ml_used': False
            },
            {
                'name': 'b-tagging ML',
                'time_ms': 30,
                'description': 'Classification jets (b-tagging)',
                'parallelizable': True,
                'ml_used': True
            },
            {
                'name': 'MET Reconstruction',
                'time_ms': 10,
                'description': 'Energie manquante transverse',
                'parallelizable': True,
                'ml_used': False
            },
            {
                'name': 'Trigger Decision',
                'time_ms': 10,
                'description': 'D√©cision finale HLT',
                'parallelizable': False,
                'ml_used': True
            }
        ]
        
        self.total_time_ms = sum(stage['time_ms'] for stage in self.stages)
    
    def analyze_pipeline(self):
        """Analyse le pipeline"""
        print("\n" + "="*70)
        print("Pipeline de Traitement HLT")
        print("="*70)
        
        print(f"\n{'Stage':<30} {'Temps (ms)':<12} {'ML':<8} {'Parall√©lisable'}")
        print("-" * 70)
        
        for stage in self.stages:
            ml_str = "Oui" if stage.get('ml_used', False) else "Non"
            para_str = "Oui" if stage.get('parallelizable', False) else "Non"
            print(f"{stage['name']:<30} {stage['time_ms']:<11} ms {ml_str:<8} {para_str}")
        
        print(f"\nTotal: {self.total_time_ms} ms")
        
        # Analyse ML
        ml_stages = [s for s in self.stages if s.get('ml_used', False)]
        ml_time = sum(s['time_ms'] for s in ml_stages)
        print(f"\nStages ML: {len(ml_stages)} ({ml_time} ms = {ml_time/self.total_time_ms*100:.1f}%)")

pipeline = HLTProcessingPipeline()
pipeline.analyze_pipeline()
```

---

## Reconstruction HLT

### Track Reconstruction

```python
class HLTTrackReconstruction:
    """
    Reconstruction de traces dans le HLT
    """
    
    def __init__(self):
        self.tracker_layers = 10  # Nombre de couches du tracker
        self.min_hits = 5  # Minimum hits pour une trace valide
    
    def hit_seeding(self, tracker_hits: np.ndarray):
        """
        Cr√©e des seeds (graines) de traces
        
        Utilise des paires/triplets de hits dans couches voisines
        """
        seeds = []
        
        # Recherche de triplets dans premi√®res couches
        for layer1 in range(self.tracker_layers - 2):
            for hit1 in tracker_hits[layer1]:
                for layer2 in range(layer1 + 1, min(layer1 + 3, self.tracker_layers)):
                    for hit2 in tracker_hits[layer2]:
                        # Calculer param√®tres de trace (pente)
                        if self._hits_compatible(hit1, hit2):
                            seeds.append({
                                'hits': [hit1, hit2],
                                'layers': [layer1, layer2],
                                'estimated_pt': self._estimate_pt_from_seed(hit1, hit2)
                            })
        
        return seeds
    
    def kalman_filter_tracking(self, seed: Dict, all_hits: np.ndarray):
        """
        Kalman filter pour extension de trace
        
        Algorithme classique mais efficace
        """
        track = seed['hits'].copy()
        current_hit = seed['hits'][-1]
        current_layer = seed['layers'][-1]
        
        # Param√®tres de trace (simplifi√©s)
        track_params = self._extract_track_params(seed['hits'])
        
        # Extension couche par couche
        for next_layer in range(current_layer + 1, self.tracker_layers):
            # Pr√©diction Kalman
            predicted_hit = self._kalman_predict(track_params, next_layer)
            
            # Trouver hit le plus proche
            closest_hit = self._find_closest_hit(all_hits[next_layer], predicted_hit)
            
            if closest_hit and self._hit_chi2_acceptable(closest_hit, predicted_hit):
                # Mise √† jour Kalman
                track.append(closest_hit)
                track_params = self._kalman_update(track_params, closest_hit)
            else:
                break
        
        return {
            'hits': track,
            'n_hits': len(track),
            'pt': self._calculate_pt(track)
        }
    
    def _hits_compatible(self, hit1: Dict, hit2: Dict) -> bool:
        """V√©rifie compatibilit√© de hits"""
        # Simplifi√©: compatibles si angle raisonnable
        return True  # Placeholder
    
    def _estimate_pt_from_seed(self, hit1: Dict, hit2: Dict) -> float:
        """Estime pT depuis seed"""
        # Simplifi√©
        return 10.0  # GeV
    
    def _extract_track_params(self, hits: List[Dict]) -> Dict:
        """Extrait param√®tres de trace"""
        return {'slope': 0.1, 'intercept': 0.0}
    
    def _kalman_predict(self, params: Dict, layer: int) -> Dict:
        """Pr√©diction Kalman"""
        return {'x': 0, 'y': 0}
    
    def _find_closest_hit(self, hits: List[Dict], predicted: Dict) -> Dict:
        """Trouve hit le plus proche"""
        if len(hits) == 0:
            return None
        return hits[0]  # Simplifi√©
    
    def _hit_chi2_acceptable(self, hit: Dict, predicted: Dict, chi2_cut: float = 5.0) -> bool:
        """V√©rifie chi2"""
        return True  # Simplifi√©
    
    def _kalman_update(self, params: Dict, hit: Dict) -> Dict:
        """Mise √† jour Kalman"""
        return params
    
    def _calculate_pt(self, track: List[Dict]) -> float:
        """Calcule pT de la trace"""
        return 25.0  # GeV

track_reco = HLTTrackReconstruction()
```

---

## Machine Learning dans le HLT

### Mod√®les pour Classification

```python
class HLTMLModels:
    """
    Mod√®les ML utilis√©s dans le HLT
    """
    
    def __init__(self):
        pass
    
    def create_btagging_model(self, input_dim=50):
        """
        Mod√®le de b-tagging (classification de jets)
        """
        model = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3),
            
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.3),
            
            nn.Linear(64, 32),
            nn.ReLU(),
            
            nn.Linear(32, 3)  # b-jet, c-jet, light-jet
        )
        
        return model
    
    def create_electron_id_model(self, input_dim=30):
        """
        Mod√®le d'identification d'√©lectrons
        """
        model = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(64, 32),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            
            nn.Linear(32, 1),
            nn.Sigmoid()  # Probabilit√© √©lectron
        )
        
        return model
    
    def create_global_classifier(self, n_features=200, n_classes=10):
        """
        Classificateur global pour d√©cision trigger
        """
        model = nn.Sequential(
            nn.Linear(n_features, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.3),
            
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(128, 64),
            nn.ReLU(),
            
            nn.Linear(64, n_classes)
        )
        
        return model

ml_models = HLTMLModels()
```

---

## Menu de Trigger HLT

### Syst√®me de Chemins

```python
class HLTTriggerMenu:
    """
    Menu de trigger HLT avec chemins configurables
    """
    
    def __init__(self):
        self.paths = {}
    
    def add_path(self, name: str, model: nn.Module, 
                threshold: float, prescale: int = 1,
                requirements: Dict = None):
        """
        Ajoute un chemin de trigger
        
        Args:
            name: Nom du chemin (ex: 'HLT_Electron_25')
            model: Mod√®le ML pour ce chemin
            threshold: Seuil de d√©cision
            prescale: Prescale factor
            requirements: Conditions additionnelles (ex: {'n_jets': 2})
        """
        self.paths[name] = {
            'model': model,
            'threshold': threshold,
            'prescale': prescale,
            'requirements': requirements or {},
            'triggered_count': 0
        }
    
    def evaluate_event(self, event_features: Dict) -> Dict:
        """
        √âvalue tous les chemins pour un √©v√©nement
        
        Args:
            event_features: Dict avec features de l'√©v√©nement
        """
        results = {}
        
        for path_name, path_config in self.paths.items():
            # V√©rifier requirements
            if not self._check_requirements(event_features, path_config['requirements']):
                results[path_name] = {'passed': False, 'reason': 'requirements_not_met'}
                continue
            
            # Inference ML
            model = path_config['model']
            features = self._extract_features_for_path(event_features, path_name)
            
            with torch.no_grad():
                if isinstance(features, np.ndarray):
                    features = torch.tensor(features, dtype=torch.float32).unsqueeze(0)
                
                output = model(features)
                
                if output.shape[1] == 1:
                    score = torch.sigmoid(output).item()
                else:
                    probs = torch.softmax(output, dim=1)
                    score = probs[0, 1].item()  # Probabilit√© classe positive
            
            # D√©cision
            passed = score > path_config['threshold']
            
            # Appliquer prescale
            if passed:
                path_config['triggered_count'] += 1
                if path_config['prescale'] > 1:
                    if path_config['triggered_count'] % path_config['prescale'] != 0:
                        passed = False
            
            results[path_name] = {
                'passed': passed,
                'score': score,
                'threshold': path_config['threshold']
            }
        
        # D√©cision globale: √©v√©nement passe si au moins un chemin passe
        event_passed = any(r['passed'] for r in results.values())
        
        return {
            'event_passed': event_passed,
            'paths': results
        }
    
    def _check_requirements(self, event_features: Dict, requirements: Dict) -> bool:
        """V√©rifie les conditions additionnelles"""
        for req_name, req_value in requirements.items():
            if req_name in event_features:
                if event_features[req_name] < req_value:
                    return False
        return True
    
    def _extract_features_for_path(self, event_features: Dict, path_name: str) -> np.ndarray:
        """Extrait features pertinentes pour un chemin"""
        # Simplifi√©: retourne toutes les features
        if 'features' in event_features:
            return event_features['features']
        return np.zeros(100)  # Placeholder
    
    def get_statistics(self) -> Dict:
        """Retourne statistiques sur les chemins"""
        stats = {}
        for path_name, path_config in self.paths.items():
            stats[path_name] = {
                'triggered_count': path_config['triggered_count'],
                'threshold': path_config['threshold'],
                'prescale': path_config['prescale']
            }
        return stats

# Exemple de menu
hlt_menu = HLTTriggerMenu()

# Cr√©er mod√®les
btag_model = ml_models.create_btagging_model(input_dim=50)
electron_model = ml_models.create_electron_id_model(input_dim=30)
global_model = ml_models.create_global_classifier(n_features=200, n_classes=10)

# Ajouter chemins
hlt_menu.add_path('HLT_Electron_25', electron_model, threshold=0.9, prescale=1)
hlt_menu.add_path('HLT_DiJet_100', global_model, threshold=0.85, prescale=1,
                 requirements={'n_jets': 2})
hlt_menu.add_path('HLT_BJet_50', btag_model, threshold=0.8, prescale=1)

print("\nMenu HLT configur√© avec 3 chemins")
```

---

## Optimisation des Performances

### Techniques d'Optimisation

```python
class HLTOptimization:
    """
    Techniques d'optimisation pour HLT
    """
    
    @staticmethod
    def batch_processing(events: List[Dict], batch_size: int = 32):
        """
        Traite √©v√©nements par batch pour efficacit√© GPU
        """
        results = []
        
        for i in range(0, len(events), batch_size):
            batch = events[i:i+batch_size]
            
            # Extract features batch
            features_batch = torch.tensor([e['features'] for e in batch], dtype=torch.float32)
            
            # Inference batch (sur GPU si disponible)
            # model(features_batch)  # Plus efficace que boucle
            
            results.extend([{'processed': True} for _ in batch])
        
        return results
    
    @staticmethod
    def early_stopping(event_features: Dict, thresholds: Dict) -> bool:
        """
        Early stopping: rejeter √©v√©nement t√¥t si score faible
        """
        # √âvaluer chemins les plus rapides d'abord
        fast_paths = ['HLT_Electron_25']  # Exemple
        
        for path_name in fast_paths:
            # Score rapide
            quick_score = np.random.random()  # Placeholder
            
            if quick_score < thresholds.get(path_name, 0.5) * 0.5:
                # Probablement background, rejeter t√¥t
                return True  # Stop early
        
        return False  # Continuer traitement complet
    
    @staticmethod
    def model_caching():
        """
        Cache mod√®les en m√©moire pour √©viter rechargement
        """
        # En pratique: charger mod√®les une fois et r√©utiliser
        return {
            'btag_model': None,  # Loaded once
            'electron_model': None,
            'global_model': None
        }
    
    @staticmethod
    def parallel_execution(paths: List[str], event_features: Dict) -> Dict:
        """
        Ex√©cute chemins en parall√®le (multi-threading)
        """
        import threading
        
        results = {}
        
        def evaluate_path(path_name):
            # √âvaluation chemin
            results[path_name] = {'passed': False}  # Placeholder
        
        threads = []
        for path_name in paths:
            thread = threading.Thread(target=evaluate_path, args=(path_name,))
            threads.append(thread)
            thread.start()
        
        for thread in threads:
            thread.join()
        
        return results

hlt_opt = HLTOptimization()
```

---

## Exercices

### Exercice 18.3.1
Optimisez un menu HLT pour minimiser la latence moyenne tout en maintenant l'efficacit√© signal > 95%.

### Exercice 18.3.2
Impl√©mentez un syst√®me de batch processing pour traiter 100 √©v√©nements en parall√®le sur GPU.

### Exercice 18.3.3
D√©veloppez un syst√®me d'early stopping qui rejette 80% des backgrounds en < 50 ms.

### Exercice 18.3.4
Analysez l'impact de la quantification 8-bit vs float32 sur les performances HLT.

---

## Points Cl√©s √† Retenir

> üìå **Le HLT dispose de ~300 ms par √©v√©nement, permettant reconstruction compl√®te**

> üìå **Les algorithmes classiques (Kalman filter, jet clustering) sont utilis√©s avec ML**

> üìå **Le menu de trigger HLT contient des centaines de chemins configurables**

> üìå **L'optimisation (batching, early stopping, parall√©lisme) est critique pour throughput**

> üìå **Les mod√®les ML peuvent √™tre plus complexes qu'au L1 (r√©seaux profonds possibles)**

> üìå **Le prescaling permet de contr√¥ler le taux de chaque chemin individuellement**

---

*Section pr√©c√©dente : [18.2 Level-1 Trigger](./18_02_L1_Trigger.md) | Section suivante : [18.4 Int√©gration de l'IA](./18_04_IA_Trigger.md)*

