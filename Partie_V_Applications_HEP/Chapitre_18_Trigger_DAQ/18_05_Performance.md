# 18.5 Latence et Requirements de Performance

---

## Introduction

Les **requirements de performance** des syst√®mes de trigger sont extr√™mement stricts et constituent l'un des d√©fis majeurs de la physique des hautes √©nergies. La latence, le throughput, l'efficacit√©, et la puret√© doivent √™tre optimis√©s simultan√©ment sous des contraintes hardware et temporelles s√©v√®res.

Cette section d√©taille les m√©triques de performance, les techniques d'optimisation, et les m√©thodes de benchmark pour √©valuer et am√©liorer les performances des syst√®mes de trigger.

---

## M√©triques de Performance

### D√©finitions et Mesures

```python
import numpy as np
import time
from typing import Dict, List, Tuple

class TriggerPerformanceMetrics:
    """
    M√©triques de performance pour syst√®mes de trigger
    """
    
    def __init__(self):
        self.metrics_definitions = {
            'latency': {
                'description': 'Temps entre input et d√©cision',
                'l1_target_us': 4.0,
                'hlt_target_ms': 300.0,
                'measurement': 'End-to-end time'
            },
            'throughput': {
                'description': 'Taux d\'√©v√©nements trait√©s',
                'l1_target_hz': 40e6,
                'hlt_target_hz': 100e3,
                'measurement': 'Events per second'
            },
            'efficiency': {
                'description': 'Fraction de signal conserv√©',
                'target': 0.95,
                'measurement': 'True positive rate'
            },
            'purity': {
                'description': 'Fraction de signal dans √©v√©nements accept√©s',
                'target': 0.80,
                'measurement': 'Signal / (Signal + Background)'
            },
            'rate': {
                'description': 'Taux de d√©clenchement',
                'l1_target_khz': 100,
                'hlt_target_hz': 1000,
                'measurement': 'Output rate'
            }
        }
    
    def compute_efficiency(self, true_labels: np.ndarray, 
                          decisions: np.ndarray,
                          signal_class: int = 1) -> Dict:
        """
        Calcule l'efficacit√© de s√©lection
        
        Args:
            true_labels: Labels r√©els (0=background, 1=signal)
            decisions: D√©cisions trigger (0=reject, 1=accept)
            signal_class: Classe consid√©r√©e comme signal
        """
        signal_mask = true_labels == signal_class
        
        # Efficacit√© signal: fraction de signal accept√©
        signal_decisions = decisions[signal_mask]
        signal_efficiency = signal_decisions.mean()
        
        # Efficacit√© par pT (si disponible)
        return {
            'signal_efficiency': signal_efficiency,
            'n_signal_total': signal_mask.sum(),
            'n_signal_accepted': signal_decisions.sum()
        }
    
    def compute_purity(self, true_labels: np.ndarray,
                      decisions: np.ndarray,
                      signal_class: int = 1) -> Dict:
        """
        Calcule la puret√© (fraction de signal dans √©v√©nements accept√©s)
        """
        accepted_mask = decisions == 1
        accepted_labels = true_labels[accepted_mask]
        
        if len(accepted_labels) == 0:
            return {'purity': 0.0, 'n_accepted': 0, 'n_signal_accepted': 0}
        
        n_signal_accepted = (accepted_labels == signal_class).sum()
        purity = n_signal_accepted / len(accepted_labels)
        
        return {
            'purity': purity,
            'n_accepted': len(accepted_labels),
            'n_signal_accepted': n_signal_accepted
        }
    
    def compute_rate(self, decisions: np.ndarray, 
                    input_rate_hz: float) -> Dict:
        """
        Calcule le taux de d√©clenchement
        """
        acceptance_rate = decisions.mean()
        output_rate_hz = input_rate_hz * acceptance_rate
        
        return {
            'acceptance_rate': acceptance_rate,
            'input_rate_hz': input_rate_hz,
            'output_rate_hz': output_rate_hz,
            'reduction_factor': 1.0 / acceptance_rate if acceptance_rate > 0 else float('inf')
        }
    
    def compute_roc_curve(self, scores: np.ndarray,
                         true_labels: np.ndarray,
                         signal_class: int = 1,
                         n_points: int = 100) -> Tuple[np.ndarray, np.ndarray]:
        """
        Calcule la courbe ROC (efficacit√© signal vs efficacit√© background)
        """
        signal_mask = true_labels == signal_class
        background_mask = ~signal_mask
        
        signal_scores = scores[signal_mask]
        background_scores = scores[background_mask]
        
        thresholds = np.linspace(scores.min(), scores.max(), n_points)
        
        signal_efficiencies = []
        background_efficiencies = []
        
        for threshold in thresholds:
            signal_passed = (signal_scores > threshold).mean()
            background_passed = (background_scores > threshold).mean()
            
            signal_efficiencies.append(signal_passed)
            background_efficiencies.append(background_passed)
        
        return np.array(signal_efficiencies), np.array(background_efficiencies), thresholds
    
    def display_metrics(self):
        """Affiche les d√©finitions de m√©triques"""
        print("\n" + "="*70)
        print("M√©triques de Performance Trigger")
        print("="*70)
        
        for metric, info in self.metrics_definitions.items():
            print(f"\n{metric.upper()}:")
            print(f"  Description: {info['description']}")
            if 'l1_target' in info:
                print(f"  L1 Target: {info['l1_target_us']} Œºs" if 'us' in info else 
                      f"  L1 Target: {info['l1_target_hz']/1e6:.1f} MHz" if 'hz' in info else
                      f"  L1 Target: {info['l1_target_khz']} kHz")
            if 'hlt_target' in info:
                print(f"  HLT Target: {info['hlt_target_ms']} ms" if 'ms' in info else
                      f"  HLT Target: {info['hlt_target_hz']/1e3:.1f} kHz")
            if 'target' in info:
                print(f"  Target: {info['target']}")
            print(f"  Measurement: {info['measurement']}")

metrics = TriggerPerformanceMetrics()
metrics.display_metrics()
```

---

## Mesure de Latence

### Benchmarks et Profiling

```python
class LatencyMeasurement:
    """
    Mesure et analyse de latence
    """
    
    def __init__(self):
        pass
    
    def measure_l1_latency(self, model, input_data, n_iterations=1000):
        """
        Mesure latence L1 (nanosecondes)
        """
        # Warm-up
        for _ in range(10):
            _ = model(input_data)
        
        # Mesures
        latencies = []
        for _ in range(n_iterations):
            start = time.perf_counter_ns()
            _ = model(input_data)
            end = time.perf_counter_ns()
            latencies.append(end - start)
        
        latencies = np.array(latencies)
        
        return {
            'mean_ns': latencies.mean(),
            'median_ns': np.median(latencies),
            'min_ns': latencies.min(),
            'max_ns': latencies.max(),
            'std_ns': latencies.std(),
            'p99_ns': np.percentile(latencies, 99),
            'p99_9_ns': np.percentile(latencies, 99.9)
        }
    
    def measure_hlt_latency(self, processing_pipeline, event_data, n_iterations=100):
        """
        Mesure latence HLT (millisecondes)
        """
        latencies = []
        
        for _ in range(n_iterations):
            start = time.perf_counter()
            result = processing_pipeline(event_data)
            end = time.perf_counter()
            latencies.append((end - start) * 1000)  # Convert to ms
        
        latencies = np.array(latencies)
        
        return {
            'mean_ms': latencies.mean(),
            'median_ms': np.median(latencies),
            'min_ms': latencies.min(),
            'max_ms': latencies.max(),
            'std_ms': latencies.std(),
            'p99_ms': np.percentile(latencies, 99)
        }
    
    def profile_pipeline_stages(self, pipeline, event_data):
        """
        Profile chaque stage du pipeline
        """
        stage_times = {}
        
        # En pratique: utiliser profiling tools (cProfile, py-spy, etc.)
        # Ici: simulation
        
        stages = ['event_building', 'track_reco', 'calo_reco', 
                 'ml_inference', 'decision']
        
        for stage in stages:
            # Mesurer temps stage
            start = time.perf_counter()
            # pipeline.execute_stage(stage, event_data)
            end = time.perf_counter()
            stage_times[stage] = (end - start) * 1000  # ms
        
        return stage_times
    
    def analyze_latency_bottlenecks(self, stage_times: Dict):
        """Identifie les bottlenecks de latence"""
        total_time = sum(stage_times.values())
        
        bottlenecks = []
        for stage, time_ms in stage_times.items():
            fraction = time_ms / total_time
            if fraction > 0.2:  # Plus de 20% du temps
                bottlenecks.append({
                    'stage': stage,
                    'time_ms': time_ms,
                    'fraction': fraction
                })
        
        return sorted(bottlenecks, key=lambda x: x['time_ms'], reverse=True)

latency_meas = LatencyMeasurement()
```

---

## Optimisation de Performance

### Techniques d'Optimisation

```python
class PerformanceOptimization:
    """
    Techniques d'optimisation de performance
    """
    
    @staticmethod
    def optimize_l1_latency(model, target_latency_ns=100):
        """
        Optimise latence L1 pour respecter budget
        """
        optimizations = {
            'quantization': 'R√©duire pr√©cision (8-bit ‚Üí 4-bit)',
            'pruning': 'R√©duire nombre de param√®tres',
            'architecture_reduction': 'R√©duire taille couches',
            'pipeline_optimization': 'Am√©liorer pipeline FPGA',
            'parallelization': 'Parall√©liser calculs'
        }
        
        return optimizations
    
    @staticmethod
    def optimize_hlt_throughput(pipeline, target_throughput_hz=100e3):
        """
        Optimise throughput HLT
        """
        strategies = {
            'batch_processing': 'Traiter √©v√©nements par batch',
            'parallel_execution': 'Ex√©cuter chemins en parall√®le',
            'early_stopping': 'Rejeter √©v√©nements t√¥t',
            'model_caching': 'Cache mod√®les en m√©moire',
            'async_processing': 'Traitement asynchrone',
            'load_balancing': '√âquilibrer charge entre n≈ìuds'
        }
        
        return strategies
    
    @staticmethod
    def optimize_efficiency_purity_balance(model, val_data, val_labels,
                                          target_rate_hz: float):
        """
        Optimise compromis efficacit√©/puret√© pour taux cible
        """
        # Chercher seuil optimal
        with torch.no_grad():
            scores = model(torch.tensor(val_data, dtype=torch.float32))
            if scores.dim() > 1:
                scores = scores[:, 1]  # Probabilit√© signal
            scores = scores.numpy()
        
        # Chercher seuil qui donne taux cible
        input_rate = len(val_data)  # Approximation
        target_acceptance = target_rate_hz / input_rate
        
        sorted_scores = np.sort(scores)[::-1]
        threshold_idx = int(target_acceptance * len(scores))
        threshold = sorted_scores[threshold_idx] if threshold_idx < len(scores) else sorted_scores[-1]
        
        # Calculer m√©triques avec ce seuil
        decisions = scores > threshold
        
        metrics = TriggerPerformanceMetrics()
        efficiency = metrics.compute_efficiency(val_labels, decisions)
        purity = metrics.compute_purity(val_labels, decisions)
        rate = metrics.compute_rate(decisions, input_rate)
        
        return {
            'threshold': threshold,
            'efficiency': efficiency['signal_efficiency'],
            'purity': purity['purity'],
            'rate_hz': rate['output_rate_hz']
        }
    
    @staticmethod
    def optimize_trigger_menu(menu, target_total_rate_hz: float):
        """
        Optimise menu de trigger pour taux total cible
        """
        # Strat√©gie: ajuster seuils et prescales
        
        optimization = {
            'strategy': 'Adjust thresholds and prescales',
            'current_rate_hz': 0,
            'target_rate_hz': target_total_rate_hz,
            'adjustments': {}
        }
        
        return optimization

opt = PerformanceOptimization()
```

---

## Benchmarks et Validation

### Tests de Performance

```python
class TriggerBenchmarking:
    """
    Syst√®me de benchmark pour triggers
    """
    
    def __init__(self):
        self.benchmark_datasets = {
            'signal': {
                'description': '√âv√©nements signal (ex: Higgs)',
                'size': 10000,
                'distribution': 'Simulated signal events'
            },
            'background': {
                'description': '√âv√©nements background (QCD, etc.)',
                'size': 1000000,
                'distribution': 'Simulated background events'
            }
        }
    
    def run_full_benchmark(self, trigger_system, test_events: Dict):
        """
        Ex√©cute benchmark complet
        """
        results = {
            'latency': {},
            'throughput': {},
            'efficiency': {},
            'purity': {},
            'rate': {}
        }
        
        # Latence
        latency_meas = LatencyMeasurement()
        results['latency'] = latency_meas.measure_hlt_latency(
            trigger_system, test_events['signal'], n_iterations=100
        )
        
        # Throughput
        start = time.perf_counter()
        n_processed = 0
        duration_s = 10  # 10 secondes de test
        
        end_time = start + duration_s
        while time.perf_counter() < end_time:
            trigger_system(test_events['signal'][n_processed % len(test_events['signal'])])
            n_processed += 1
        
        elapsed = time.perf_counter() - start
        results['throughput'] = {
            'events_per_sec': n_processed / elapsed,
            'duration_s': elapsed
        }
        
        # Efficacit√© et puret√©
        all_events = np.concatenate([test_events['signal'], test_events['background']])
        all_labels = np.concatenate([
            np.ones(len(test_events['signal'])),
            np.zeros(len(test_events['background']))
        ])
        
        decisions = np.array([trigger_system(e) for e in all_events])
        
        metrics = TriggerPerformanceMetrics()
        results['efficiency'] = metrics.compute_efficiency(all_labels, decisions)
        results['purity'] = metrics.compute_purity(all_labels, decisions)
        results['rate'] = metrics.compute_rate(decisions, len(all_events))
        
        return results
    
    def compare_trigger_versions(self, version1, version2, test_events):
        """
        Compare deux versions de trigger
        """
        results_v1 = self.run_full_benchmark(version1, test_events)
        results_v2 = self.run_full_benchmark(version2, test_events)
        
        comparison = {
            'latency_improvement': (results_v1['latency']['mean_ms'] - 
                                  results_v2['latency']['mean_ms']) / results_v1['latency']['mean_ms'],
            'efficiency_change': results_v2['efficiency']['signal_efficiency'] - results_v1['efficiency']['signal_efficiency'],
            'purity_change': results_v2['purity']['purity'] - results_v1['purity']['purity'],
            'throughput_improvement': (results_v2['throughput']['events_per_sec'] - 
                                     results_v1['throughput']['events_per_sec']) / results_v1['throughput']['events_per_sec']
        }
        
        return comparison

benchmark = TriggerBenchmarking()
```

---

## Monitoring en Production

### Surveillance Continue

```python
class ProductionMonitoring:
    """
    Monitoring de performance en production
    """
    
    def __init__(self):
        self.metrics_history = {
            'latency': [],
            'throughput': [],
            'efficiency': [],
            'rate': []
        }
    
    def monitor_live_performance(self, trigger_system, 
                                event_stream, 
                                sampling_rate: float = 0.01):
        """
        Surveille performance en temps r√©el
        
        Args:
            sampling_rate: Fraction d'√©v√©nements √† monitorer (pour r√©duire overhead)
        """
        monitored_events = []
        monitored_latencies = []
        
        for i, event in enumerate(event_stream):
            if np.random.random() < sampling_rate:
                start = time.perf_counter()
                decision = trigger_system(event)
                end = time.perf_counter()
                
                monitored_events.append(decision)
                monitored_latencies.append((end - start) * 1000)
        
        return {
            'avg_latency_ms': np.mean(monitored_latencies),
            'throughput_estimate': len(monitored_events) / sampling_rate,
            'acceptance_rate': np.mean(monitored_events)
        }
    
    def detect_performance_degradation(self, current_metrics: Dict,
                                      baseline_metrics: Dict,
                                      thresholds: Dict):
        """
        D√©tecte d√©gradation de performance
        """
        alerts = []
        
        if current_metrics['latency'] > baseline_metrics['latency'] * thresholds.get('latency_multiplier', 1.5):
            alerts.append('Latency degraded')
        
        if current_metrics['efficiency'] < baseline_metrics['efficiency'] * thresholds.get('efficiency_threshold', 0.95):
            alerts.append('Efficiency degraded')
        
        if current_metrics['rate'] > baseline_metrics['rate'] * thresholds.get('rate_multiplier', 1.2):
            alerts.append('Rate too high')
        
        return alerts

monitoring = ProductionMonitoring()
```

---

## Exercices

### Exercice 18.5.1
Mesurez la latence d'un mod√®le L1 et optimisez-le pour respecter un budget de 80 ns.

### Exercice 18.5.2
Cr√©ez un syst√®me de benchmark qui compare l'efficacit√© et la puret√© de deux menus de trigger diff√©rents.

### Exercice 18.5.3
D√©veloppez un syst√®me de monitoring qui d√©tecte automatiquement les d√©gradations de performance en production.

### Exercice 18.5.4
Optimisez un menu de trigger pour maximiser l'efficacit√© signal tout en respectant un budget de taux strict.

---

## Points Cl√©s √† Retenir

> üìå **La latence L1 doit √™tre < 4 Œºs, HLT < 300 ms**

> üìå **Le throughput L1 est de 40 MHz, HLT de 100 kHz**

> üìå **L'efficacit√© signal et la puret√© doivent √™tre optimis√©es simultan√©ment**

> üìå **Le taux de d√©clenchement doit respecter les budgets de bande passante**

> üìå **Le profiling identifie les bottlenecks pour optimisation cibl√©e**

> üìå **Le monitoring en production est essentiel pour d√©tecter les probl√®mes**

---

*Section pr√©c√©dente : [18.4 Int√©gration de l'IA](./18_04_IA_Trigger.md)*

