# 20.3 M√©thodes Non Supervis√©es

---

## Introduction

Les **m√©thodes non supervis√©es** sont essentielles pour la d√©tection d'anomalies en physique des hautes √©nergies, car elles ne n√©cessitent pas de labels de signal (qui sont par d√©finition inconnus pour la nouvelle physique). Ces m√©thodes apprennent directement depuis les donn√©es pour identifier des patterns et outliers.

Cette section pr√©sente diverses m√©thodes non supervis√©es utilis√©es pour la d√©tection d'anomalies, incluant les m√©thodes bas√©es sur densit√©, clustering, et isolation.

---

## Types de M√©thodes Non Supervis√©es

### Classification

```python
import numpy as np
import torch
import torch.nn as nn
from typing import List, Dict, Tuple
from sklearn.ensemble import IsolationForest
from sklearn.cluster import DBSCAN
from sklearn.neighbors import LocalOutlierFactor

class UnsupervisedMethods:
    """
    Vue d'ensemble des m√©thodes non supervis√©es
    """
    
    def __init__(self):
        self.methods = {
            'density_based': {
                'examples': ['DBSCAN', 'LOF (Local Outlier Factor)'],
                'principle': 'Anomalies = points dans r√©gions de faible densit√©',
                'advantages': ['D√©tecte clusters de formes arbitraires'],
                'disadvantages': ['Sensible √† param√®tres', 'Co√ªt computationnel']
            },
            'isolation_based': {
                'examples': ['Isolation Forest', 'Extended Isolation Forest'],
                'principle': 'Anomalies = faciles √† isoler',
                'advantages': ['Rapide', 'Bien pour haute dimension'],
                'disadvantages': ['Moins pr√©cis pour clusters d\'anomalies']
            },
            'distance_based': {
                'examples': ['k-NN distance', 'Average k-NN distance'],
                'principle': 'Anomalies = points loin de leurs voisins',
                'advantages': ['Simple', 'Intuitif'],
                'disadvantages': ['Sensible curse of dimensionality']
            },
            'clustering_based': {
                'examples': ['k-means outliers', 'Hierarchical clustering'],
                'principle': 'Anomalies = points loin des clusters',
                'advantages': ['Interpr√©table'],
                'disadvantages': ['N√©cessite nombre clusters']
            },
            'neural_network_based': {
                'examples': ['Autoencoders', 'VAE', 'GAN discriminators'],
                'principle': 'Anomalies = mal reconstruites/g√©n√©r√©es',
                'advantages': ['Capte patterns complexes'],
                'disadvantages': ['Requiert entra√Ænement', 'Black box']
            }
        }
    
    def display_methods(self):
        """Affiche les m√©thodes"""
        print("\n" + "="*70)
        print("M√©thodes Non Supervis√©es pour D√©tection d'Anomalies")
        print("="*70)
        
        for method_type, info in self.methods.items():
            print(f"\n{method_type.replace('_', ' ').title()}:")
            print(f"  Exemples: {', '.join(info['examples'])}")
            print(f"  Principe: {info['principle']}")
            print(f"  Avantages:")
            for adv in info['advantages']:
                print(f"    + {adv}")
            print(f"  Inconv√©nients:")
            for disadv in info['disadvantages']:
                print(f"    - {disadv}")

unsupervised = UnsupervisedMethods()
unsupervised.display_methods()
```

---

## Isolation Forest

### Principe et Impl√©mentation

```python
class IsolationForestAnomalyDetection:
    """
    Isolation Forest pour d√©tection d'anomalies
    
    Principe: Anomalies sont faciles √† isoler (peu de splits n√©cessaires)
    """
    
    def __init__(self, n_estimators=100, max_samples='auto', contamination=0.1):
        """
        Args:
            n_estimators: Nombre d'arbres
            max_samples: Nombre √©chantillons par arbre
            contamination: Fraction attendue d'anomalies
        """
        self.model = IsolationForest(
            n_estimators=n_estimators,
            max_samples=max_samples,
            contamination=contamination,
            random_state=42
        )
    
    def fit(self, X):
        """Entra√Æne sur donn√©es background"""
        self.model.fit(X)
        return self
    
    def predict_anomalies(self, X):
        """
        Pr√©dit anomalies
        
        Returns:
            predictions: 1 = normal, -1 = anomalie
            scores: Score d'anomalie (plus n√©gatif = plus anormal)
        """
        predictions = self.model.predict(X)
        scores = self.model.score_samples(X)
        
        return {
            'predictions': predictions,
            'scores': scores,
            'anomaly_indices': np.where(predictions == -1)[0]
        }
    
    def compute_anomaly_scores(self, X):
        """Retourne seulement les scores"""
        return self.model.score_samples(X)

class ExtendedIsolationForest:
    """
    Extended Isolation Forest
    
    Am√©lioration qui utilise hyperplanes de dimension quelconque
    """
    
    def __init__(self, n_estimators=100, extension_level=1):
        """
        Args:
            extension_level: Niveau d'extension (dimension hyperplanes)
        """
        # En pratique: utiliser biblioth√®que sp√©cialis√©e
        # Ici: simulation avec Isolation Forest standard
        self.base_model = IsolationForest(n_estimators=n_estimators)
        self.extension_level = extension_level
    
    def fit(self, X):
        """Entra√Æne mod√®le"""
        self.base_model.fit(X)
        return self
    
    def predict(self, X):
        """Pr√©dit anomalies"""
        return self.base_model.predict(X)

# Test Isolation Forest
iso_forest = IsolationForestAnomalyDetection(n_estimators=100, contamination=0.05)

# Simuler donn√©es
background_data = np.random.randn(10000, 10)
anomaly_data = np.random.randn(100, 10) * 2 + 5
all_data = np.vstack([background_data, anomaly_data])

iso_forest.fit(background_data)
results = iso_forest.predict_anomalies(all_data)

print(f"\nIsolation Forest:")
print(f"  Anomalies d√©tect√©es: {len(results['anomaly_indices'])}")
print(f"  Score moyen background: {results['scores'][:10000].mean():.4f}")
print(f"  Score moyen anomalies: {results['scores'][10000:].mean():.4f}")
```

---

## Local Outlier Factor (LOF)

### D√©tection Bas√©e sur Densit√© Locale

```python
class LocalOutlierFactorDetection:
    """
    Local Outlier Factor pour d√©tection d'anomalies
    
    Compare densit√© locale d'un point avec densit√© de ses voisins
    """
    
    def __init__(self, n_neighbors=20, contamination=0.1):
        """
        Args:
            n_neighbors: Nombre de voisins √† consid√©rer
            contamination: Fraction attendue d'anomalies
        """
        self.model = LocalOutlierFactor(
            n_neighbors=n_neighbors,
            contamination=contamination,
            novelty=False
        )
        self.n_neighbors = n_neighbors
    
    def fit(self, X):
        """Entra√Æne sur donn√©es background"""
        self.model.fit(X)
        return self
    
    def predict(self, X):
        """
        Pr√©dit anomalies
        
        Note: LOF n√©cessite refit pour nouvelles donn√©es en mode novelty=False
        """
        predictions = self.model.fit_predict(X)
        scores = -self.model.negative_outlier_factor_  # Convert to positive (higher = more anomalous)
        
        return {
            'predictions': predictions,
            'scores': scores,
            'anomaly_indices': np.where(predictions == -1)[0]
        }

lof = LocalOutlierFactorDetection(n_neighbors=20, contamination=0.05)
lof_results = lof.predict(all_data)

print(f"\nLocal Outlier Factor:")
print(f"  Anomalies d√©tect√©es: {len(lof_results['anomaly_indices'])}")
print(f"  Score moyen background: {lof_results['scores'][:10000].mean():.4f}")
```

---

## DBSCAN pour D√©tection d'Anomalies

### Clustering avec Points de Bruit

```python
class DBSCANAnomalyDetection:
    """
    DBSCAN: points non assign√©s √† clusters = anomalies
    """
    
    def __init__(self, eps=0.5, min_samples=5):
        """
        Args:
            eps: Distance maximale entre voisins
            min_samples: Nombre minimal de points pour former cluster
        """
        self.model = DBSCAN(eps=eps, min_samples=min_samples)
        self.eps = eps
        self.min_samples = min_samples
    
    def fit_predict(self, X):
        """
        Clustering: -1 = bruit (anomalies)
        """
        labels = self.model.fit_predict(X)
        
        # Anomalies = labels = -1
        anomaly_indices = np.where(labels == -1)[0]
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        
        return {
            'labels': labels,
            'anomaly_indices': anomaly_indices,
            'n_anomalies': len(anomaly_indices),
            'n_clusters': n_clusters
        }

dbscan = DBSCANAnomalyDetection(eps=0.3, min_samples=10)
dbscan_results = dbscan.fit_predict(all_data[:5000])  # DBSCAN peut √™tre lent

print(f"\nDBSCAN:")
print(f"  Clusters trouv√©s: {dbscan_results['n_clusters']}")
print(f"  Anomalies (bruit): {dbscan_results['n_anomalies']}")
```

---

## M√©thodes Bas√©es sur Distance

### k-NN Distance

```python
class KNNAnomalyDetection:
    """
    D√©tection d'anomalies bas√©e sur distance k-NN
    """
    
    def __init__(self, k=5):
        """
        Args:
            k: Nombre de voisins
        """
        self.k = k
    
    def compute_knn_distances(self, X_train, X_test):
        """
        Calcule distances aux k plus proches voisins
        
        Anomalies = grandes distances aux k-NN
        """
        from sklearn.neighbors import NearestNeighbors
        
        nn = NearestNeighbors(n_neighbors=self.k + 1)  # +1 car inclut point lui-m√™me
        nn.fit(X_train)
        
        distances, indices = nn.kneighbors(X_test)
        
        # Prendre distances aux k voisins (exclure point lui-m√™me)
        knn_distances = distances[:, 1:]  # Exclure distance √† soi (indice 0)
        avg_knn_distance = knn_distances.mean(axis=1)
        
        return {
            'knn_distances': knn_distances,
            'avg_knn_distance': avg_knn_distance,
            'max_knn_distance': knn_distances.max(axis=1)
        }
    
    def detect_anomalies(self, X_train, X_test, threshold_percentile=95):
        """
        D√©tecte anomalies avec seuil sur distance k-NN
        """
        knn_results = self.compute_knn_distances(X_train, X_test)
        
        # Seuil depuis donn√©es d'entra√Ænement
        train_distances = self.compute_knn_distances(X_train, X_train)
        threshold = np.percentile(train_distances['avg_knn_distance'], threshold_percentile)
        
        # D√©tecter anomalies
        anomaly_mask = knn_results['avg_knn_distance'] > threshold
        anomaly_indices = np.where(anomaly_mask)[0]
        
        return {
            'anomaly_indices': anomaly_indices,
            'scores': knn_results['avg_knn_distance'],
            'threshold': threshold
        }

knn_detector = KNNAnomalyDetection(k=5)
knn_results = knn_detector.detect_anomalies(background_data[:5000], all_data, threshold_percentile=95)

print(f"\nk-NN Anomaly Detection:")
print(f"  Seuil: {knn_results['threshold']:.4f}")
print(f"  Anomalies d√©tect√©es: {len(knn_results['anomaly_indices'])}")
```

---

## One-Class SVM

### Support Vector Machine pour D√©tection

```python
class OneClassSVMDetection:
    """
    One-Class SVM pour d√©tection d'anomalies
    
    Apprend fronti√®re autour des donn√©es normales
    """
    
    def __init__(self, nu=0.1, kernel='rbf', gamma='scale'):
        """
        Args:
            nu: Limite sup√©rieure fraction d'outliers
            kernel: Type de kernel ('rbf', 'linear', 'poly')
            gamma: Param√®tre kernel RBF
        """
        from sklearn.svm import OneClassSVM
        
        self.model = OneClassSVM(nu=nu, kernel=kernel, gamma=gamma)
        self.nu = nu
    
    def fit(self, X):
        """Entra√Æne sur donn√©es background"""
        self.model.fit(X)
        return self
    
    def predict(self, X):
        """
        Pr√©dit: 1 = normal, -1 = anomalie
        """
        predictions = self.model.predict(X)
        scores = self.model.score_samples(X)
        
        return {
            'predictions': predictions,
            'scores': scores,
            'anomaly_indices': np.where(predictions == -1)[0]
        }

oc_svm = OneClassSVMDetection(nu=0.05, kernel='rbf')
oc_svm.fit(background_data[:5000])
svm_results = oc_svm.predict(all_data)

print(f"\nOne-Class SVM:")
print(f"  Anomalies d√©tect√©es: {len(svm_results['anomaly_indices'])}")
print(f"  Score moyen background: {svm_results['scores'][:10000].mean():.4f}")
```

---

## Comparaison des M√©thodes

### Benchmark

```python
class UnsupervisedMethodComparison:
    """
    Compare diff√©rentes m√©thodes non supervis√©es
    """
    
    def compare_methods(self, X_train, X_test, true_labels=None):
        """
        Compare performances de diff√©rentes m√©thodes
        """
        results = {}
        
        # Isolation Forest
        iso_forest = IsolationForestAnomalyDetection(contamination=0.05)
        iso_forest.fit(X_train)
        iso_results = iso_forest.predict_anomalies(X_test)
        results['Isolation Forest'] = {
            'n_anomalies': len(iso_results['anomaly_indices']),
            'scores': iso_results['scores']
        }
        
        # LOF
        lof = LocalOutlierFactorDetection(contamination=0.05)
        lof_results = lof.predict(X_test)  # Note: fit inclus
        results['LOF'] = {
            'n_anomalies': len(lof_results['anomaly_indices']),
            'scores': lof_results['scores']
        }
        
        # k-NN
        knn = KNNAnomalyDetection(k=5)
        knn_results = knn.detect_anomalies(X_train, X_test)
        results['k-NN'] = {
            'n_anomalies': len(knn_results['anomaly_indices']),
            'scores': knn_results['scores']
        }
        
        # One-Class SVM
        oc_svm = OneClassSVMDetection(nu=0.05)
        oc_svm.fit(X_train)
        svm_results = oc_svm.predict(X_test)
        results['One-Class SVM'] = {
            'n_anomalies': len(svm_results['anomaly_indices']),
            'scores': svm_results['scores']
        }
        
        # √âvaluer si true_labels disponibles
        if true_labels is not None:
            for method_name, result in results.items():
                predictions = np.zeros(len(X_test))
                predictions[result['anomaly_indices']] = 1
                
                # M√©triques (si binaire)
                from sklearn.metrics import precision_recall_fscore_support
                precision, recall, f1, _ = precision_recall_fscore_support(
                    true_labels, predictions, average='binary', zero_division=0
                )
                
                result['precision'] = precision
                result['recall'] = recall
                result['f1'] = f1
        
        return results
    
    def display_comparison(self, results):
        """Affiche comparaison"""
        print("\n" + "="*70)
        print("Comparaison des M√©thodes Non Supervis√©es")
        print("="*70)
        
        print(f"\n{'M√©thode':<20} {'Anomalies':<15} {'Precision':<12} {'Recall':<12} {'F1':<12}")
        print("-" * 70)
        
        for method, result in results.items():
            anomalies = result['n_anomalies']
            precision = result.get('precision', 0)
            recall = result.get('recall', 0)
            f1 = result.get('f1', 0)
            
            print(f"{method:<20} {anomalies:<15} {precision:<11.3f} {recall:<11.3f} {f1:<11.3f}")

# Comparer m√©thodes
comparison = UnsupervisedMethodComparison()

# Simuler labels (derniers 100 = anomalies)
true_labels = np.zeros(len(all_data))
true_labels[-100:] = 1

comp_results = comparison.compare_methods(
    background_data[:5000], all_data, true_labels=true_labels
)
comparison.display_comparison(comp_results)
```

---

## Exercices

### Exercice 20.3.1
Comparez Isolation Forest, LOF, et k-NN sur un dataset simul√© avec diff√©rentes distributions d'anomalies.

### Exercice 20.3.2
Analysez l'impact des hyperparam√®tres (n_neighbors, eps, contamination) sur les performances.

### Exercice 20.3.3
Impl√©mentez une m√©thode de d√©tection d'anomalies bas√©e sur clustering hi√©rarchique.

### Exercice 20.3.4
D√©veloppez un syst√®me qui combine plusieurs m√©thodes non supervis√©es avec voting ou stacking.

---

## Points Cl√©s √† Retenir

> üìå **Les m√©thodes non supervis√©es ne n√©cessitent pas labels de signal**

> üìå **Isolation Forest est rapide et efficace pour haute dimension**

> üìå **LOF d√©tecte anomalies locales en comparant densit√©s**

> üìå **DBSCAN identifie anomalies comme points de bruit (non-clustered)**

> üìå **k-NN distance est simple mais sensible √† curse of dimensionality**

> üìå **La combinaison de m√©thodes peut am√©liorer robustesse**

---

*Section pr√©c√©dente : [20.2 Autoencoders](./20_02_Autoencoders.md) | Section suivante : [20.4 R√©seaux de Tenseurs](./20_04_Tenseurs.md)*

