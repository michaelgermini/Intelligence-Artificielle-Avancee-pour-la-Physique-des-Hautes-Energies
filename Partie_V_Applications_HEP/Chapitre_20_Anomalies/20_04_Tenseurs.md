# 20.4 RÃ©seaux de Tenseurs pour la DÃ©tection d'Anomalies

---

## Introduction

Les **rÃ©seaux de tenseurs** offrent une approche unique pour la dÃ©tection d'anomalies en exploitant leur structure compacte et leur capacitÃ© Ã  capturer des corrÃ©lations complexes entre variables. Leur efficacitÃ© computationnelle les rend particuliÃ¨rement attractifs pour les applications temps rÃ©el comme les triggers.

Cette section prÃ©sente l'utilisation des rÃ©seaux de tenseurs (MPS, Tensor Train) pour la dÃ©tection d'anomalies, incluant les autoencoders basÃ©s sur tenseurs et les mÃ©thodes spÃ©cifiques aux structures tensorielles.

---

## Avantages des RÃ©seaux de Tenseurs

### Pourquoi les Tenseurs pour Anomalies ?

```python
import numpy as np
import torch
import torch.nn as nn
from typing import Dict, List, Tuple

class TensorNetworkAdvantages:
    """
    Avantages des rÃ©seaux de tenseurs pour dÃ©tection d'anomalies
    """
    
    def __init__(self):
        self.advantages = {
            'compression': {
                'description': 'ReprÃ©sentation compacte',
                'benefit': 'Moins de paramÃ¨tres = moins de risque overfitting',
                'impact': 'Meilleure gÃ©nÃ©ralisation'
            },
            'interpretability': {
                'description': 'Structure explicite',
                'benefit': 'Bond dimensions rÃ©vÃ¨lent complexitÃ© nÃ©cessaire',
                'impact': 'ComprÃ©hension des corrÃ©lations importantes'
            },
            'efficiency': {
                'description': 'EfficacitÃ© computationnelle',
                'benefit': 'Contractions rapides, dÃ©ployable sur FPGA',
                'impact': 'Utilisable dans triggers temps rÃ©el'
            },
            'correlations': {
                'description': 'Capte corrÃ©lations complexes',
                'benefit': 'Structure tensorielle encode dÃ©pendances multi-variables',
                'impact': 'DÃ©tecte patterns subtils d\'anomalies'
            }
        }
    
    def display_advantages(self):
        """Affiche les avantages"""
        print("\n" + "="*70)
        print("Avantages RÃ©seaux de Tenseurs pour Anomalies")
        print("="*70)
        
        for advantage, info in self.advantages.items():
            print(f"\n{advantage.replace('_', ' ').title()}:")
            print(f"  Description: {info['description']}")
            print(f"  BÃ©nÃ©fice: {info['benefit']}")
            print(f"  Impact: {info['impact']}")

advantages = TensorNetworkAdvantages()
advantages.display_advantages()
```

---

## Tensor Train Autoencoder

### Architecture TT pour Autoencoder

```python
class TensorTrainAutoencoder(nn.Module):
    """
    Autoencodeur basÃ© sur Tensor Train (TT)
    
    Utilise dÃ©composition TT pour encodeur et dÃ©codeur
    """
    
    def __init__(self, input_dims=[4, 4, 4, 4], bond_dims=[2, 3, 2], latent_dim=8):
        """
        Args:
            input_dims: Dimensions de chaque mode de l'input tensorisÃ©
            bond_dims: Bond dimensions pour TT
            latent_dim: Dimension espace latent (flatten)
        """
        super().__init__()
        
        self.input_dims = input_dims
        self.bond_dims = bond_dims
        self.latent_dim = latent_dim
        self.input_size = np.prod(input_dims)
        
        # Encodeur: TT decomposition
        # Input tensorisÃ© â†’ compression TT â†’ latent
        self.tt_encoder = self._create_tt_layers(input_dims, bond_dims, latent_dim)
        
        # DÃ©codeur: reconstruction depuis latent
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 128),
            nn.ReLU(),
            nn.Linear(128, self.input_size)
        )
    
    def _create_tt_layers(self, input_dims, bond_dims, latent_dim):
        """
        CrÃ©e couches TT pour encodeur
        
        SimplifiÃ©: utilise couches linÃ©aires qui simulent compression TT
        """
        # En pratique: utiliser vraie dÃ©composition TT
        # Ici: approximation avec MLP
        input_size = np.prod(input_dims)
        
        layers = nn.Sequential(
            nn.Linear(input_size, bond_dims[0] * input_dims[0]),
            nn.ReLU(),
            nn.Linear(bond_dims[0] * input_dims[0], bond_dims[1] * input_dims[1]),
            nn.ReLU(),
            nn.Linear(bond_dims[1] * input_dims[1], latent_dim)
        )
        
        return layers
    
    def forward(self, x):
        """Forward pass"""
        # Encoder avec TT
        encoded = self.tt_encoder(x.view(x.size(0), -1))
        
        # Decoder
        decoded = self.decoder(encoded)
        decoded = decoded.view(x.size(0), *self.input_dims)
        
        return decoded
    
    def compute_anomaly_score(self, x):
        """Score d'anomalie"""
        with torch.no_grad():
            reconstructed = self.forward(x)
            
            # Erreur de reconstruction
            error = torch.mean((x - reconstructed)**2, dim=tuple(range(1, len(x.shape))))
            
            return error

# CrÃ©er TT Autoencoder
tt_autoencoder = TensorTrainAutoencoder(
    input_dims=[5, 5, 4],
    bond_dims=[3, 2],
    latent_dim=10
)

print(f"\nTensor Train Autoencoder:")
print(f"  Input size: {tt_autoencoder.input_size}")
print(f"  Latent dim: {tt_autoencoder.latent_dim}")
print(f"  ParamÃ¨tres: {sum(p.numel() for p in tt_autoencoder.parameters()):,}")

# Comparer avec autoencodeur standard
standard_ae = nn.Sequential(
    nn.Linear(tt_autoencoder.input_size, 64),
    nn.ReLU(),
    nn.Linear(64, 10),
    nn.ReLU(),
    nn.Linear(10, 64),
    nn.ReLU(),
    nn.Linear(64, tt_autoencoder.input_size)
)

print(f"  Autoencodeur standard: {sum(p.numel() for p in standard_ae.parameters()):,} paramÃ¨tres")
print(f"  Compression: {sum(p.numel() for p in standard_ae.parameters()) / sum(p.numel() for p in tt_autoencoder.parameters()):.2f}Ã—")
```

---

## MPS pour ModÃ©lisation de Distributions

### Matrix Product State pour DensitÃ©

```python
class MPSDensityModel:
    """
    ModÃ¨le de densitÃ© avec MPS (Matrix Product State)
    
    ModÃ©lise distribution jointe des features comme MPS
    """
    
    def __init__(self, n_features=10, bond_dim=4, n_categories_per_feature=10):
        """
        Args:
            n_features: Nombre de features
            bond_dim: Bond dimension du MPS
            n_categories_per_feature: Nombre de valeurs possibles par feature
        """
        super().__init__()
        
        self.n_features = n_features
        self.bond_dim = bond_dim
        self.n_categories = n_categories_per_feature
        
        # CrÃ©er tenseurs MPS
        # En pratique: utiliser vraie structure MPS
        # Ici: approximation
        
        # Tensors pour chaque site
        self.mps_tensors = nn.ModuleList()
        
        for i in range(n_features):
            if i == 0:
                # Premier site: (bond_dim, n_categories)
                tensor = nn.Parameter(torch.randn(bond_dim, n_categories_per_feature))
            elif i == n_features - 1:
                # Dernier site: (n_categories, bond_dim)
                tensor = nn.Parameter(torch.randn(n_categories_per_feature, bond_dim))
            else:
                # Sites intermÃ©diaires: (bond_dim, n_categories, bond_dim)
                tensor = nn.Parameter(torch.randn(bond_dim, n_categories_per_feature, bond_dim))
            
            self.mps_tensors.append(nn.Parameter(tensor))
    
    def compute_log_probability(self, x_discrete):
        """
        Calcule log-probabilitÃ© d'une configuration
        
        Args:
            x_discrete: (batch, n_features) indices discrets
        """
        # Contraction MPS pour calculer probabilitÃ©
        # SimplifiÃ©: approximation linÃ©aire
        
        # En pratique: vraie contraction MPS
        # log p(x) = log(contraction des tenseurs selon indices x)
        
        # Approximation
        log_prob = torch.zeros(x_discrete.shape[0])
        
        for i in range(self.n_features):
            feature_values = x_discrete[:, i]
            # Prendre valeurs correspondantes dans tenseurs
            # (SimplifiÃ©)
            log_prob += torch.randn(x_discrete.shape[0]) * 0.1  # Placeholder
        
        return log_prob
    
    def compute_anomaly_score(self, x_discrete):
        """
        Score d'anomalie = -log probabilitÃ©
        
        Anomalies = faibles probabilitÃ©s
        """
        log_prob = self.compute_log_probability(x_discrete)
        anomaly_score = -log_prob
        
        return anomaly_score

mps_density = MPSDensityModel(n_features=8, bond_dim=4, n_categories_per_feature=10)
```

---

## Tensor Train pour DÃ©tection d'Anomalies Multi-VariÃ©es

### ModÃ©lisation de CorrÃ©lations

```python
class TTTAnomalyDetector:
    """
    DÃ©tecteur d'anomalies basÃ© sur Tensor Train
    
    ModÃ©lise distribution jointe des features
    """
    
    def __init__(self, feature_dims, bond_dims):
        """
        Args:
            feature_dims: Dimensions de chaque feature (aprÃ¨s discrÃ©tisation)
            bond_dims: Bond dimensions pour TT
        """
        self.feature_dims = feature_dims
        self.bond_dims = bond_dims
        self.n_features = len(feature_dims)
        
        # Tensors TT (core tensors)
        # Structure: T[i] a shape (r[i-1], d[i], r[i])
        self.tt_cores = []
        
        # Initialiser cores
        for i in range(self.n_features):
            if i == 0:
                shape = (1, feature_dims[i], bond_dims[i])
            elif i == self.n_features - 1:
                shape = (bond_dims[i-1], feature_dims[i], 1)
            else:
                shape = (bond_dims[i-1], feature_dims[i], bond_dims[i])
            
            core = nn.Parameter(torch.randn(*shape))
            self.tt_cores.append(core)
        
        self.tt_cores = nn.ParameterList(self.tt_cores)
    
    def compute_probability(self, x_indices):
        """
        Calcule probabilitÃ© d'une configuration
        
        Args:
            x_indices: (batch, n_features) indices pour chaque feature
        """
        # Contraction TT
        # Pour chaque configuration, contracter cores selon indices
        
        batch_size = x_indices.shape[0]
        probs = torch.ones(batch_size)
        
        # Contraction simplifiÃ©e (approximation)
        for i in range(self.n_features):
            feature_idx = x_indices[:, i]
            # Extraire slices correspondantes des cores
            # (SimplifiÃ© ici)
            probs = probs * torch.randn(batch_size).abs() * 0.1  # Placeholder
        
        return probs
    
    def compute_anomaly_score(self, x_indices):
        """
        Score d'anomalie depuis probabilitÃ© TT
        """
        probs = self.compute_probability(x_indices)
        anomaly_score = -torch.log(probs + 1e-10)  # -log prob
        
        return anomaly_score

tt_detector = TTTAnomalyDetector(
    feature_dims=[5, 5, 5, 5],
    bond_dims=[3, 3, 3]
)

print(f"\nTensor Train Anomaly Detector:")
print(f"  Features: {tt_detector.n_features}")
print(f"  Bond dims: {tt_detector.bond_dims}")
```

---

## Autoencoder Tensor Train pour Ã‰vÃ©nements HEP

### Application SpÃ©cifique

```python
class HEPTensorAutoencoder(nn.Module):
    """
    Autoencodeur Tensor Train pour Ã©vÃ©nements HEP
    
    Tensorise features selon structure physique
    """
    
    def __init__(self, n_jets=4, n_leptons=2, jet_features=8, lepton_features=4):
        """
        Tensorise selon: [jets, leptons, MET]
        
        Structure: Tensor avec modes = [jet_1, jet_2, ..., lepton_1, lepton_2, MET]
        """
        super().__init__()
        
        self.n_jets = n_jets
        self.n_leptons = n_leptons
        self.jet_features = jet_features
        self.lepton_features = lepton_features
        
        # Input tensorisÃ©
        input_dims = [jet_features] * n_jets + [lepton_features] * n_leptons + [4]  # MET
        self.input_dims = input_dims
        self.input_size = np.prod(input_dims)
        
        # TT encoder avec bond dimensions adaptatives
        bond_dims = [min(8, dim) for dim in input_dims[:-1]]
        latent_dim = 16
        
        # Encodeur (simplifiÃ©: MLP qui simule TT compression)
        self.encoder = nn.Sequential(
            nn.Linear(self.input_size, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, latent_dim)
        )
        
        # DÃ©codeur
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 128),
            nn.ReLU(),
            nn.Linear(128, self.input_size)
        )
    
    def forward(self, event_dict):
        """
        Forward avec structure d'Ã©vÃ©nement
        
        Args:
            event_dict: {
                'jets': (batch, n_jets, jet_features),
                'leptons': (batch, n_leptons, lepton_features),
                'met': (batch, 4)
            }
        """
        # Flatten selon structure tensorielle
        jets_flat = event_dict['jets'].view(event_dict['jets'].shape[0], -1)
        leptons_flat = event_dict['leptons'].view(event_dict['leptons'].shape[0], -1)
        met_flat = event_dict['met']
        
        x = torch.cat([jets_flat, leptons_flat, met_flat], dim=1)
        
        # Encode
        latent = self.encoder(x)
        
        # Decode
        decoded = self.decoder(latent)
        
        # Reshape
        decoded_dict = {
            'jets': decoded[:, :self.n_jets * self.jet_features].view(
                -1, self.n_jets, self.jet_features
            ),
            'leptons': decoded[:, 
                self.n_jets * self.jet_features:
                self.n_jets * self.jet_features + self.n_leptons * self.lepton_features
            ].view(-1, self.n_leptons, self.lepton_features),
            'met': decoded[:, -4:]
        }
        
        return decoded_dict
    
    def compute_anomaly_score(self, event_dict):
        """Score d'anomalie par composante"""
        with torch.no_grad():
            decoded = self.forward(event_dict)
            
            scores = {}
            
            # Erreur par composante
            scores['jets'] = torch.mean((event_dict['jets'] - decoded['jets'])**2, dim=(1, 2))
            scores['leptons'] = torch.mean((event_dict['leptons'] - decoded['leptons'])**2, dim=(1, 2))
            scores['met'] = torch.mean((event_dict['met'] - decoded['met'])**2, dim=1)
            
            # Score total
            scores['total'] = scores['jets'] + scores['leptons'] + scores['met']
            
            return scores

hep_tt_ae = HEPTensorAutoencoder(n_jets=4, n_leptons=2)

print(f"\nHEP Tensor Autoencoder:")
print(f"  Input size: {hep_tt_ae.input_size}")
print(f"  ParamÃ¨tres: {sum(p.numel() for p in hep_tt_ae.parameters()):,}")
```

---

## Exercices

### Exercice 20.4.1
ImplÃ©mentez un autoencodeur Tensor Train complet avec vraie contraction TT pour encoder/dÃ©coder.

### Exercice 20.4.2
Comparez performance d'un autoencodeur TT vs autoencodeur standard sur donnÃ©es HEP simulÃ©es.

### Exercice 20.4.3
Analysez l'impact des bond dimensions sur capacitÃ© de dÃ©tection d'anomalies.

### Exercice 20.4.4
DÃ©veloppez un modÃ¨le MPS qui modÃ©lise distribution jointe de features discrÃ©tisÃ©es et utilise pour dÃ©tection d'anomalies.

---

## Points ClÃ©s Ã  Retenir

> ğŸ“Œ **Les rÃ©seaux de tenseurs offrent compression et efficacitÃ© computationnelle**

> ğŸ“Œ **Les autoencoders TT peuvent encoder/dÃ©coder avec moins de paramÃ¨tres**

> ğŸ“Œ **Les MPS peuvent modÃ©liser distributions jointes de features discrÃ©tisÃ©es**

> ğŸ“Œ **La structure tensorielle capture corrÃ©lations multi-variables importantes**

> ğŸ“Œ **L'efficacitÃ© permet dÃ©ploiement sur FPGA pour triggers**

> ğŸ“Œ **L'interprÃ©tabilitÃ© via bond dimensions rÃ©vÃ¨le complexitÃ© nÃ©cessaire**

---

*Section prÃ©cÃ©dente : [20.3 MÃ©thodes Non SupervisÃ©es](./20_03_Non_Supervise.md) | Section suivante : [20.5 Quantification de l'Incertitude](./20_05_Incertitude.md)*

