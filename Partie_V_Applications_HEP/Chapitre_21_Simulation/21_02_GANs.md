# 21.2 Generative Adversarial Networks (GANs) pour la Simulation

---

## Introduction

Les **Generative Adversarial Networks (GANs)** sont parmi les modÃ¨les gÃ©nÃ©ratifs les plus populaires pour la simulation d'Ã©vÃ©nements en physique des hautes Ã©nergies. Leur capacitÃ© Ã  gÃ©nÃ©rer des Ã©chantillons de haute qualitÃ© et leur flexibilitÃ© les rendent adaptÃ©s pour remplacer ou accÃ©lÃ©rer des parties coÃ»teuses de la simulation Monte Carlo.

Cette section prÃ©sente les principes des GANs, leur application spÃ©cifique Ã  la gÃ©nÃ©ration d'Ã©vÃ©nements HEP, et les variantes spÃ©cialisÃ©es dÃ©veloppÃ©es pour ce domaine.

---

## Architecture des GANs

### Principe Adversarial

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, Tuple

class Generator(nn.Module):
    """
    GÃ©nÃ©rateur pour GAN
    
    Transforme bruit en Ã©vÃ©nements rÃ©alistes
    """
    
    def __init__(self, noise_dim=100, output_dim=50, hidden_dims=[256, 512, 256]):
        super().__init__()
        
        layers = []
        prev_dim = noise_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU()
            ])
            prev_dim = hidden_dim
        
        layers.append(nn.Linear(prev_dim, output_dim))
        layers.append(nn.Tanh())  # Normaliser sortie
        
        self.model = nn.Sequential(*layers)
    
    def forward(self, noise):
        """GÃ©nÃ¨re Ã©vÃ©nements depuis bruit"""
        return self.model(noise)

class Discriminator(nn.Module):
    """
    Discriminateur pour GAN
    
    Distingue Ã©vÃ©nements rÃ©els vs gÃ©nÃ©rÃ©s
    """
    
    def __init__(self, input_dim=50, hidden_dims=[256, 128, 64]):
        super().__init__()
        
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.LeakyReLU(0.2),
                nn.Dropout(0.3)
            ])
            prev_dim = hidden_dim
        
        layers.append(nn.Linear(prev_dim, 1))
        layers.append(nn.Sigmoid())  # ProbabilitÃ© rÃ©el vs faux
        
        self.model = nn.Sequential(*layers)
    
    def forward(self, x):
        """PrÃ©dit si Ã©vÃ©nement est rÃ©el"""
        return self.model(x)

# CrÃ©er GAN
generator = Generator(noise_dim=100, output_dim=50)
discriminator = Discriminator(input_dim=50)

print(f"\nGAN Architecture:")
print(f"  GÃ©nÃ©rateur paramÃ¨tres: {sum(p.numel() for p in generator.parameters()):,}")
print(f"  Discriminateur paramÃ¨tres: {sum(p.numel() for p in discriminator.parameters()):,}")
```

---

## Fonction de CoÃ»t et EntraÃ®nement

### Min-Max Game

```python
class GANTraining:
    """
    EntraÃ®nement d'un GAN
    """
    
    def __init__(self, generator, discriminator, lr_g=0.0002, lr_d=0.0002):
        self.generator = generator
        self.discriminator = discriminator
        
        self.optimizer_g = torch.optim.Adam(generator.parameters(), lr=lr_g, betas=(0.5, 0.999))
        self.optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=lr_d, betas=(0.5, 0.999))
        
        self.criterion = nn.BCELoss()
    
    def train_discriminator(self, real_data, batch_size, noise_dim):
        """
        EntraÃ®ne discriminateur
        
        Objectif: maximiser log(D(real)) + log(1 - D(G(noise)))
        """
        self.optimizer_d.zero_grad()
        
        # Ã‰vÃ©nements rÃ©els
        real_labels = torch.ones(real_data.size(0), 1)
        real_output = self.discriminator(real_data)
        loss_real = self.criterion(real_output, real_labels)
        
        # Ã‰vÃ©nements gÃ©nÃ©rÃ©s
        noise = torch.randn(real_data.size(0), noise_dim)
        fake_data = self.generator(noise)
        fake_labels = torch.zeros(fake_data.size(0), 1)
        fake_output = self.discriminator(fake_data.detach())  # DÃ©tacher pour ne pas entraÃ®ner G
        loss_fake = self.criterion(fake_output, fake_labels)
        
        # Loss totale
        loss_d = loss_real + loss_fake
        loss_d.backward()
        self.optimizer_d.step()
        
        return {
            'loss_d': loss_d.item(),
            'loss_real': loss_real.item(),
            'loss_fake': loss_fake.item(),
            'accuracy_real': (real_output > 0.5).float().mean().item(),
            'accuracy_fake': (fake_output < 0.5).float().mean().item()
        }
    
    def train_generator(self, batch_size, noise_dim):
        """
        EntraÃ®ne gÃ©nÃ©rateur
        
        Objectif: minimiser log(1 - D(G(noise))) = maximiser log(D(G(noise)))
        """
        self.optimizer_g.zero_grad()
        
        # GÃ©nÃ©rer Ã©vÃ©nements
        noise = torch.randn(batch_size, noise_dim)
        fake_data = self.generator(noise)
        
        # GÃ©nÃ©rateur veut tromper discriminateur
        fake_labels = torch.ones(batch_size, 1)  # Vouloir Ãªtre classifiÃ© comme rÃ©el
        fake_output = self.discriminator(fake_data)
        
        loss_g = self.criterion(fake_output, fake_labels)
        loss_g.backward()
        self.optimizer_g.step()
        
        return {
            'loss_g': loss_g.item(),
            'generator_confidence': fake_output.mean().item()
        }
    
    def train_step(self, real_data, batch_size, noise_dim, n_d_steps=1):
        """
        Une Ã©tape d'entraÃ®nement complÃ¨te
        
        Args:
            n_d_steps: Nombre de pas pour discriminateur par pas gÃ©nÃ©rateur
        """
        # EntraÃ®ner discriminateur
        d_results = []
        for _ in range(n_d_steps):
            result = self.train_discriminator(real_data, batch_size, noise_dim)
            d_results.append(result)
        
        # EntraÃ®ner gÃ©nÃ©rateur
        g_result = self.train_generator(batch_size, noise_dim)
        
        return {
            'discriminator': d_results[-1],  # Dernier rÃ©sultat
            'generator': g_result
        }

# Test entraÃ®nement
gan_trainer = GANTraining(generator, discriminator)

# Simuler donnÃ©es
real_events = torch.randn(32, 50)  # Batch de 32 Ã©vÃ©nements

# Une Ã©tape d'entraÃ®nement
step_result = gan_trainer.train_step(real_events, batch_size=32, noise_dim=100, n_d_steps=1)

print(f"\nEntraÃ®nement GAN:")
print(f"  Loss discriminateur: {step_result['discriminator']['loss_d']:.4f}")
print(f"  Loss gÃ©nÃ©rateur: {step_result['generator']['loss_g']:.4f}")
print(f"  Accuracy rÃ©el: {step_result['discriminator']['accuracy_real']:.2%}")
print(f"  Accuracy faux: {step_result['discriminator']['accuracy_fake']:.2%}")
```

---

## GANs Conditionnels

### GÃ©nÃ©ration ConditionnÃ©e

```python
class ConditionalGenerator(nn.Module):
    """
    GÃ©nÃ©rateur conditionnel
    
    GÃ©nÃ¨re Ã©vÃ©nements conditionnÃ©s sur paramÃ¨tres (ex: Ã©nergie, processus)
    """
    
    def __init__(self, noise_dim=100, condition_dim=10, output_dim=50):
        super().__init__()
        
        # ConcatÃ©ner bruit et condition
        input_dim = noise_dim + condition_dim
        
        self.model = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Linear(256, output_dim),
            nn.Tanh()
        )
    
    def forward(self, noise, condition):
        """
        Args:
            noise: (batch, noise_dim)
            condition: (batch, condition_dim) - paramÃ¨tres conditionnels
        """
        # ConcatÃ©ner
        input_concat = torch.cat([noise, condition], dim=1)
        return self.model(input_concat)

class ConditionalDiscriminator(nn.Module):
    """
    Discriminateur conditionnel
    """
    
    def __init__(self, input_dim=50, condition_dim=10):
        super().__init__()
        
        # ConcatÃ©ner Ã©vÃ©nement et condition
        input_concat_dim = input_dim + condition_dim
        
        self.model = nn.Sequential(
            nn.Linear(input_concat_dim, 256),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x, condition):
        """PrÃ©dit si (Ã©vÃ©nement, condition) est rÃ©el"""
        input_concat = torch.cat([x, condition], dim=1)
        return self.model(input_concat)

# CrÃ©er GAN conditionnel
c_generator = ConditionalGenerator(noise_dim=100, condition_dim=5, output_dim=50)
c_discriminator = ConditionalDiscriminator(input_dim=50, condition_dim=5)

print(f"\nGAN Conditionnel:")
print(f"  Permet gÃ©nÃ©ration selon paramÃ¨tres (Ã©nergie, type processus, etc.)")
```

---

## Application aux Jets

### GAN pour GÃ©nÃ©ration de Jets

```python
class JetGeneratorGAN(nn.Module):
    """
    GAN spÃ©cialisÃ© pour gÃ©nÃ©ration de jets
    """
    
    def __init__(self, n_particles_per_jet=30, particle_features=4):
        """
        Args:
            n_particles_per_jet: Nombre max de particules par jet
            particle_features: Features par particule (pT, Î·, Ï†, E)
        """
        super().__init__()
        
        self.n_particles = n_particles_per_jet
        self.particle_features = particle_features
        self.jet_dim = n_particles_per_jet * particle_features
        
        # GÃ©nÃ©rateur
        self.generator = Generator(
            noise_dim=100,
            output_dim=self.jet_dim,
            hidden_dims=[512, 1024, 512]
        )
        
        # Discriminateur
        self.discriminator = Discriminator(
            input_dim=self.jet_dim,
            hidden_dims=[512, 256, 128]
        )
    
    def generate_jet(self, noise):
        """
        GÃ©nÃ¨re un jet (ensemble de particules)
        
        Returns:
            jet: (batch, n_particles, features)
        """
        flat_jet = self.generator(noise)
        jet = flat_jet.view(-1, self.n_particles, self.particle_features)
        
        # Appliquer contraintes physiques (ex: pT > 0)
        jet[:, :, 0] = torch.abs(jet[:, :, 0])  # pT positif
        
        return jet
    
    def compute_jet_features(self, jet):
        """
        Calcule observables du jet
        
        Args:
            jet: (batch, n_particles, features) [pT, Î·, Ï†, E]
        """
        # Masse invariante du jet
        pT = jet[:, :, 0]
        eta = jet[:, :, 1]
        phi = jet[:, :, 2]
        E = jet[:, :, 3]
        
        # Composantes 4-momentum
        px = pT * torch.cos(phi)
        py = pT * torch.sin(phi)
        pz = pT * torch.sinh(eta)
        
        # Somme sur particules
        px_total = px.sum(dim=1)
        py_total = py.sum(dim=1)
        pz_total = pz.sum(dim=1)
        E_total = E.sum(dim=1)
        
        # Masse invariante
        m_invariant = torch.sqrt(E_total**2 - px_total**2 - py_total**2 - pz_total**2)
        
        return {
            'mass': m_invariant,
            'pT': torch.sqrt(px_total**2 + py_total**2),
            'eta': torch.atanh(pz_total / torch.sqrt(E_total**2 - m_invariant**2 + 1e-10))
        }

jet_gan = JetGeneratorGAN(n_particles_per_jet=30, particle_features=4)

# GÃ©nÃ©rer jet
noise = torch.randn(1, 100)
jet = jet_gan.generate_jet(noise)

jet_features = jet_gan.compute_jet_features(jet)

print(f"\nJet GAN:")
print(f"  Jet gÃ©nÃ©rÃ©: {jet.shape}")
print(f"  Masse invariante: {jet_features['mass'].item():.2f} GeV")
print(f"  pT jet: {jet_features['pT'].item():.2f} GeV")
```

---

## Wasserstein GAN (WGAN)

### AmÃ©lioration de StabilitÃ©

```python
class WGANGenerator(nn.Module):
    """GÃ©nÃ©rateur pour WGAN (identique Ã  GAN standard)"""
    
    def __init__(self, noise_dim=100, output_dim=50):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(noise_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, output_dim)
            # Pas de Tanh pour WGAN
        )
    
    def forward(self, noise):
        return self.model(noise)

class WGANCritic(nn.Module):
    """
    Critic pour WGAN (remplace discriminateur)
    
    Output valeur rÃ©elle (pas probabilitÃ©)
    """
    
    def __init__(self, input_dim=50):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 1)
            # Pas de sigmoid (output valeur rÃ©elle)
        )
    
    def forward(self, x):
        return self.model(x)

class WGANTraining:
    """
    EntraÃ®nement WGAN avec gradient penalty
    """
    
    def __init__(self, generator, critic, lr_g=0.0001, lr_c=0.0001):
        self.generator = generator
        self.critic = critic
        
        self.optimizer_g = torch.optim.RMSprop(generator.parameters(), lr=lr_g)
        self.optimizer_c = torch.optim.RMSprop(critic.parameters(), lr=lr_c)
        
        self.gradient_penalty_weight = 10.0
    
    def compute_gradient_penalty(self, real_data, fake_data):
        """
        Calcule gradient penalty pour Lipschitz constraint
        """
        batch_size = real_data.size(0)
        alpha = torch.rand(batch_size, 1)
        alpha = alpha.expand_as(real_data)
        
        # Interpolation entre rÃ©el et faux
        interpolated = alpha * real_data + (1 - alpha) * fake_data
        interpolated.requires_grad_(True)
        
        # Critic sur interpolation
        critic_interpolated = self.critic(interpolated)
        
        # Gradient
        gradients = torch.autograd.grad(
            outputs=critic_interpolated,
            inputs=interpolated,
            grad_outputs=torch.ones_like(critic_interpolated),
            create_graph=True,
            retain_graph=True
        )[0]
        
        # Gradient penalty
        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()
        
        return gradient_penalty
    
    def train_critic(self, real_data, noise):
        """EntraÃ®ne critic"""
        self.optimizer_c.zero_grad()
        
        fake_data = self.generator(noise).detach()
        
        # Loss critic: maximiser E[C(real)] - E[C(fake)]
        real_score = self.critic(real_data).mean()
        fake_score = self.critic(fake_data).mean()
        
        # Gradient penalty
        gradient_penalty = self.compute_gradient_penalty(real_data, fake_data)
        
        loss_c = fake_score - real_score + self.gradient_penalty_weight * gradient_penalty
        loss_c.backward()
        self.optimizer_c.step()
        
        return {
            'loss_c': loss_c.item(),
            'real_score': real_score.item(),
            'fake_score': fake_score.item()
        }
    
    def train_generator(self, noise):
        """EntraÃ®ne gÃ©nÃ©rateur"""
        self.optimizer_g.zero_grad()
        
        fake_data = self.generator(noise)
        fake_score = self.critic(fake_data).mean()
        
        # Loss gÃ©nÃ©rateur: minimiser -E[C(fake)]
        loss_g = -fake_score
        loss_g.backward()
        self.optimizer_g.step()
        
        return {
            'loss_g': loss_g.item(),
            'fake_score': fake_score.item()
        }

wgan_gen = WGANGenerator(noise_dim=100, output_dim=50)
wgan_critic = WGANCritic(input_dim=50)

wgan_trainer = WGANTraining(wgan_gen, wgan_critic)

print(f"\nWGAN:")
print(f"  Plus stable que GAN standard")
print(f"  Utilise Wasserstein distance au lieu de JS divergence")
```

---

## Exercices

### Exercice 21.2.1
EntraÃ®nez un GAN simple pour apprendre Ã  gÃ©nÃ©rer des Ã©vÃ©nements avec une distribution 2D spÃ©cifique.

### Exercice 21.2.2
ImplÃ©mentez un GAN conditionnel qui gÃ©nÃ¨re des Ã©vÃ©nements selon l'Ã©nergie du centre de masse.

### Exercice 21.2.3
Comparez stabilitÃ© d'entraÃ®nement entre GAN standard et WGAN sur mÃªme dataset.

### Exercice 21.2.4
DÃ©veloppez un GAN pour gÃ©nÃ©ration de jets avec contraintes physiques (conservation Ã©nergie, etc.).

---

## Points ClÃ©s Ã  Retenir

> ğŸ“Œ **Les GANs utilisent jeu adversarial entre gÃ©nÃ©rateur et discriminateur**

> ğŸ“Œ **L'entraÃ®nement GAN est instable et nÃ©cessite soin (WGAN amÃ©liore cela)**

> ğŸ“Œ **Les GANs conditionnels permettent gÃ©nÃ©ration selon paramÃ¨tres spÃ©cifiques**

> ğŸ“Œ **Les GANs peuvent gÃ©nÃ©rer Ã©vÃ©nements/jets complexes rapidement**

> ğŸ“Œ **La validation est cruciale pour garantir qualitÃ© physique**

> ğŸ“Œ **Les GANs sont 100-1000Ã— plus rapides que simulation MC complÃ¨te**

---

*Section prÃ©cÃ©dente : [21.1 Monte Carlo](./21_01_Monte_Carlo.md) | Section suivante : [21.3 Normalizing Flows](./21_03_Normalizing_Flows.md)*

