# 21.3 Normalizing Flows

---

## Introduction

Les **Normalizing Flows** sont des modÃ¨les gÃ©nÃ©ratifs qui apprennent une transformation inversible entre une distribution simple (gaussienne) et la distribution complexe des donnÃ©es. Leur avantage principal est de fournir une densitÃ© explicite, permettant Ã©chantillonnage exact et Ã©valuation de probabilitÃ©s, ce qui est particuliÃ¨rement utile pour la simulation en physique des hautes Ã©nergies.

Cette section prÃ©sente les principes des normalizing flows, leur application Ã  la gÃ©nÃ©ration d'Ã©vÃ©nements HEP, et les architectures spÃ©cialisÃ©es dÃ©veloppÃ©es.

---

## Principe des Normalizing Flows

### Transformation Inversible

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Tuple

class NormalizingFlowPrinciple:
    """
    Principes de base des normalizing flows
    """
    
    def __init__(self):
        self.concepts = {
            'transformation': {
                'description': 'Transformation inversible f: X â†’ Z',
                'base_distribution': 'Distribution simple (gaussienne)',
                'target_distribution': 'Distribution complexe des donnÃ©es'
            },
            'change_of_variables': {
                'formula': 'p_X(x) = p_Z(f(x)) |det(âˆ‚f/âˆ‚x)|',
                'importance': 'Permet calcul densitÃ© explicite'
            },
            'composition': {
                'description': 'Composition de transformations simples',
                'formula': 'f = f_K â—‹ f_{K-1} â—‹ ... â—‹ f_1',
                'benefit': 'FlexibilitÃ© avec transformations simples'
            }
        }
    
    def display_principles(self):
        """Affiche les principes"""
        print("\n" + "="*70)
        print("Principes des Normalizing Flows")
        print("="*70)
        
        for concept, info in self.concepts.items():
            print(f"\n{concept.replace('_', ' ').title()}:")
            if isinstance(info, dict):
                for key, value in info.items():
                    print(f"  {key}: {value}")

principle = NormalizingFlowPrinciple()
principle.display_principles()
```

---

## Couche Affine CouplÃ©e (Affine Coupling)

### Architecture de Base

```python
class AffineCouplingLayer(nn.Module):
    """
    Couche affine couplÃ©e
    
    Transformation simple et inversible
    """
    
    def __init__(self, dim, hidden_dim=64):
        super().__init__()
        
        self.dim = dim
        self.split_dim = dim // 2
        
        # Network pour calculer scale et shift
        self.network = nn.Sequential(
            nn.Linear(self.split_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, (dim - self.split_dim) * 2)  # scale et shift
        )
    
    def forward(self, x):
        """
        Forward: x â†’ z
        
        Args:
            x: (batch, dim)
        Returns:
            z: (batch, dim)
            log_det: log dÃ©terminant jacobien
        """
        x1, x2 = x[:, :self.split_dim], x[:, self.split_dim:]
        
        # Calculer scale et shift
        params = self.network(x1)
        log_scale, shift = torch.split(params, self.split_dim, dim=1)
        
        # Transformation
        z1 = x1
        z2 = x2 * torch.exp(log_scale) + shift
        
        z = torch.cat([z1, z2], dim=1)
        
        # Log dÃ©terminant jacobien
        log_det = log_scale.sum(dim=1)
        
        return z, log_det
    
    def inverse(self, z):
        """
        Inverse: z â†’ x
        """
        z1, z2 = z[:, :self.split_dim], z[:, self.split_dim:]
        
        # Calculer scale et shift
        params = self.network(z1)
        log_scale, shift = torch.split(params, self.split_dim, dim=1)
        
        # Transformation inverse
        x1 = z1
        x2 = (z2 - shift) * torch.exp(-log_scale)
        
        x = torch.cat([x1, x2], dim=1)
        
        return x

# Test couche affine
coupling = AffineCouplingLayer(dim=10, hidden_dim=32)

x = torch.randn(5, 10)
z, log_det = coupling(x)
x_reconstructed = coupling.inverse(z)

print(f"\nAffine Coupling Layer:")
print(f"  Erreur reconstruction: {(x - x_reconstructed).abs().max().item():.6f}")
print(f"  Log det jacobien moyen: {log_det.mean().item():.4f}")
```

---

## Real NVP (Non-volume Preserving)

### Flow avec Permutations

```python
class RealNVPFlow(nn.Module):
    """
    Real NVP Flow
    
    Composition de couches de couplage avec permutations
    """
    
    def __init__(self, dim, n_layers=4, hidden_dim=64):
        super().__init__()
        
        self.dim = dim
        self.n_layers = n_layers
        
        # Couches de couplage
        self.coupling_layers = nn.ModuleList([
            AffineCouplingLayer(dim, hidden_dim) for _ in range(n_layers)
        ])
        
        # Permutations alÃ©atoires (fixÃ©es)
        self.permutations = []
        for i in range(n_layers):
            perm = torch.randperm(dim)
            self.register_buffer(f'perm_{i}', perm)
            self.permutations.append(perm)
    
    def permute(self, x, perm):
        """Applique permutation"""
        return x[:, perm]
    
    def inverse_permute(self, x, perm):
        """Applique permutation inverse"""
        inv_perm = torch.argsort(perm)
        return x[:, inv_perm]
    
    def forward(self, x):
        """
        Forward: x â†’ z
        
        Returns:
            z: (batch, dim)
            log_det_total: log dÃ©terminant total
        """
        log_det_total = torch.zeros(x.size(0))
        z = x
        
        for i, coupling in enumerate(self.coupling_layers):
            # Permutation
            perm = getattr(self, f'perm_{i}')
            z = self.permute(z, perm)
            
            # Couplage
            z, log_det = coupling(z)
            log_det_total += log_det
        
        return z, log_det_total
    
    def inverse(self, z):
        """Inverse: z â†’ x"""
        x = z
        
        # Inverse dans ordre inverse
        for i in reversed(range(self.n_layers)):
            coupling = self.coupling_layers[i]
            perm = getattr(self, f'perm_{i}')
            
            # Inverse couplage
            x = coupling.inverse(x)
            
            # Inverse permutation
            x = self.inverse_permute(x, perm)
        
        return x
    
    def log_prob(self, x):
        """
        Calcule log-probabilitÃ© p(x)
        
        p(x) = p_z(f(x)) + log|det(âˆ‚f/âˆ‚x)|
        """
        z, log_det = self.forward(x)
        
        # ProbabilitÃ© dans espace latent (gaussien standard)
        log_prob_z = -0.5 * (z**2).sum(dim=1) - 0.5 * self.dim * np.log(2 * np.pi)
        
        # Log prob dans espace original
        log_prob_x = log_prob_z + log_det
        
        return log_prob_x
    
    def sample(self, n_samples=1000):
        """
        Ã‰chantillonne depuis la distribution
        
        z ~ N(0,1) â†’ x = f^{-1}(z)
        """
        # Ã‰chantillonner depuis distribution de base
        z = torch.randn(n_samples, self.dim)
        
        # Transformer
        x = self.inverse(z)
        
        return x

# CrÃ©er Real NVP flow
flow = RealNVPFlow(dim=20, n_layers=6, hidden_dim=64)

print(f"\nReal NVP Flow:")
print(f"  Dimensions: {flow.dim}")
print(f"  Nombre de couches: {flow.n_layers}")
print(f"  ParamÃ¨tres: {sum(p.numel() for p in flow.parameters()):,}")

# Test
x_test = torch.randn(10, 20)
z, log_det = flow(x_test)
x_reconstructed = flow.inverse(z)

log_prob = flow.log_prob(x_test)
samples = flow.sample(n_samples=100)

print(f"  Erreur reconstruction: {(x_test - x_reconstructed).abs().max().item():.6f}")
print(f"  Log prob moyen: {log_prob.mean().item():.4f}")
print(f"  Ã‰chantillons gÃ©nÃ©rÃ©s: {samples.shape}")
```

---

## Neural Spline Flows

### Flows avec Splines

```python
class NeuralSplineCoupling(nn.Module):
    """
    Couche de couplage avec splines rationnelles quadratiques
    
    Plus flexible que transformation affine
    """
    
    def __init__(self, dim, hidden_dim=64, n_bins=8):
        super().__init__()
        
        self.dim = dim
        self.split_dim = dim // 2
        self.n_bins = n_bins
        
        # Network pour calculer paramÃ¨tres spline
        # Pour chaque output: widths, heights, derivatives (n_bins + 1 valeurs chacun)
        self.network = nn.Sequential(
            nn.Linear(self.split_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, (dim - self.split_dim) * (n_bins * 3 + 1))
        )
    
    def forward(self, x):
        """
        Forward avec splines (simplifiÃ©)
        
        En pratique: implÃ©mentation complÃ¨te de RQ-spline
        """
        x1, x2 = x[:, :self.split_dim], x[:, self.split_dim:]
        
        # Calculer paramÃ¨tres spline
        params = self.network(x1)
        # SimplifiÃ©: utiliser transformation affine approximÃ©e
        # En pratique: implÃ©menter vraie spline
        
        # Approximation avec affine
        log_scale = params[:, :(self.dim - self.split_dim)]
        shift = params[:, (self.dim - self.split_dim):2*(self.dim - self.split_dim)]
        
        z1 = x1
        z2 = x2 * torch.exp(log_scale) + shift
        
        z = torch.cat([z1, z2], dim=1)
        log_det = log_scale.sum(dim=1)
        
        return z, log_det

# Note: ImplÃ©mentation complÃ¨te de spline flows est complexe
# Ici: structure de base
```

---

## Application aux Ã‰vÃ©nements HEP

### Flow pour GÃ©nÃ©ration d'Ã‰vÃ©nements

```python
class HEPEventFlow(nn.Module):
    """
    Normalizing Flow pour gÃ©nÃ©ration d'Ã©vÃ©nements HEP
    """
    
    def __init__(self, event_dim=50, n_layers=8, hidden_dim=128):
        super().__init__()
        
        self.event_dim = event_dim
        
        # Normalizing flow
        self.flow = RealNVPFlow(dim=event_dim, n_layers=n_layers, hidden_dim=hidden_dim)
        
        # Normalisation des donnÃ©es (important pour flows)
        self.register_buffer('data_mean', torch.zeros(event_dim))
        self.register_buffer('data_std', torch.ones(event_dim))
    
    def fit_normalization(self, data):
        """Ajuste normalisation aux donnÃ©es"""
        self.data_mean = data.mean(dim=0)
        self.data_std = data.std(dim=0) + 1e-6  # Ã‰viter division par zÃ©ro
    
    def normalize(self, x):
        """Normalise donnÃ©es"""
        return (x - self.data_mean) / self.data_std
    
    def denormalize(self, x):
        """DÃ©normalise donnÃ©es"""
        return x * self.data_std + self.data_mean
    
    def forward(self, x):
        """Forward avec normalisation"""
        x_norm = self.normalize(x)
        z, log_det = self.flow(x_norm)
        return z, log_det
    
    def inverse(self, z):
        """Inverse avec dÃ©normalisation"""
        x_norm = self.flow.inverse(z)
        x = self.denormalize(x_norm)
        return x
    
    def log_prob(self, x):
        """Log probabilitÃ© avec normalisation"""
        x_norm = self.normalize(x)
        log_prob_norm = self.flow.log_prob(x_norm)
        
        # Ajuster pour changement de variables (normalisation)
        log_det_norm = -self.data_std.log().sum()
        
        return log_prob_norm + log_det_norm
    
    def sample(self, n_samples=1000):
        """Ã‰chantillonne Ã©vÃ©nements"""
        z = torch.randn(n_samples, self.event_dim)
        x = self.inverse(z)
        return x

hep_flow = HEPEventFlow(event_dim=50, n_layers=8)

print(f"\nHEP Event Flow:")
print(f"  Dimensions Ã©vÃ©nement: {hep_flow.event_dim}")
print(f"  ParamÃ¨tres: {sum(p.numel() for p in hep_flow.parameters()):,}")

# Simuler ajustement
training_data = torch.randn(10000, 50)
hep_flow.fit_normalization(training_data)

# GÃ©nÃ©rer Ã©vÃ©nements
generated_events = hep_flow.sample(n_samples=1000)

print(f"  Ã‰vÃ©nements gÃ©nÃ©rÃ©s: {generated_events.shape}")
print(f"  Moyenne gÃ©nÃ©rÃ©e: {generated_events.mean(dim=0)[:5]}")
print(f"  Std gÃ©nÃ©rÃ©e: {generated_events.std(dim=0)[:5]}")
```

---

## Flows Conditionnels

### GÃ©nÃ©ration ConditionnÃ©e

```python
class ConditionalRealNVP(nn.Module):
    """
    Real NVP conditionnel
    
    GÃ©nÃ¨re Ã©vÃ©nements conditionnÃ©s sur paramÃ¨tres
    """
    
    def __init__(self, event_dim=50, condition_dim=5, n_layers=6, hidden_dim=128):
        super().__init__()
        
        self.event_dim = event_dim
        self.condition_dim = condition_dim
        
        # Couches de couplage conditionnelles
        self.coupling_layers = nn.ModuleList()
        self.permutations = []
        
        for i in range(n_layers):
            # Network inclut condition
            network = nn.Sequential(
                nn.Linear(event_dim // 2 + condition_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, (event_dim - event_dim // 2) * 2)
            )
            
            coupling = ConditionalAffineCoupling(network, event_dim)
            self.coupling_layers.append(coupling)
            
            perm = torch.randperm(event_dim)
            self.register_buffer(f'perm_{i}', perm)
            self.permutations.append(perm)
    
    def forward(self, x, condition):
        """Forward conditionnel"""
        log_det_total = torch.zeros(x.size(0))
        z = x
        
        for i, coupling in enumerate(self.coupling_layers):
            perm = getattr(self, f'perm_{i}')
            z = z[:, perm]
            
            z, log_det = coupling(z, condition)
            log_det_total += log_det
        
        return z, log_det_total
    
    def inverse(self, z, condition):
        """Inverse conditionnel"""
        x = z
        
        for i in reversed(range(len(self.coupling_layers))):
            coupling = self.coupling_layers[i]
            perm = getattr(self, f'perm_{i}')
            
            x = coupling.inverse(x, condition)
            inv_perm = torch.argsort(perm)
            x = x[:, inv_perm]
        
        return x
    
    def sample(self, condition, n_samples=1000):
        """Ã‰chantillonne conditionnel"""
        z = torch.randn(n_samples, self.event_dim)
        x = self.inverse(z, condition)
        return x

class ConditionalAffineCoupling(nn.Module):
    """Couplage affine conditionnel"""
    
    def __init__(self, network, dim):
        super().__init__()
        self.network = network
        self.dim = dim
        self.split_dim = dim // 2
    
    def forward(self, x, condition):
        x1, x2 = x[:, :self.split_dim], x[:, self.split_dim:]
        
        # ConcatÃ©ner avec condition
        input_concat = torch.cat([x1, condition], dim=1)
        params = self.network(input_concat)
        log_scale, shift = torch.split(params, self.split_dim, dim=1)
        
        z1 = x1
        z2 = x2 * torch.exp(log_scale) + shift
        
        z = torch.cat([z1, z2], dim=1)
        log_det = log_scale.sum(dim=1)
        
        return z, log_det
    
    def inverse(self, z, condition):
        z1, z2 = z[:, :self.split_dim], z[:, self.split_dim:]
        
        input_concat = torch.cat([z1, condition], dim=1)
        params = self.network(input_concat)
        log_scale, shift = torch.split(params, self.split_dim, dim=1)
        
        x1 = z1
        x2 = (z2 - shift) * torch.exp(-log_scale)
        
        x = torch.cat([x1, x2], dim=1)
        return x

cond_flow = ConditionalRealNVP(event_dim=50, condition_dim=5, n_layers=6)

print(f"\nConditional Flow:")
print(f"  GÃ©nÃ¨re Ã©vÃ©nements selon condition (Ã©nergie, processus, etc.)")
```

---

## EntraÃ®nement

### Loss et Optimisation

```python
class FlowTraining:
    """
    EntraÃ®nement d'un normalizing flow
    """
    
    def __init__(self, flow, lr=0.001):
        self.flow = flow
        self.optimizer = torch.optim.Adam(flow.parameters(), lr=lr)
    
    def train_step(self, data):
        """
        Une Ã©tape d'entraÃ®nement
        
        Loss = -log_prob (negative log-likelihood)
        """
        self.optimizer.zero_grad()
        
        # Calculer log probabilitÃ©
        log_prob = self.flow.log_prob(data)
        
        # Loss = negative log-likelihood
        loss = -log_prob.mean()
        
        loss.backward()
        self.optimizer.step()
        
        return {
            'loss': loss.item(),
            'avg_log_prob': log_prob.mean().item()
        }
    
    def train(self, data_loader, n_epochs=50):
        """EntraÃ®nement complet"""
        losses = []
        
        for epoch in range(n_epochs):
            epoch_losses = []
            
            for batch in data_loader:
                result = self.train_step(batch)
                epoch_losses.append(result['loss'])
            
            avg_loss = np.mean(epoch_losses)
            losses.append(avg_loss)
            
            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/{n_epochs}: Loss = {avg_loss:.4f}")
        
        return losses

# Exemple entraÃ®nement
flow_trainer = FlowTraining(flow, lr=0.001)

# Simuler donnÃ©es
train_data = torch.randn(1000, 20)

# Une Ã©tape
step_result = flow_trainer.train_step(train_data)
print(f"\nEntraÃ®nement Flow:")
print(f"  Loss: {step_result['loss']:.4f}")
print(f"  Log prob moyen: {step_result['avg_log_prob']:.4f}")
```

---

## Exercices

### Exercice 21.3.1
ImplÃ©mentez une couche de couplage affine complÃ¨te et testez son inverse.

### Exercice 21.3.2
CrÃ©ez un Real NVP flow pour apprendre une distribution 2D complexe (ex: deux gaussiennes).

### Exercice 21.3.3
EntraÃ®nez un normalizing flow sur donnÃ©es d'Ã©vÃ©nements HEP simulÃ©es et comparez distributions gÃ©nÃ©rÃ©es vs rÃ©elles.

### Exercice 21.3.4
ImplÃ©mentez un flow conditionnel qui gÃ©nÃ¨re Ã©vÃ©nements selon l'Ã©nergie du centre de masse.

---

## Points ClÃ©s Ã  Retenir

> ğŸ“Œ **Les normalizing flows apprennent transformation inversible vers distribution simple**

> ğŸ“Œ **Ils fournissent densitÃ© explicite (contrairement GANs)**

> ğŸ“Œ **L'Ã©chantillonnage est exact (pas d'approximation)**

> ğŸ“Œ **Real NVP et Neural Spline Flows sont architectures populaires**

> ğŸ“Œ **Les flows conditionnels permettent gÃ©nÃ©ration selon paramÃ¨tres**

> ğŸ“Œ **L'entraÃ®nement maximise likelihood (plus stable que GANs)**

---

*Section prÃ©cÃ©dente : [21.2 GANs](./21_02_GANs.md) | Section suivante : [21.4 Compression](./21_04_Compression.md)*

