# 21.4 AccÃ©lÃ©ration par Compression de ModÃ¨les

---

## Introduction

Les modÃ¨les gÃ©nÃ©ratifs pour la simulation (GANs, Normalizing Flows) peuvent Ãªtre grands et coÃ»teux en termes de mÃ©moire et calcul. La **compression de modÃ¨les** permet de rÃ©duire la taille des modÃ¨les tout en prÃ©servant leur capacitÃ© de gÃ©nÃ©ration, rendant possible le dÃ©ploiement sur hardware limitÃ© (FPGAs pour triggers) ou l'accÃ©lÃ©ration de l'infÃ©rence.

Cette section prÃ©sente les techniques de compression appliquÃ©es aux modÃ¨les gÃ©nÃ©ratifs pour simulation, incluant quantification, pruning, et distillation.

---

## Compression pour ModÃ¨les GÃ©nÃ©ratifs

### DÃ©fis SpÃ©cifiques

```python
import torch
import torch.nn as nn
import numpy as np
from typing import Dict

class GenerativeModelCompression:
    """
    Compression de modÃ¨les gÃ©nÃ©ratifs pour simulation
    """
    
    def __init__(self):
        self.challenges = {
            'quality_preservation': {
                'description': 'PrÃ©server qualitÃ© gÃ©nÃ©ration',
                'importance': 'Critique pour validitÃ© physique'
            },
            'latency': {
                'description': 'RÃ©duire latence gÃ©nÃ©ration',
                'use_case': 'Triggers temps rÃ©el'
            },
            'memory': {
                'description': 'RÃ©duire mÃ©moire requise',
                'use_case': 'DÃ©ploiement FPGA/edge'
            },
            'throughput': {
                'description': 'Augmenter throughput',
                'use_case': 'GÃ©nÃ©ration massive'
            }
        }
    
    def display_challenges(self):
        """Affiche les dÃ©fis"""
        print("\n" + "="*70)
        print("DÃ©fis Compression ModÃ¨les GÃ©nÃ©ratifs")
        print("="*70)
        
        for challenge, info in self.challenges.items():
            print(f"\n{challenge.replace('_', ' ').title()}:")
            print(f"  Description: {info['description']}")
            print(f"  Importance/Usage: {info.get('importance') or info.get('use_case')}")

compression = GenerativeModelCompression()
compression.display_challenges()
```

---

## Quantification des ModÃ¨les GÃ©nÃ©ratifs

### Post-Training Quantization

```python
class QuantizedGenerator(nn.Module):
    """
    GÃ©nÃ©rateur quantifiÃ©
    """
    
    def __init__(self, original_generator):
        super().__init__()
        self.original = original_generator
        
        # Copier structure
        self.quantized_layers = nn.ModuleList()
        
        # Quantifier chaque couche
        for module in original_generator.modules():
            if isinstance(module, nn.Linear):
                # CrÃ©er couche quantifiÃ©e
                quant_layer = QuantizedLinear(module)
                self.quantized_layers.append(quant_layer)
    
    def forward(self, x):
        """Forward avec quantification"""
        # En pratique: remplacer couches par versions quantifiÃ©es
        return self.original(x)  # SimplifiÃ©

class QuantizedLinear(nn.Module):
    """
    Couche linÃ©aire quantifiÃ©e
    """
    
    def __init__(self, linear_layer, n_bits=8):
        super().__init__()
        
        self.n_bits = n_bits
        self.quantization_levels = 2 ** n_bits
        
        # Quantifier poids
        weight = linear_layer.weight.data
        weight_min, weight_max = weight.min(), weight.max()
        
        # Scale et zero point
        self.scale = (weight_max - weight_min) / (self.quantization_levels - 1)
        self.zero_point = -weight_min / self.scale
        
        # Quantifier
        weight_quant = torch.round(weight / self.scale + self.zero_point)
        weight_quant = torch.clamp(weight_quant, 0, self.quantization_levels - 1)
        
        # DÃ©quantifier (pour simulation)
        self.register_buffer('weight', (weight_quant - self.zero_point) * self.scale)
        self.register_buffer('bias', linear_layer.bias.data if linear_layer.bias is not None else None)
    
    def forward(self, x):
        """Forward quantifiÃ©"""
        return F.linear(x, self.weight, self.bias)

def quantize_generator(generator, n_bits=8):
    """
    Quantifie gÃ©nÃ©rateur
    
    Returns:
        quantized_generator: GÃ©nÃ©rateur quantifiÃ©
        compression_ratio: Ratio de compression
    """
    original_params = sum(p.numel() * 32 for p in generator.parameters())  # 32 bits
    quantized_params = sum(p.numel() * n_bits for p in generator.parameters())
    
    compression_ratio = original_params / quantized_params
    
    print(f"\nQuantification GÃ©nÃ©rateur:")
    print(f"  Bits: {n_bits}")
    print(f"  Compression: {compression_ratio:.2f}Ã—")
    print(f"  Taille originale: {original_params / 1e6:.2f} MB")
    print(f"  Taille quantifiÃ©e: {quantized_params / 1e6:.2f} MB")
    
    return generator, compression_ratio

# Test quantification
test_generator = nn.Sequential(
    nn.Linear(100, 256),
    nn.ReLU(),
    nn.Linear(256, 512),
    nn.ReLU(),
    nn.Linear(512, 50)
)

quant_gen, ratio = quantize_generator(test_generator, n_bits=8)
```

---

## Pruning pour ModÃ¨les GÃ©nÃ©ratifs

### Pruning Magnitude-Based

```python
class PrunedGenerator(nn.Module):
    """
    GÃ©nÃ©rateur Ã©laguÃ© (pruned)
    """
    
    def __init__(self, original_generator, pruning_ratio=0.5):
        super().__init__()
        
        # Copier structure et Ã©laguer
        self.pruned_layers = nn.ModuleList()
        
        for module in original_generator.modules():
            if isinstance(module, nn.Linear):
                pruned_layer = self._prune_linear(module, pruning_ratio)
                self.pruned_layers.append(pruned_layer)
    
    def _prune_linear(self, linear_layer, pruning_ratio):
        """Ã‰lague couche linÃ©aire"""
        weight = linear_layer.weight.data.clone()
        
        # Calculer seuil
        threshold = torch.quantile(torch.abs(weight), pruning_ratio)
        
        # Masquer poids petits
        mask = torch.abs(weight) > threshold
        weight[~mask] = 0
        
        # CrÃ©er nouvelle couche
        pruned_layer = nn.Linear(
            linear_layer.in_features,
            linear_layer.out_features,
            bias=linear_layer.bias is not None
        )
        pruned_layer.weight.data = weight
        if linear_layer.bias is not None:
            pruned_layer.bias.data = linear_layer.bias.data.clone()
        
        # Enregistrer masque pour sparse operations
        pruned_layer.register_buffer('mask', mask)
        
        return pruned_layer
    
    def forward(self, x):
        """Forward avec poids Ã©laguÃ©s"""
        for layer in self.pruned_layers:
            x = layer(x)
        return x

def prune_generator(generator, pruning_ratio=0.5):
    """
    Ã‰lague gÃ©nÃ©rateur
    
    Returns:
        pruned_generator: GÃ©nÃ©rateur Ã©laguÃ©
        sparsity: Taux de sparsitÃ©
    """
    total_params = sum(p.numel() for p in generator.parameters())
    
    # Compter poids aprÃ¨s Ã©lagage
    pruned_params = 0
    for module in generator.modules():
        if isinstance(module, nn.Linear):
            weight = module.weight.data
            threshold = torch.quantile(torch.abs(weight), pruning_ratio)
            n_remaining = (torch.abs(weight) > threshold).sum().item()
            pruned_params += n_remaining
    
    sparsity = 1.0 - (pruned_params / total_params)
    
    print(f"\nPruning GÃ©nÃ©rateur:")
    print(f"  Ratio Ã©lagage: {pruning_ratio:.1%}")
    print(f"  SparsitÃ©: {sparsity:.1%}")
    print(f"  ParamÃ¨tres restants: {pruned_params:,} / {total_params:,}")
    
    return generator, sparsity

pruned_gen, sparsity = prune_generator(test_generator, pruning_ratio=0.5)
```

---

## Distillation pour ModÃ¨les GÃ©nÃ©ratifs

### Knowledge Distillation

```python
class GenerativeDistillation:
    """
    Distillation d'un grand gÃ©nÃ©rateur vers petit
    """
    
    def __init__(self, teacher_generator, student_generator, temperature=4.0):
        """
        Args:
            teacher_generator: Grand modÃ¨le (enseignant)
            student_generator: Petit modÃ¨le (Ã©lÃ¨ve)
            temperature: TempÃ©rature pour soft targets
        """
        self.teacher = teacher_generator
        self.student = student_generator
        self.temperature = temperature
    
    def compute_distillation_loss(self, noise, alpha=0.5):
        """
        Loss de distillation
        
        Combine:
        - Loss sur donnÃ©es rÃ©elles (si disponibles)
        - Loss entre outputs teacher/student
        """
        # GÃ©nÃ©rer avec teacher et student
        with torch.no_grad():
            teacher_output = self.teacher(noise)
        
        student_output = self.student(noise)
        
        # Loss de distillation (MSE entre outputs)
        distillation_loss = F.mse_loss(
            student_output / self.temperature,
            teacher_output / self.temperature
        ) * (self.temperature ** 2)
        
        return distillation_loss
    
    def train_student(self, data_loader, n_epochs=50, lr=0.001):
        """
        EntraÃ®ne Ã©tudiant avec distillation
        """
        optimizer = torch.optim.Adam(self.student.parameters(), lr=lr)
        
        self.teacher.eval()  # Teacher en mode eval
        
        losses = []
        
        for epoch in range(n_epochs):
            epoch_loss = 0
            
            for batch in data_loader:
                optimizer.zero_grad()
                
                # GÃ©nÃ©rer bruit
                noise = torch.randn(batch.size(0), 100)
                
                # Loss de distillation
                loss = self.compute_distillation_loss(noise)
                
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
            
            avg_loss = epoch_loss / len(data_loader)
            losses.append(avg_loss)
            
            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/{n_epochs}: Loss = {avg_loss:.4f}")
        
        return losses

# Exemple distillation
teacher = nn.Sequential(
    nn.Linear(100, 512),
    nn.ReLU(),
    nn.Linear(512, 1024),
    nn.ReLU(),
    nn.Linear(1024, 512),
    nn.ReLU(),
    nn.Linear(512, 50)
)

student = nn.Sequential(
    nn.Linear(100, 128),
    nn.ReLU(),
    nn.Linear(128, 256),
    nn.ReLU(),
    nn.Linear(256, 50)
)

distiller = GenerativeDistillation(teacher, student, temperature=4.0)

print(f"\nDistillation:")
print(f"  Teacher paramÃ¨tres: {sum(p.numel() for p in teacher.parameters()):,}")
print(f"  Student paramÃ¨tres: {sum(p.numel() for p in student.parameters()):,}")
print(f"  Compression: {sum(p.numel() for p in teacher.parameters()) / sum(p.numel() for p in student.parameters()):.2f}Ã—")
```

---

## Compression CombinÃ©e

### Techniques Multiples

```python
class ComprehensiveCompression:
    """
    Compression combinant plusieurs techniques
    """
    
    def __init__(self, generator):
        self.original = generator
        self.original_size = sum(p.numel() * 32 for p in generator.parameters())
    
    def apply_compression_pipeline(self, 
                                  quantization_bits=8,
                                  pruning_ratio=0.3,
                                  use_distillation=False):
        """
        Applique pipeline de compression
        """
        compressed_gen = self.original
        
        # 1. Pruning
        print("\n1. Pruning...")
        compressed_gen, sparsity = prune_generator(compressed_gen, pruning_ratio)
        
        # 2. Quantification
        print("\n2. Quantification...")
        compressed_gen, quant_ratio = quantize_generator(compressed_gen, quantization_bits)
        
        # 3. Distillation (optionnel)
        if use_distillation:
            print("\n3. Distillation...")
            # CrÃ©er Ã©tudiant plus petit
            student = self._create_student_model()
            distiller = GenerativeDistillation(compressed_gen, student)
            # EntraÃ®ner Ã©tudiant
            compressed_gen = student
        
        # Calculer compression totale
        final_size = sum(p.numel() * quantization_bits for p in compressed_gen.parameters())
        total_compression = self.original_size / final_size
        
        print(f"\n{'='*70}")
        print(f"Compression Totale:")
        print(f"  Taille originale: {self.original_size / 1e6:.2f} MB")
        print(f"  Taille finale: {final_size / 1e6:.2f} MB")
        print(f"  Compression totale: {total_compression:.2f}Ã—")
        print(f"{'='*70}")
        
        return compressed_gen, total_compression
    
    def _create_student_model(self):
        """CrÃ©e modÃ¨le Ã©tudiant plus petit"""
        return nn.Sequential(
            nn.Linear(100, 128),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Linear(256, 50)
        )

# Test compression combinÃ©e
compressor = ComprehensiveCompression(test_generator)
compressed, ratio = compressor.apply_compression_pipeline(
    quantization_bits=8,
    pruning_ratio=0.3,
    use_distillation=False
)
```

---

## Optimisation pour FPGA

### DÃ©ploiement Hardware

```python
class FPGAGeneratorOptimization:
    """
    Optimisations spÃ©cifiques pour dÃ©ploiement FPGA
    """
    
    def __init__(self):
        self.optimizations = {
            'fixed_point': {
                'description': 'Conversion en arithmÃ©tique fixed-point',
                'benefit': 'Hardware efficace'
            },
            'layer_fusion': {
                'description': 'Fusionner couches consÃ©cutives',
                'benefit': 'RÃ©duire latence'
            },
            'parallelization': {
                'description': 'ParallÃ©lisation des opÃ©rations',
                'benefit': 'Augmenter throughput'
            },
            'memory_optimization': {
                'description': 'Optimiser accÃ¨s mÃ©moire',
                'benefit': 'RÃ©duire bandwidth requis'
            }
        }
    
    def convert_to_fixed_point(self, generator, n_bits=16, integer_bits=8):
        """
        Convertit gÃ©nÃ©rateur en fixed-point
        
        Args:
            n_bits: Nombre total de bits
            integer_bits: Bits pour partie entiÃ¨re
        """
        # Scale factor
        fractional_bits = n_bits - integer_bits
        scale = 2 ** fractional_bits
        
        print(f"\nConversion Fixed-Point:")
        print(f"  Bits totaux: {n_bits}")
        print(f"  Bits entiers: {integer_bits}")
        print(f"  Bits fractionnaires: {fractional_bits}")
        print(f"  Scale: {scale}")
        
        return generator  # En pratique: conversion rÃ©elle
    
    def optimize_for_hls4ml(self, generator):
        """
        Optimise pour hls4ml (High-Level Synthesis)
        """
        optimizations = [
            'RÃ©duction prÃ©cision (16 bits)',
            'Fusion layers',
            'Optimisation loops',
            'Pipelining'
        ]
        
        print(f"\nOptimisations hls4ml:")
        for opt in optimizations:
            print(f"  â€¢ {opt}")
        
        return generator

fpga_optimizer = FPGAGeneratorOptimization()
fpga_optimizer.display_optimizations()
```

---

## Exercices

### Exercice 21.4.1
Quantifiez un gÃ©nÃ©rateur Ã  diffÃ©rentes prÃ©cisions (16, 8, 4 bits) et analysez l'impact sur qualitÃ© gÃ©nÃ©ration.

### Exercice 21.4.2
Appliquez pruning progressif (magnitude-based) et analysez tradeoff sparsitÃ©/qualitÃ©.

### Exercice 21.4.3
ImplÃ©mentez distillation d'un grand GAN vers petit et comparez performances.

### Exercice 21.4.4
Combine quantification + pruning et mesure compression totale et impact performance.

---

## Points ClÃ©s Ã  Retenir

> ğŸ“Œ **La compression permet dÃ©ploiement sur hardware limitÃ© (FPGA, edge)**

> ğŸ“Œ **La quantification rÃ©duit prÃ©cision (8-16 bits) avec impact limitÃ© sur qualitÃ©**

> ğŸ“Œ **Le pruning Ã©limine poids non essentiels (sparsitÃ©)**

> ğŸ“Œ **La distillation transfÃ¨re connaissance grand â†’ petit modÃ¨le**

> ğŸ“Œ **La combinaison de techniques donne compression maximale**

> ğŸ“Œ **L'optimisation FPGA nÃ©cessite fixed-point et optimisations spÃ©cifiques**

---

*Section prÃ©cÃ©dente : [21.3 Normalizing Flows](./21_03_Normalizing_Flows.md) | Section suivante : [21.5 Validation](./21_05_Validation.md)*

