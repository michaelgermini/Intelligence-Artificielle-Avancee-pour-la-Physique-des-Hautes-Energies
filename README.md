# ğŸ”¬ Intelligence Artificielle AvancÃ©e pour la Physique des Hautes Ã‰nergies

## RÃ©seaux de Tenseurs, Compression de ModÃ¨les et DÃ©ploiement Hardware

[![GitHub](https://img.shields.io/badge/GitHub-Repository-blue?style=flat-square&logo=github)](https://github.com/michaelgermini/Intelligence-Artificielle-Avancee-pour-la-Physique-des-Hautes-Energies)
[![License](https://img.shields.io/badge/License-MIT-green?style=flat-square)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Active%20Development-yellow?style=flat-square)](https://github.com/michaelgermini/Intelligence-Artificielle-Avancee-pour-la-Physique-des-Hautes-Energies)

---

## ğŸ“– Ã€ Propos de ce Livre

Ce livre est conÃ§u comme une ressource complÃ¨te pour les chercheurs et ingÃ©nieurs travaillant Ã  l'intersection de l'intelligence artificielle et de la physique des hautes Ã©nergies. Il couvre les techniques avancÃ©es de compression de modÃ¨les deep learning, les rÃ©seaux de tenseurs, et leur dÃ©ploiement sur hardware spÃ©cialisÃ© (FPGA).

**Repository GitHub** : [https://github.com/michaelgermini/Intelligence-Artificielle-Avancee-pour-la-Physique-des-Hautes-Energies](https://github.com/michaelgermini/Intelligence-Artificielle-Avancee-pour-la-Physique-des-Hautes-Energies)

### Public Cible

Ce livre s'adresse Ã  :

- ğŸ”¬ **Physiciens** souhaitant maÃ®triser les techniques d'IA modernes pour leurs recherches
- ğŸ’» **Informaticiens** s'intÃ©ressant aux applications scientifiques et au calcul haute performance
- âš¡ **IngÃ©nieurs hardware** travaillant sur le dÃ©ploiement de modÃ¨les ML sur FPGA/ASIC
- ğŸ§  **Chercheurs en machine learning** explorant les rÃ©seaux de tenseurs et la compression
- ğŸ“ **Ã‰tudiants avancÃ©s** en physique, informatique ou ingÃ©nierie
- ğŸ›ï¸ **Professionnels CERN/LHC** cherchant Ã  optimiser les systÃ¨mes de trigger et de reconstruction

### PrÃ©requis

- **MathÃ©matiques** : Connaissances de base en algÃ¨bre linÃ©aire, calcul matriciel, probabilitÃ©s
- **Programmation** : FamiliaritÃ© avec Python (numpy, pandas) et les concepts de programmation orientÃ©e objet
- **Machine Learning** : Notions fondamentales de deep learning (rÃ©seaux de neurones, backpropagation, optimisation)
- **Physique** (optionnel) : IntÃ©rÃªt pour la physique des particules facilitant la comprÃ©hension des applications

### ğŸ¯ Objectifs d'Apprentissage

Ã€ l'issue de ce livre, vous serez capable de :

âœ… Comprendre et implÃ©menter les rÃ©seaux de tenseurs pour la compression de modÃ¨les  
âœ… Appliquer les techniques de pruning, quantification et distillation  
âœ… Convertir des modÃ¨les ML vers des formats optimisÃ©s pour FPGA  
âœ… DÃ©ployer des modÃ¨les sur hardware spÃ©cialisÃ© avec hls4ml  
âœ… RÃ©soudre des problÃ¨mes concrets de trigger et reconstruction en HEP  
âœ… Optimiser les pipelines ML pour contraintes temps rÃ©el  
âœ… Contribuer Ã  des projets open source dans le domaine

---

## ğŸ¯ Points Forts

- **ğŸ“– Contenu Complet** : 28 chapitres couvrant de la thÃ©orie Ã  l'implÃ©mentation
- **ğŸ’» Code Pratique** : 6 exemples complets avec code fonctionnel et testÃ©
- **ğŸ”¬ Applications RÃ©elles** : Cas d'usage concrets du CERN et du LHC
- **âš¡ Focus Hardware** : DÃ©tails sur le dÃ©ploiement FPGA avec hls4ml
- **ğŸ“Š Visualisations** : Graphiques, diagrammes et exemples visuels
- **ğŸ”— Ressources** : Glossaire complet, rÃ©fÃ©rences, datasets et outils
- **ğŸŒ Open Source** : Tout le contenu est librement accessible et modifiable

## ğŸ“š Structure du Livre

```
Livre_IA_HEP/
â”œâ”€â”€ Partie_I_Fondements/
â”‚   â”œâ”€â”€ Chapitre_01_Introduction_HEP/
â”‚   â”œâ”€â”€ Chapitre_02_Algebre_Lineaire/
â”‚   â””â”€â”€ Chapitre_03_Deep_Learning/
â”œâ”€â”€ Partie_II_Reseaux_Tenseurs/
â”‚   â”œâ”€â”€ Chapitre_04_Introduction_Tenseurs/
â”‚   â”œâ”€â”€ Chapitre_05_Decompositions/
â”‚   â”œâ”€â”€ Chapitre_06_Physique_Quantique/
â”‚   â””â”€â”€ Chapitre_07_Conversion_NN_TN/
â”œâ”€â”€ Partie_III_Compression/
â”‚   â”œâ”€â”€ Chapitre_08_Pruning/
â”‚   â”œâ”€â”€ Chapitre_09_Quantification/
â”‚   â”œâ”€â”€ Chapitre_10_Knowledge_Distillation/
â”‚   â”œâ”€â”€ Chapitre_11_Low_Rank/
â”‚   â””â”€â”€ Chapitre_12_pQuant/
â”œâ”€â”€ Partie_IV_Hardware/
â”‚   â”œâ”€â”€ Chapitre_13_FPGA_Introduction/
â”‚   â”œâ”€â”€ Chapitre_14_NN_sur_FPGA/
â”‚   â”œâ”€â”€ Chapitre_15_hls4ml/
â”‚   â”œâ”€â”€ Chapitre_16_Hardware_NAS/
â”‚   â””â”€â”€ Chapitre_17_TN_Hardware/
â”œâ”€â”€ Partie_V_Applications_HEP/
â”‚   â”œâ”€â”€ Chapitre_18_Trigger_DAQ/
â”‚   â”œâ”€â”€ Chapitre_19_Reconstruction/
â”‚   â”œâ”€â”€ Chapitre_20_Anomalies/
â”‚   â””â”€â”€ Chapitre_21_Simulation/
â”œâ”€â”€ Partie_VI_Implementation/
â”‚   â”œâ”€â”€ Chapitre_22_Python_DL/
â”‚   â”œâ”€â”€ Chapitre_23_Decompositions_Code/
â”‚   â”œâ”€â”€ Chapitre_24_Cpp_Performance/
â”‚   â””â”€â”€ Chapitre_25_Pipelines/
â”œâ”€â”€ Partie_VII_Recherche/
â”‚   â”œâ”€â”€ Chapitre_26_Methodologie/
â”‚   â”œâ”€â”€ Chapitre_27_Open_Source/
â”‚   â””â”€â”€ Chapitre_28_Communication/
â””â”€â”€ Annexes/
    â”œâ”€â”€ Annexe_A_Maths/
    â”œâ”€â”€ Annexe_B_Installation/
    â”œâ”€â”€ Annexe_C_Datasets/
    â”œâ”€â”€ Annexe_D_Glossaire/
    â””â”€â”€ Annexe_E_Ressources/
â””â”€â”€ Exemples_Pratiques/
    â”œâ”€â”€ 01_Exemple_Trigger_Reel.md
    â”œâ”€â”€ 02_Compression_PyTorch_Complete.md
    â”œâ”€â”€ 03_Tensor_Train_Probleme_Reel.md
    â”œâ”€â”€ 04_Workflow_hls4ml_Complet.md
    â”œâ”€â”€ 05_Comparaison_FPGA_GPU_CPU.md
    â””â”€â”€ 06_Reconstruction_Evenement_Complet.md
```

### ğŸ“– DÃ©tail des Parties

**Partie I : Fondements ThÃ©oriques** (Chapitres 1-3)
- Introduction Ã  la physique des hautes Ã©nergies et au CERN
- AlgÃ¨bre linÃ©aire avancÃ©e (SVD, low-rank, produits tensoriels)
- Deep learning moderne (CNNs, Transformers, optimisation)

**Partie II : RÃ©seaux de Tenseurs** (Chapitres 4-7)
- Fondements des rÃ©seaux de tenseurs
- DÃ©compositions (CP, Tucker, Tensor Train, HT, Tensor Ring)
- Applications en physique quantique (MPS, PEPS, MERA)
- Conversion de rÃ©seaux de neurones en rÃ©seaux de tenseurs

**Partie III : Compression de ModÃ¨les** (Chapitres 8-12)
- Pruning (structurÃ©, non-structurÃ©, dynamique, Lottery Ticket)
- Quantification (PTQ, QAT, mixed-precision, binaire/ternaire)
- Knowledge Distillation (logits, features, relations)
- Approximations low-rank (SVD, LoRA)
- BibliothÃ¨que pQuant pour compression

**Partie IV : Hardware** (Chapitres 13-17)
- Introduction aux FPGA et HLS
- DÃ©ploiement de rÃ©seaux de neurones sur FPGA
- Framework hls4ml (CERN)
- Hardware-Aware Neural Architecture Search
- RÃ©seaux de tenseurs sur hardware

**Partie V : Applications HEP** (Chapitres 18-21)
- SystÃ¨mes de trigger et DAQ
- Reconstruction d'Ã©vÃ©nements (traces, jets, leptons, MET)
- DÃ©tection d'anomalies et nouvelle physique
- Simulation Monte Carlo avec GANs et Normalizing Flows

**Partie VI : ImplÃ©mentation** (Chapitres 22-25)
- Python pour deep learning (PyTorch, TensorFlow)
- ImplÃ©mentation de dÃ©compositions tensorielles
- Performance avec C++ (templates, parallÃ©lisation, pybind11)
- Pipelines de compression end-to-end

**Partie VII : Recherche** (Chapitres 26-28)
- MÃ©thodologie de recherche scientifique
- Contribution open source
- Communication scientifique (articles, prÃ©sentations, posters)

---

## ğŸš€ Installation et Configuration

### PrÃ©requis SystÃ¨me

```bash
# Python 3.8+ requis
python --version

# Git pour cloner le repository
git clone https://github.com/michaelgermini/Intelligence-Artificielle-Avancee-pour-la-Physique-des-Hautes-Energies.git
cd Intelligence-Artificielle-Avancee-pour-la-Physique-des-Hautes-Energies
```

### Installation des DÃ©pendances

```bash
# Environnement virtuel (recommandÃ©)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Installation des packages Python
pip install -r requirements.txt

# Packages optionnels pour exemples spÃ©cifiques
pip install torch torchvision  # PyTorch
pip install tensorflow keras  # TensorFlow
pip install tensorly tntorch  # RÃ©seaux de tenseurs
pip install hls4ml  # DÃ©ploiement FPGA
pip install uproot awkward  # DonnÃ©es HEP
```

Voir [Annexe B : Guide d'Installation](./Annexes/Annexe_B_Installation/annexe_B.md) pour les dÃ©tails complets.

### Configuration FPGA (Optionnel)

Pour les chapitres sur FPGA et hls4ml :
- Vivado HLS ou Vitis HLS (Xilinx)
- Voir [Annexe B](./Annexes/Annexe_B_Installation/annexe_B.md) pour l'installation

---

## ğŸ’» Exemples Pratiques

Le livre inclut **6 exemples pratiques complets** avec code fonctionnel :

### 1. ğŸ”¥ Exemple Trigger RÃ©el
SystÃ¨me de trigger IA pour le LHC avec contraintes de latence rÃ©elles (â‰¤ 4 Î¼s)
- Dataset CMS avec uproot
- ModÃ¨le ultra-lÃ©ger avec quantification INT8
- MÃ©triques HEP (signal efficiency, background rejection)

### 2. ğŸ—œï¸ Compression PyTorch ComplÃ¨te
Workflow end-to-end de compression d'un modÃ¨le ResNet-18
- Pruning structurÃ©
- Quantification INT8 post-training
- Knowledge Distillation
- Comparaison systÃ©matique avec visualisations

### 3. ğŸ”¢ Tensor Train sur ProblÃ¨me RÃ©el
DÃ©composition Tensor Train pour compresser une couche dense 1024Ã—1024
- Analyse trade-off compression vs erreur
- IntÃ©gration dans un modÃ¨le PyTorch
- Test sur dataset MNIST

### 4. âš¡ Workflow hls4ml Complet
Conversion d'un modÃ¨le Keras vers FPGA avec hls4ml
- Configuration et optimisation pour latence
- Simulation et validation
- Estimation des ressources FPGA
- Benchmarking et tuning

### 5. ğŸ“Š Comparaison FPGA vs GPU vs CPU
Benchmarking complet des diffÃ©rentes plateformes hardware
- Latence, throughput, consommation Ã©nergÃ©tique
- Visualisations comparatives
- Recommandations par use case

### 6. ğŸ”¬ Reconstruction Ã‰vÃ©nement Complet
Pipeline complet de reconstruction d'Ã©vÃ©nements HEP
- Reconstruction de traces avec ML
- Classification de jets et b-tagging
- Identification de leptons
- Reconstruction MET corrigÃ©e
- Visualisation d'Ã©vÃ©nements

Voir le [README des Exemples](./Exemples_Pratiques/README.md) pour plus de dÃ©tails.

---

## ğŸš€ Comment Utiliser ce Livre

### 1. **Lecture SÃ©quentielle** ğŸ“–
Pour une comprÃ©hension complÃ¨te, suivez les parties dans l'ordre :
- Commencez par la **Partie I** pour les fondements
- Poursuivez avec la **Partie II** pour les rÃ©seaux de tenseurs
- Explorez la **Partie III** pour la compression
- Appliquez avec la **Partie IV** (hardware) et **Partie V** (applications)

### 2. **RÃ©fÃ©rence Rapide** ğŸ”
- Utilisez [INDEX.md](./INDEX.md) pour naviguer rapidement
- Consultez les **Annexes** pour des rÃ©fÃ©rences rapides
- Utilisez le **Glossaire** (Annexe D) pour les dÃ©finitions

### 3. **Apprentissage Pratique** ğŸ’»
- Chaque chapitre contient des exemples de code
- ExÃ©cutez les **6 exemples pratiques** dans `Exemples_Pratiques/`
- Adaptez le code Ã  vos propres projets

### 4. **Recherche et Contribution** ğŸ”¬
- Consultez la **Partie VII** pour la mÃ©thodologie de recherche
- Contribuez au projet via GitHub (voir section Contribution)

---

## ğŸ“ Conventions et Style

- `` `Code inline` `` pour les noms de fonctions, variables, et commandes
- **Gras** pour les termes importants introduits pour la premiÃ¨re fois
- *Italique* pour l'emphase
- Les blocs de code sont annotÃ©s avec le langage utilisÃ© (Python, C++, etc.)
- Les formules mathÃ©matiques utilisent la notation LaTeX standard
- Les rÃ©fÃ©rences aux chapitres utilisent des liens relatifs

---

## ğŸ”— Ressources ComplÃ©mentaires

### Datasets et Outils

- **[CERN Open Data Portal](http://opendata.cern.ch/)** - DonnÃ©es ouvertes du LHC
- **[TrackML Challenge](https://www.kaggle.com/c/trackml-particle-identification)** - Challenge de reconstruction de traces
- **[Jet Tagging](https://opendata.cern.ch/record/14050)** - DonnÃ©es pour le tagging de jets

### BibliothÃ¨ques et Frameworks

- **[hls4ml](https://fastmachinelearning.org/hls4ml/)** - Conversion ML vers FPGA (CERN)
- **[pQuant](https://github.com/cern/pquant)** - BibliothÃ¨que de compression de modÃ¨les
- **[TensorNetwork](https://github.com/google/TensorNetwork)** - Calculs avec rÃ©seaux de tenseurs
- **[TensorLy](https://tensorly.org/)** - DÃ©compositions tensorielles en Python
- **[uproot](https://uproot.readthedocs.io/)** - AccÃ¨s aux fichiers ROOT en Python

### Documentation Technique

- **[PyTorch Documentation](https://pytorch.org/docs/)** - Framework deep learning
- **[TensorFlow Documentation](https://www.tensorflow.org/api_docs)** - Framework ML
- **[Xilinx Vivado HLS](https://www.xilinx.com/products/design-tools/vivado/integration/esl-design.html)** - High-Level Synthesis

Voir [Annexe E : Ressources et RÃ©fÃ©rences](./Annexes/Annexe_E_Ressources/ressources.md) pour une liste exhaustive.

---

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! Ce livre est un projet open source et Ã©volutif.

### Comment Contribuer

1. **Fork** le repository
2. CrÃ©ez une **branche** pour votre contribution (`git checkout -b feature/AmeliorationChapitre`)
3. **Commitez** vos modifications (`git commit -m 'Ajout de contenu sur...'`)
4. **Push** vers la branche (`git push origin feature/AmeliorationChapitre`)
5. Ouvrez une **Pull Request**

### Types de Contributions ApprÃ©ciÃ©es

- âœ… Correction d'erreurs (typos, formules, code)
- âœ… AmÃ©lioration d'exemples existants
- âœ… Ajout d'exemples pratiques supplÃ©mentaires
- âœ… Traduction en d'autres langues
- âœ… AmÃ©lioration de la documentation
- âœ… Ajout de visualisations et diagrammes
- âœ… Tests et validation du code

### Normes de Contribution

- Respecter le style et format Markdown utilisÃ©
- Tester le code avant de le soumettre
- Documenter les nouvelles fonctionnalitÃ©s
- Suivre les conventions de nommage existantes

Voir [Chapitre 27 : Contribution Open Source](./Partie_VII_Recherche/Chapitre_27_Open_Source/27_introduction.md) pour les bonnes pratiques.

---

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

Vous Ãªtes libre de :
- âœ… Utiliser ce contenu pour vos recherches et projets
- âœ… Modifier et adapter le contenu
- âœ… Partager et redistribuer
- âœ… Utiliser commercialement (avec attribution)

---

## ğŸ‘¥ Auteurs et Contact

**Auteur Principal** : Michael Germini  
**Email** : michael@germini.info  
**GitHub** : [@michaelgermini](https://github.com/michaelgermini)

### Remerciements

Ce livre s'inspire des travaux de recherche menÃ©s au CERN, en particulier dans les domaines de :
- Trigger systems avec IA (CMS, ATLAS)
- Reconstruction d'Ã©vÃ©nements avec machine learning
- Optimisation hardware pour applications HEP

---

## ğŸ—ºï¸ Roadmap et Ã‰tat du Projet

### âœ… Statut Actuel (100% Complet)

- [x] **Partie I** : Fondements ThÃ©oriques (3/3 chapitres)
- [x] **Partie II** : RÃ©seaux de Tenseurs (4/4 chapitres)
- [x] **Partie III** : Compression de ModÃ¨les (5/5 chapitres)
- [x] **Partie IV** : Hardware (5/5 chapitres)
- [x] **Partie V** : Applications HEP (4/4 chapitres)
- [x] **Partie VI** : ImplÃ©mentation (4/4 chapitres)
- [x] **Partie VII** : Recherche (3/3 chapitres)
- [x] **Annexes** : Toutes les annexes (5/5)
- [x] **Exemples Pratiques** : 6 exemples complets

### ğŸ”„ AmÃ©liorations Futures

- [ ] Conversion des exemples en notebooks Jupyter interactifs
- [ ] Ajout de tests automatisÃ©s pour le code
- [ ] CrÃ©ation d'un site web interactif
- [ ] GÃ©nÃ©ration automatique en PDF/LaTeX
- [ ] Version multilingue (anglais, franÃ§ais)
- [ ] VidÃ©os tutoriels pour les concepts clÃ©s
- [ ] IntÃ©gration avec Google Colab pour exÃ©cution en ligne

---

## ğŸ“ Support et Questions

- **Issues GitHub** : [Ouvrir une issue](https://github.com/michaelgermini/Intelligence-Artificielle-Avancee-pour-la-Physique-des-Hautes-Energies/issues) pour signaler des bugs ou suggÃ©rer des amÃ©liorations
- **Discussions** : Utilisez les [Discussions GitHub](https://github.com/michaelgermini/Intelligence-Artificielle-Avancee-pour-la-Physique-des-Hautes-Energies/discussions) pour poser des questions
- **Email** : michael@germini.info (pour questions gÃ©nÃ©rales)

---

## â­ Star le Projet

Si ce livre vous est utile, n'hÃ©sitez pas Ã  â­ **star** le repository ! Cela aide Ã  faire connaÃ®tre le projet.

---

## ğŸ“° Mises Ã  Jour

- **DÃ©cembre 2024** : Publication initiale sur GitHub
- **DÃ©cembre 2024** : Ajout de 6 exemples pratiques complets
- **DÃ©cembre 2024** : Completion de tous les chapitres et annexes

---

*Ce livre est en dÃ©veloppement continu. Contributions et suggestions bienvenues !*  
*DerniÃ¨re mise Ã  jour : DÃ©cembre 2024*

